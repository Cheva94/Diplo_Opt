{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbarsce/AprendizajePorRefuerzos/blob/master/lab_1_intro_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4spynKMzGS4t"
      },
      "source": [
        "# Notebook 1: Introducción al aprendizaje por refuerzos\n",
        "\n",
        "Curso Aprendizaje por Refuerzos, Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\n",
        "\n",
        "FaMAF, 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mlW-PAFGS4z"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "En el siguiente notebook se muestra cómo ejecutar agentes de aprendizaje por refuerzos, los cuáles son necesarios para realizar este Lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlIwTm_eyO0T"
      },
      "source": [
        "### Repaso rápido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87Luqo5UyO0T",
        "tags": []
      },
      "source": [
        "\n",
        "* Recompensa: señal $r$ recibida desde el entorno que recompensa o castiga el agente según su desempeño con respecto al objetivo de la tarea.\n",
        "\n",
        "* Valor: función $v_\\pi (s)$ que establece cuánto el agente espera percibir de recompensa al seguir la política $\\pi$ partiendo desde el estado $s$. También se la suele expresar como $Q_\\pi(s,a)$, indicando cuánto el agente espera percibir siguiendo la política $\\pi$ partiendo desde el estado $s$ y siguiendo la acción $a$.\n",
        "\n",
        "* Política: función $\\pi(s) \\to a$ que mapea un estado a una acción. Se suele expresar como probabilidad de elegir la acción $\\pi(a \\mid s)$. La política $\\epsilon$-greedy, en donde $\\epsilon$ es la probabilidad de exploración (normalmente menor que la probabilidad de explotación) está dada por $\\pi(a \\mid s) = 1 - \\epsilon$ si $a$ es la mejor acción, caso contrario pasa a estar dada por $\\pi(a \\mid s) = \\epsilon$.\n",
        "\n",
        "Por otra parte, en la política Softmax, no se busca la acción con máxima probabilidad sino que se computa la probabilidad de cada una mediante la función Softmax y se realiza un sorteo entre ellas pesado por la misma. Así, para cada acción $a$, $$\\pi(a \\mid s) = \\frac{e^{Q(s,a)/\\tau}}{\\sum_{\\widetilde{a} \\in A}e^{Q(s,\\widetilde{a})/\\tau}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V65VukjpyO0T",
        "tags": []
      },
      "source": [
        "En este notebook vemos dos algoritmos para actualizar la función de valor (y, por lo tanto, la política de selección de acciones):\n",
        "\n",
        "* Actualización por SARSA (on-policy).\n",
        "\n",
        "$$Q(s,a) \\gets Q(s,a) + \\alpha (r + \\gamma Q(s',a') - Q(s,a))$$\n",
        "\n",
        "Algoritmo completo (a modo de referencia):\n",
        "\n",
        "![Algoritmo SARSA](https://github.com/DiploDatos/AprendizajePorRefuerzos/blob/master/images/sarsa.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHP2ShFqyO0U",
        "tags": []
      },
      "source": [
        "* Actualización por Q-Learning (off-policy)\n",
        "\n",
        "$$Q(s,a) \\gets Q(s,a) + \\alpha (r + \\gamma \\arg\\max_{a'} Q(s',a') - Q(s,a))$$\n",
        "\n",
        "Algoritmo completo (a modo de referencia):\n",
        "\n",
        "![Algoritmo Q-Learning](https://github.com/DiploDatos/AprendizajePorRefuerzos/blob/master/images/q_learning.png?raw=1)\n",
        "\n",
        "Fuente de las imágenes: capítulo 6 de [Reinforcement Learning: An Introduction](http://www.incompleteideas.net/book/the-book.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7BO38B6GS4z"
      },
      "source": [
        "## Librería a usar: Librería OpenAI Gym\n",
        "\n",
        "[OpenAI Gym](https://gym.openai.com/) (Brockman et. al., 2016) es una librería de OpenAI que ofrece entornos y una interfaz estándar con la cuál probar nuestros agentes. Su objetivo es proveer benchmarks unificados para ver el desempeño de algoritmos en el entorno y así poder saber con facilidad cómo es su desempeño comparado con los demás. Parte de la siguiente sección está basada en la [documentación oficial de OpenAI](https://gym.openai.com/docs/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY7NVUdpGS40"
      },
      "source": [
        "La interfaz principal de los ambientes de gym es la interfaz Env. La misma posee cinco métodos principales:\n",
        "\n",
        "* ```reset(self, seed)``` : Reinicia el estado del entorno, a su estado inicial, devolviendo una observación de dicho estado. Opcionalmente, establece la semilla aleatoria del generador de números aleatorios del presente entorno.\n",
        "\n",
        "* ```step(self, action)``` : \"Avanza\" un timestep del ambiente. Devuelve: ```observation, reward, terminated, truncated, info```.\n",
        "\n",
        "* ```render(self)``` : Muestra en pantalla una parte del ambiente.\n",
        "\n",
        "* ```close(self)``` : Finaliza con la instancia del agente.\n",
        "\n",
        "\n",
        "Por otra parte, cada entorno posee los siguientes tres atributos principales:\n",
        "\n",
        "* ```action_space``` : El objeto de tipo Space correspondiente al espacio de acciones válidas.\n",
        "\n",
        "* ```observation_space``` : El objeto de tipo Space correspondiente a todos los rangos posibles de observaciones.\n",
        "\n",
        "* ```reward_range``` : Tupla que contiene los valores mínimo y máximo de recompensa posible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTMpnXzcGS40"
      },
      "source": [
        "Algunas de las ejecuciones contienen videos. Para poder verlos se necesita previamente instalar la librería ffmpeg; para instalarla desde Linux ejecutar en consola\n",
        "\n",
        "```apt-get install ffmpeg```\n",
        "\n",
        "desde Mac, reemplazar *apt-get* por *brew*\n",
        "\n",
        "desde Windows, descargarla desde\n",
        "\n",
        "[https://ffmpeg.org/download.html](https://ffmpeg.org/download.html)\n",
        "\n",
        "(Nota: las animaciones son a modo ilustrativo, si no se desea instalar la librería se puede directamente eliminar la línea de código donde se llama al método ``env.render(mode='human')``)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZDT_O0UGS40"
      },
      "source": [
        "Código básico de importación y funciones de graficación (no modificar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hideCode": true,
        "id": "5jUjOuhYGS42",
        "tags": [
          "hide-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# @title Código básico importacion y utilerías (no modificar)\n",
        "\n",
        "import itertools\n",
        "from typing import Any, Callable, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_reward_per_episode(reward_ep) -> None:\n",
        "    episode_rewards = np.array(reward_ep)\n",
        "\n",
        "    # se suaviza la curva de convergencia\n",
        "    episode_number = np.linspace(\n",
        "        1, len(episode_rewards) + 1, len(episode_rewards) + 1)\n",
        "    acumulated_rewards = np.cumsum(episode_rewards)\n",
        "\n",
        "    reward_per_episode = [\n",
        "        acumulated_rewards[i] / episode_number[i]\n",
        "        for i in range(len(acumulated_rewards))\n",
        "    ]\n",
        "\n",
        "    plt.plot(reward_per_episode)\n",
        "    plt.title(\"Recompensa acumulada por episodio\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_steps_per_episode(timesteps_ep) -> None:\n",
        "    # se muestra la curva de aprendizaje de los pasos por episodio\n",
        "    episode_steps = np.array(timesteps_ep)\n",
        "    plt.plot(np.array(range(0, len(episode_steps))), episode_steps)\n",
        "    plt.title(\"Pasos (timesteps) por episodio\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_steps_per_episode_smooth(timesteps_ep) -> None:\n",
        "    episode_steps = np.array(timesteps_ep)\n",
        "\n",
        "    # se suaviza la curva de aprendizaje\n",
        "    episode_number = np.linspace(\n",
        "        1, len(episode_steps) + 1, len(episode_steps) + 1)\n",
        "    acumulated_steps = np.cumsum(episode_steps)\n",
        "\n",
        "    steps_per_episode = [\n",
        "        acumulated_steps[i] / episode_number[i] for i in range(\n",
        "            len(acumulated_steps))\n",
        "    ]\n",
        "\n",
        "    plt.plot(steps_per_episode)\n",
        "    plt.title(\"Pasos (timesteps) acumulados por episodio\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def draw_value_matrix(q) -> None:\n",
        "    n_rows = 4\n",
        "    n_columns = 12\n",
        "    n_actions = 4\n",
        "\n",
        "    # se procede con los cálculos previos a la graficación de la matriz de valor\n",
        "    q_value_matrix = np.empty((n_rows, n_columns))\n",
        "    for row in range(n_rows):\n",
        "        for column in range(n_columns):\n",
        "            state_values = []\n",
        "\n",
        "            for action in range(n_actions):\n",
        "                state_values.append(\n",
        "                    q.get((row * n_columns + column, action), -100))\n",
        "\n",
        "            maximum_value = max(\n",
        "                state_values\n",
        "            )  # determinamos la acción que arroja máximo valor\n",
        "\n",
        "            q_value_matrix[row, column] = maximum_value\n",
        "\n",
        "    # el valor del estado objetivo se asigna en -1 (reward recibido al llegar)\n",
        "    # para que se coloree de forma apropiada\n",
        "    q_value_matrix[3, 11] = -1\n",
        "\n",
        "    # se grafica la matriz de valor\n",
        "    plt.imshow(q_value_matrix, cmap=plt.cm.RdYlGn)\n",
        "    plt.tight_layout()\n",
        "    plt.colorbar()\n",
        "\n",
        "    for row, column in itertools.product(\n",
        "        range(q_value_matrix.shape[0]), range(q_value_matrix.shape[1])\n",
        "    ):\n",
        "        left_action = q.get((row * n_columns + column, 3), -1000)\n",
        "        down_action = q.get((row * n_columns + column, 2), -1000)\n",
        "        right_action = q.get((row * n_columns + column, 1), -1000)\n",
        "        up_action = q.get((row * n_columns + column, 0), -1000)\n",
        "\n",
        "        arrow_direction = \"D\"\n",
        "        best_action = down_action\n",
        "\n",
        "        if best_action < right_action:\n",
        "            arrow_direction = \"R\"\n",
        "            best_action = right_action\n",
        "        if best_action < left_action:\n",
        "            arrow_direction = \"L\"\n",
        "            best_action = left_action\n",
        "        if best_action < up_action:\n",
        "            arrow_direction = \"U\"\n",
        "            best_action = up_action\n",
        "        if best_action == -1:\n",
        "            arrow_direction = \"\"\n",
        "\n",
        "        # notar que column, row están invertidos en orden en la línea de abajo\n",
        "        # porque representan a x,y del plot\n",
        "        plt.text(column, row, arrow_direction, horizontalalignment=\"center\")\n",
        "\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n Matriz de mejor acción-valor (en números): \\n\\n\", q_value_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ1y2VTGGS42"
      },
      "source": [
        "Ejemplo: agente CartPole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPkxgtSiGS43",
        "outputId": "144a3a83-7886-431e-e98e-a9b858d3483c"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "\n",
        "  !pip install gymnasium\n",
        "\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "# no es posible mostrar videos de ejecución del agente desde Colab\n",
        "if not IN_COLAB:\n",
        "\n",
        "    env = gym.make('CartPole-v0', render_mode=\"human\")\n",
        "    env.reset()\n",
        "    env.render()\n",
        "\n",
        "    for _ in range(500):\n",
        "        env.render()\n",
        "        # se ejecuta una acción aleatoria\n",
        "        obs, reward, terminated, truncated, info = env.step(\n",
        "           env.action_space.sample())\n",
        "\n",
        "        done = terminated or truncated\n",
        "        if done:\n",
        "            env.reset()\n",
        "    env.close()\n",
        "    clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ_A87JtGS43"
      },
      "source": [
        "Ejemplo: agente Mountain Car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHDTjlpEGS43"
      },
      "outputs": [],
      "source": [
        "if not IN_COLAB:\n",
        "    env = gym.make('MountainCar-v0', render_mode=\"human\")\n",
        "    obs = env.reset()\n",
        "    for t in range(500):\n",
        "        env.render()\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "            break\n",
        "    env.close()\n",
        "    clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrdYut6HGS44"
      },
      "source": [
        "## Ejemplo 1: The Cliff.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6bNIHCQGS44"
      },
      "source": [
        "![](https://github.com/GIDISIA/RLDiplodatos/blob/master/images/cliffwalking.png?raw=1)\n",
        "\n",
        "donde S= starting point, G= goal\n",
        "\n",
        "(imagen de Sutton y Barto, 2018)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYvKmWcsGS44"
      },
      "source": [
        "Descripción del entorno:\n",
        "\n",
        "Acciones:\n",
        "\n",
        "* $\\uparrow$ - Arriba\n",
        "* $\\downarrow$ - Abajo\n",
        "* $\\rightarrow$ - Derecha\n",
        "* $\\leftarrow$ - Izquierda\n",
        "\n",
        "Función de recompensa:\n",
        "\n",
        "* $-1$ en todos los demás estados\n",
        "* $-100$ en el acantilado\n",
        "\n",
        "Nota: caer en el acantilado devuelve al agente al estado inicial en un mismo episodio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ2D3hokGS44"
      },
      "source": [
        "Vemos los bloques básicos de nuestro agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKgvYRiOGS44"
      },
      "source": [
        "Definimos el método de elección de acciones. En este caso el mismo utiliza la política de exploración $\\epsilon$-greedy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8XHFSZiGS44"
      },
      "outputs": [],
      "source": [
        "def choose_action_e_greedy(\n",
        "        state: int,\n",
        "        actions: range,\n",
        "        q: dict,\n",
        "        hyperparameters: dict,\n",
        "        random_state: np.random.RandomState,\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Elije una acción de acuerdo al aprendizaje realizado previamente\n",
        "    usando una política de exploración épsilon-greedy\n",
        "    \"\"\"\n",
        "    # ej: para 4 acciones inicializa en [0,0,0,0]\n",
        "    q_values = [q.get((state, a), 0.0) for a in actions]\n",
        "    max_q = max(q_values)\n",
        "    # sorteamos un número: es menor a épsilon?\n",
        "    if random_state.uniform() < hyperparameters['epsilon']:\n",
        "        # sí: se selecciona una acción aleatoria\n",
        "        return random_state.choice(actions)\n",
        "\n",
        "    count = q_values.count(max_q)\n",
        "\n",
        "    # hay más de un máximo valor de estado-acción?\n",
        "    if count > 1:\n",
        "        # sí: seleccionamos uno de ellos aleatoriamente\n",
        "        best = [i for i in range(len(actions)) if q_values[i] == max_q]\n",
        "        i = random_state.choice(best)\n",
        "    else:\n",
        "        # no: seleccionamos el máximo valor de estado-acción\n",
        "        i = q_values.index(max_q)\n",
        "\n",
        "    return actions[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDGO_qGWyO0W"
      },
      "outputs": [],
      "source": [
        "def choose_action_softmax() -> int:\n",
        "    \"\"\"\n",
        "    Elije una acción de acuerdo al aprendizaje realizado previamente\n",
        "    usando una política softmax\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: implementar\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH6yG3UNGS45"
      },
      "source": [
        "Definimos el esqueleto del método learn, el cuál toma una transición y cambia el dict de los valores de Q de acuerdo a algún algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RgIJcbGGS45"
      },
      "outputs": [],
      "source": [
        "def learn_SARSA(\n",
        "        state: Any, # COMPLETAR tipo de cada parámetro\n",
        "        action: Any,\n",
        "        reward: Any,\n",
        "        next_state: Any,\n",
        "        next_action: Any,\n",
        "        hyperparameters: Any,\n",
        "        q: Any,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Realiza una actualización según el algoritmo SARSA, para una transición\n",
        "    de estado dada\n",
        "    Args:\n",
        "        state: COMPLETAR\n",
        "        action: COMPLETAR\n",
        "        reward: COMPLETAR\n",
        "        next_state: COMPLETAR\n",
        "        next_action: COMPLETAR\n",
        "        hyperparameters: COMPLETAR\n",
        "        q: COMPLETAR\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO - completa con tu código aquí\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilHnd2pOyO0X"
      },
      "outputs": [],
      "source": [
        "# completar argumentos de la función para hacer una actualización Q-learning\n",
        "def learn_Q_learning(\n",
        "        # COMPLETAR\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Realiza una actualización según el algoritmo Q-learning, para una transición\n",
        "    de estado dada\n",
        "    Args: COMPLETAR\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO - completa con tu código aquí\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xrOq-6jGS45"
      },
      "source": [
        "Finalmente, definimos el método principal de iteraciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EatmEq8XGS45"
      },
      "outputs": [],
      "source": [
        "def run(\n",
        "    learning_function: Callable,\n",
        "    hyperparameters: dict,\n",
        "    episodes_to_run: int,\n",
        "    env: gym.Env,\n",
        "    actions: range,\n",
        "    q: dict,\n",
        "    random_state: np.random.RandomState,\n",
        ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Corre el algoritmo de RL para el ambiente FrozenLake-v0.\n",
        "    Args:\n",
        "        learning_function: función de actualización de algoritmo de aprendizaje\n",
        "        hyperparameters: hiperparámetros del algoritmo de aprendizaje\n",
        "        episodes_to_run: cantidad de episodios a ejecutar\n",
        "        env: ambiente de Gymnasium\n",
        "        actions: lista de acciones posibles\n",
        "        q: diccionario de valores de estado-acción\n",
        "        random_state: generador de números aleatorios\n",
        "    \"\"\"\n",
        "    # registro de la cantidad de pasos que le llevó en cada episodio\n",
        "    # llegar a la salida\n",
        "    timesteps_of_episode = []\n",
        "    # cantidad de recompensa que recibió el agente en cada episodio\n",
        "    reward_of_episode = []\n",
        "\n",
        "    for _ in range(episodes_to_run):\n",
        "        # se ejecuta una instancia del agente hasta que el mismo\n",
        "        # llega a la salida o tarda más de 2000 pasos\n",
        "\n",
        "        # reinicia el ambiente, obteniendo el estado inicial del mismo\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        t = 0\n",
        "\n",
        "        # elige una acción basado en el estado actual.\n",
        "        # Filtra el primer elemento de state, que es el estado en sí mismo\n",
        "        action = choose_action_e_greedy(\n",
        "            state, actions, q, hyperparameters, random_state)\n",
        "\n",
        "        while not done:\n",
        "            # el agente ejecuta la acción elegida y obtiene los resultados\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            next_action = choose_action_e_greedy(\n",
        "                next_state, actions, q, hyperparameters, random_state)\n",
        "\n",
        "            episode_reward += reward\n",
        "            learning_function(\n",
        "                state,\n",
        "                action,\n",
        "                reward,\n",
        "                next_state,\n",
        "                next_action,\n",
        "                hyperparameters,\n",
        "                q\n",
        "            )\n",
        "\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # if the algorithm does not converge, it stops after 2000 timesteps\n",
        "            if not done and t < 2000:\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "            else:\n",
        "                # el algoritmo no ha podido llegar a la meta antes de dar 2000 pasos\n",
        "                done = True  # se establece manualmente la bandera done\n",
        "                timesteps_of_episode = np.append(timesteps_of_episode, [int(t + 1)])\n",
        "                reward_of_episode = np.append(\n",
        "                    reward_of_episode, max(episode_reward, -100)\n",
        "                )\n",
        "\n",
        "            t += 1\n",
        "\n",
        "    return reward_of_episode.mean(), timesteps_of_episode, reward_of_episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQqcI9RTGS46"
      },
      "source": [
        "Definidos los métodos básicos, procedemos a instanciar a nuestro agente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWE7qhNvGS46"
      },
      "outputs": [],
      "source": [
        "# se crea el diccionario que contendrá los valores de Q\n",
        "# para cada tupla (estado, acción)\n",
        "q = {}\n",
        "\n",
        "# definimos sus híper-parámetros básicos\n",
        "hyperparameters = {\n",
        "    \"alpha\": 0.5,\n",
        "    \"gamma\": 1,\n",
        "    \"epsilon\": 0.1,\n",
        "    \"tau\": 25,\n",
        "}\n",
        "\n",
        "\n",
        "# se puede cambiar por learn_Q_learning, una vez que se implemente\n",
        "learning_function = learn_SARSA\n",
        "episodes_to_run = 500\n",
        "\n",
        "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
        "actions = range(env.action_space.n)\n",
        "\n",
        "# se declara una semilla aleatoria\n",
        "random_state = np.random.RandomState(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nBgpEltGS47"
      },
      "source": [
        "Ya instanciado, ejecutamos nuestro agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_Y6EMN3GS47"
      },
      "outputs": [],
      "source": [
        "avg_steps_per_episode, timesteps_ep, reward_ep = run(\n",
        "    learning_function,\n",
        "    hyperparameters,\n",
        "    episodes_to_run,\n",
        "    env,\n",
        "    actions,\n",
        "    q,\n",
        "    random_state\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgNpJUV9GS47"
      },
      "source": [
        "### Análisis de la ejecución del agente\n",
        "\n",
        "#### Análisis de convergencia\n",
        "\n",
        "A diferencia de lo que sucede en el aprendizaje supervisado, en el aprendizaje por refuerzos el rendimiento se evalúa por una función específica que es la función de recompensa. En la práctica, la función de recompensa puede ser externa (y provista por el entorno) o bien puede ser una función creada por diseño (a modo de dirigir el agente hacia lo que por diseño se considera mejor, en nuestro ejemplo podría ser con una recompensa de $+1$ cada vez que el agente llega al estado objetivo). Esto se conoce como *reward shaping*, y hay que tener mucho cuidado con los posibles efectos secundarios de su uso.\n",
        "\n",
        "Como el objetivo de RL es maximizar la recompensa obtenida, es posible utilizar la información sobre la obtención de la recompensas en cada time-step o episodio para evaluar el rendimiento parcial del agente (esto depende mucho de la particularidad de la distribución de la recompensa para el problema tratado)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiciXUMeGS47"
      },
      "source": [
        "Para analizar la ejecución del agente, vamos a ver cómo se desempeñó el mismo en dos curvas:\n",
        "\n",
        "* Recompensa obtenida en cada episodio: nos dirá cuánta recompensa obtuvo el agente sumando cada una de recompensas individuales de cada episodio. Con esta medida podremos tener una noción de cómo se desempeñó esquivando el acantilado y llegando lo antes posible a la meta.\n",
        "\n",
        "* Pasos transcurridos en cada episodio: indicará cuántos pasos le ha llevado al agente la ejecución del episodio.\n",
        "\n",
        "Se estila suavizar ambas curvas para apreciar mejor su progresión (aunque a veces suele analizarse la curva de pasos por episodio sin suavizar)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQKMwQ2mGS47"
      },
      "source": [
        "Veamos recompensa por episodio (recordar que en este entorno cada paso otorga una recompensa de $-1$ excepto al caer al acantilado, donde la recompensa es de $-100$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "SsDunXlHGS47",
        "outputId": "5f596668-2d4d-4575-a5a2-49d41ed9155d"
      },
      "outputs": [],
      "source": [
        "plot_reward_per_episode(reward_ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjznrgIeGS48"
      },
      "source": [
        "Veamos pasos por episodio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "7AaVw7EEGS48",
        "outputId": "42c8b5af-85fd-4c40-ea72-b2307db603af"
      },
      "outputs": [],
      "source": [
        "plot_steps_per_episode(timesteps_ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BB8NDDhGS49"
      },
      "source": [
        "Suavizando..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "NaAXjfdKGS49",
        "outputId": "4392abcb-8a3e-4f84-982c-111677245ab7"
      },
      "outputs": [],
      "source": [
        "plot_steps_per_episode_smooth(timesteps_ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI7pxgtoGS49"
      },
      "source": [
        "#### Análisis de matriz de acción-valor y política óptima\n",
        "\n",
        "Siendo que este es un ejemplo tabular y de pocos estados / acciones, es posible realizar un análisis de convergencia desde otro punto de vista: desde el valor de la función $Q(s,a)$ para la mejor acción de cada estado, al finalizar el entrenamiento del agente, (sería la acción que el agente ejecutaría en cada estado bajo una política *greedy*). Ambos nos brindarán información sobre la convergencia alcanzada por el agente.\n",
        "\n",
        "Tener en cuenta que este análisis se hace principalmente con fines educativos, para entornos más complejos el mismo puede no ser factible. En tales casos, un análisis alternativo podría consistir en hacer que el agente ejecute su política para la que fue entrenado, para hacer una evaluación a partir del comportamiento del mismo (esto último sería el *test de la política*, frente al *entrenamiento de la política* previo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "rMpKRT2NGS4-",
        "outputId": "d725517c-9a9c-4d89-e003-16dc16e4954e"
      },
      "outputs": [],
      "source": [
        "draw_value_matrix(q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce50gtxcGS4-"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VQRIwfLGS4-",
        "tags": []
      },
      "source": [
        "## Actividades\n",
        "\n",
        "1. Implementar y ejecutar el algoritmo SARSA en \"The Cliff\".\n",
        "\n",
        "2. Implementar y ejecutar el algoritmo Q-Learning en \"The Cliff\". ¿Cómo converge con respecto a SARSA? ¿A qué se debe? Comentar.\n",
        "\n",
        "3. Ejecutando con distintos híper-parámetros, realizar una breve descripción sobre cómo afectan a la convergencia los distintos valores de $\\alpha$, $\\epsilon$ y $\\gamma$.\n",
        "\n",
        "4. (Opcional) Implementar política de exploración Softmax, en donde cada acción tiene una probabilidad $$\\pi(a \\mid s) = \\frac{e^{Q(s,a)/\\tau}}{\\sum_{\\widetilde{a} \\in A}e^{Q(s,\\widetilde{a})/\\tau}}$$\n",
        "\n",
        "5. (Opcional) Implementar Dyna-Q a partir del algoritmo Q-Learning, incorporando una actualización mediante un modelo. Comentar cómo se desempeña respecto a los demás algoritmos.\n",
        "\n",
        "\n",
        "Para dejar el lab listo para su corrección, dejar link a repo de Github con un notebook ejecutando el agente en la planilla enviada en Slack."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjITu5NIyO0b"
      },
      "source": [
        "FIN"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Edit Metadata",
    "colab": {
      "include_colab_link": true,
      "name": "lab_1_intro_rl.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aprendizajeporrefuerzos-7kOLcKCl-py3.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
