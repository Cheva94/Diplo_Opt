{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
    "\n",
    "## Aprendizaje por refuerzos\n",
    "\n",
    "### Trabajo práctico entregable 2/2 (materia completa)\n",
    "\n",
    "**Estudiante:**\n",
    "- [Chevallier-Boutell, Ignacio José.](https://www.linkedin.com/in/nachocheva/)\n",
    "\n",
    "**Docentes:**\n",
    "- Palombarini, Jorge (Mercado Libre).\n",
    "- Barsce, Juan Cruz (Mercado Libre).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import time\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Traemos las funciones ya definidas y utilizadas en el primer laboratorio\n",
    "from utyls_RL import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los videos de las ejecuciones hay que tener instalado ffmpeg (`apt-get install ffmpeg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio 1\n",
    "\n",
    "Crear un entorno propio y entrenar agentes de RL en el mismo, utilizando diferentes algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheva's Odyssey: reglas del juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mapa del juego consiste en una grilla 6x6, como se muestra a continuación. Al comenzar un episodio, el agente se ubica en la posición $S$ (elemento [5, 5]) y debe llegar hasta $G$ (elemento [0, 2]) para terminar dicho episodio. El agente debe realizar tantos movimientos como sean necesarios para llegar a la meta y finalizar el episodio.\n",
    "\n",
    "|J|A|G|A|~|~|\n",
    "|-|-|-|-|-|-|\n",
    "|~|A|A|A|~|~|\n",
    "|~|~|~|~|~|~|\n",
    "|~|~|~|~|~|~|\n",
    "|A|A|~|~|~|~|\n",
    "|P|A|~|~|~|S|\n",
    "\n",
    "El espacio de estados $\\mathcal{S}$ tiene 36 elementos. Para calcular el valor del estado asociado a cada elemento del mapa debemos calcular\n",
    "    $$\\text{Fila Actual} \\times \\text{Número de Columnas} + \\text{Columna Actual}$$\n",
    "\n",
    "donde debemos contar desde 0. De esta manera, el estado inicial $S$ es el estado 35 y el estado terminal $G$ es el estado 2.\n",
    "\n",
    "El espacio de acciones $\\mathcal{A}$ tiene 4 elementos para todos los estados que no están en el borde del mapa:\n",
    "- 0 $\\Rightarrow$ Se mueve hacia arriba.\n",
    "- 1 $\\Rightarrow$ Se mueve hacia la derecha.\n",
    "- 2 $\\Rightarrow$ Se mueve hacia abajo.\n",
    "- 3 $\\Rightarrow$ Se mueve hacia la izquierda.\n",
    "\n",
    "Para los estados que están en los bordes, sólo se puede elegir entre 2 acciones si están en los vértices o entre 3 si están en las aristas, según corresponda. Observamos que se cumple que $\\mathcal{A}, \\mathcal{S} \\in \\mathbb{N}$.\n",
    "\n",
    "Además de la salida $S$ y la meta $G$, tenemos otros elementos en el mapa. Los elementos vacíos (~) representan pasto, mientras que los elementos con $A$ son agua. El elemento $J$ es un jetpack y el elemento $P$ representa un premio extra. A partir de esto, la función de recompensa es tal que el agente recibe:\n",
    "- $-1$ cuando ingresa en a un elemento con pasto o cuando busca el jetpack o el premio extra. Sus efectos no se pierden, pero tampoco se acumulan.\n",
    "- $-8$ cuando ingresa en a un elemento con agua. En caso de contar con el jetpack, el costo por pasar por el agua se reduce a $-2$.\n",
    "- $+0$ si alcanza la meta sin el premio extra y $+24$ cuando la alcanza con el premio extra.\n",
    "\n",
    "Hay principalmente 5 caminos relevantes:\n",
    "- **SG:** Ir directo a la meta requiere 8 pasos temporales, otorgando -14 puntos.\n",
    "- **SJG:** Buscar el jetpack e ir a la meta requiere 12 pasos temporales, otorgando -12 puntos.\n",
    "- **SPG:** Buscar el premio extra e ir a la meta requiere 12 pasos temporales, otorgando -8 puntos.\n",
    "- **SPJG:** Buscar el premio extra, luego el jetpack e ir a la meta requiere 12 pasos temporales, otorgando -2 puntos.\n",
    "- **SJPG:** Buscar el jetpack, luego el premio extra e ir a la meta requiere 22 pasos temporales, otorgando 0 puntos (puntuación máxima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la clase del agente asociado al juego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChevasOdyssey(gym.Env):\n",
    "    # Tipo de renderizado posible, además de None.\n",
    "    metadata = {\"render.modes\": [\"console\"]}\n",
    "\n",
    "    # Definimos los valores de las acciones\n",
    "    UP = 0\n",
    "    RIGHT = 1\n",
    "    DOWN = 2\n",
    "    LEFT = 3\n",
    "\n",
    "    # Efectos del Jetpack y el premio\n",
    "    JETPACK = 1\n",
    "    PREMIO = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ChevasOdyssey, self).__init__()\n",
    "\n",
    "        # Tamaño del mapa\n",
    "        self.grid_shape = (6, 6)\n",
    "\n",
    "        # Espacio de acciones\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Espacio de estados\n",
    "        self.observation_space = spaces.Discrete(36)\n",
    "\n",
    "        # Inicializamos en agente en la salida\n",
    "        self.agent_pos = 35\n",
    "\n",
    "        # Ubicamos el jetpack\n",
    "        self.jetpack_pos = 0\n",
    "\n",
    "        # Ubicamos el premio extra\n",
    "        self.extra_pos = 30\n",
    "\n",
    "        # Ubicamos la meta\n",
    "        self.goal_pos = 2\n",
    "\n",
    "        # Ubicamos el agua\n",
    "        self.water_pos = [1, 3, 7, 8, 9, 24, 25, 31]\n",
    "\n",
    "    def reset(self, seed=None) -> Tuple[np.array, dict]:\n",
    "        \"\"\"\n",
    "        Reinicia el ambiente y devuelve la observación inicial\n",
    "        \"\"\"\n",
    "\n",
    "        # Inicializamos en agente en el punto de partida\n",
    "        self.agent_pos = 35\n",
    "\n",
    "        self.JETPACK = 1\n",
    "        \n",
    "        self.PREMIO = 0\n",
    "\n",
    "        return (self.agent_pos, {})\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.UP:\n",
    "            self.agent_pos -= 6\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        elif action == self.DOWN:\n",
    "            self.agent_pos += 6\n",
    "        elif action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Se recibió una acción inválida={action} que no es parte del\\\n",
    "                    espacio de acciones\"\n",
    "            )\n",
    "\n",
    "        # Evitamos que el agente se salga de los límites de la grilla\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 36)\n",
    "\n",
    "        # Consigue el jetpack\n",
    "        if self.agent_pos == self.jetpack_pos:\n",
    "            self.JETPACK = 4\n",
    "\n",
    "        # Consigue el premio extra\n",
    "        if self.agent_pos == self.extra_pos:\n",
    "            self.PREMIO = 24\n",
    "\n",
    "        # Llegada a la meta\n",
    "        terminated = bool(self.agent_pos == self.goal_pos)\n",
    "        truncated = False\n",
    "\n",
    "        # Asignación de recompensa\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = self.PREMIO\n",
    "        elif self.agent_pos in  self.water_pos:\n",
    "            reward = -8 / self.JETPACK\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # Información adicional\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            self.agent_pos,\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        print('Not implemented yet.')\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validamos que el ambiente cumpla con la interfaz de gym. Si el entorno no cumple con la interfaz, se lanzará una excepción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChevasOdyssey()\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaciones\n",
    "\n",
    "Se utilizan las 3 combinaciones que mejores resultados arrojaron en el primer laboratoio:\n",
    "* SARSA con SoftMax.\n",
    "* Q-learning con $\\epsilon$-greedy.\n",
    "* Dyna-Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChevasOdyssey()\n",
    "actions = range(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA con SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = SoftMax\n",
    "\n",
    "max_iter = 1000\n",
    "Sw_eps = [20000]\n",
    "Sw_a = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "Sw_g = [1]\n",
    "Sw_t = [0.005]\n",
    "\n",
    "for p_eps in Sw_eps:\n",
    "    episodes_to_run = p_eps\n",
    "    for p_a in Sw_a:\n",
    "        for p_g in Sw_g:\n",
    "            for p_t in Sw_t:\n",
    "                hyperparameters = {\n",
    "                    \"alpha\": p_a,\n",
    "                    \"gamma\": p_g,\n",
    "                    \"tau\": p_t,\n",
    "                }\n",
    "\n",
    "                random_state = np.random.RandomState(1994)\n",
    "\n",
    "                start = time.time()\n",
    "                ep_timesteps, ep_return, ep_goal, ep_drop, ep_early, q = run_SARSA(\n",
    "                    policy,\n",
    "                    hyperparameters,\n",
    "                    episodes_to_run,\n",
    "                    env,\n",
    "                    actions,\n",
    "                    random_state,\n",
    "                    max_iter\n",
    "                )\n",
    "                WallTime = time.time() - start\n",
    "\n",
    "                env.close()\n",
    "\n",
    "                # Guardamos los resultados\n",
    "                runType = f'SARSA-{policy.__name__}'\n",
    "                runPar = f'its-{max_iter}_eps-{episodes_to_run}'\n",
    "                runHPar = f'a-{hyperparameters[\"alpha\"]}_g-{hyperparameters[\"gamma\"]}_t-{hyperparameters[\"tau\"]}'\n",
    "\n",
    "                with open(f'Outputs/Lab2/{runType}_{runPar}_{runHPar}.csv', 'w') as f:\n",
    "                    f.write('Goal\\tDrop\\tEarly\\tTotal\\n')\n",
    "                    f.write(f'{ep_goal}\\t{ep_drop}\\t{ep_early}\\t{episodes_to_run}\\n')\n",
    "\n",
    "                    f.write(f'Wall Time[s]\\t{WallTime}\\n')\n",
    "                    \n",
    "                    f.write('State\\tAction\\tQ-value\\n')\n",
    "                    for s in range(37):\n",
    "                        for a in range(4):\n",
    "                            f.write(f'{s}\\t{a}\\t{q[(s,a)]:.6f}\\n')\n",
    "\n",
    "                    f.write('Return\\tTimeSteps\\n')\n",
    "                    for i in range(episodes_to_run):\n",
    "                        f.write(f'{ep_return[i]:.0f}\\t{ep_timesteps[i]:.0f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# File = pd.read_csv(f'Outputs/Lab2/SARSA-epGreedy_its-2000_eps-100000_a-0.05_g-1_e-0.005.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "\n",
    "# axs[0].plot(mean_evol(File[:,0]))#, label=rf'$\\gamma$ = {p_g}')\n",
    "# axs[1].plot(mean_evol(File[:,1]))#, label=rf'$\\gamma$ = {p_g}')\n",
    "\n",
    "# # axs[0].set_ylim(-18, -12)\n",
    "# # axs[1].set_ylim(12, 18)\n",
    "\n",
    "# # axs[0].axhline(-13, color='red', linestyle=':', label='Óptimo')\n",
    "# # axs[0].axhline(-17, color='blue', linestyle=':', label='Seguro')\n",
    "\n",
    "# axs[0].set_xlabel('Episodios')\n",
    "# axs[1].set_xlabel('Episodios')\n",
    "\n",
    "# # axs[1].axhline(13, color='red', linestyle=':', label='Óptimo')\n",
    "# # axs[1].axhline(17, color='blue', linestyle=':', label='Seguro')\n",
    "\n",
    "# axs[0].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "# axs[1].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "\n",
    "# axs[0].set_ylabel('Retorno promedio')\n",
    "# axs[1].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "# axs[0].legend(loc='lower right', frameon=False, fancybox=True, title='Camino')\n",
    "# axs[1].legend(loc='upper right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "# plt.savefig('Outputs/Lab2/SARSA-epGreedy_its-2000_eps-100_a-0.05_g-1_e-0.005.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q y Q-learning con $\\epsilon$-greedy\n",
    "\n",
    "Cuando se quiera hacer Q-learning con $\\epsilon$-greedy, se usa Dyna-Q con 0 pasos de planificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CliffWalking-v0\")#, render_mode=\"human\")\n",
    "# actions = range(env.action_space.n)\n",
    "# policy = epGreedy\n",
    "\n",
    "# max_iter = 2000\n",
    "# Sw_eps = [100000]\n",
    "# Sw_a = [0.05]\n",
    "# Sw_g = [0.65]\n",
    "# Sw_e = [0.005]\n",
    "# Sw_s = [0, 50, 10]\n",
    "\n",
    "# for p_eps in Sw_eps:\n",
    "#     episodes_to_run = p_eps\n",
    "#     for p_a in Sw_a:\n",
    "#         for p_g in Sw_g:\n",
    "#             for p_e in Sw_e:\n",
    "#                 for p_s in Sw_s:\n",
    "#                     hyperparameters = {\n",
    "#                         \"alpha\": p_a,\n",
    "#                         \"gamma\": p_g,\n",
    "#                         \"epsilon\": p_e,\n",
    "#                         \"steps\": p_s\n",
    "#                     }\n",
    "#                     print(hyperparameters)\n",
    "\n",
    "#                     random_state = np.random.RandomState(1994)\n",
    "\n",
    "#                     start = time.time()\n",
    "#                     ep_timesteps, ep_return, ep_goal, ep_drop, ep_early, q = run_DynaQ(\n",
    "#                         policy,\n",
    "#                         hyperparameters,\n",
    "#                         episodes_to_run,\n",
    "#                         env,\n",
    "#                         actions,\n",
    "#                         random_state,\n",
    "#                         max_iter\n",
    "#                     )\n",
    "#                     WallTime = time.time() - start\n",
    "\n",
    "#                     env.close()\n",
    "\n",
    "#                     # Guardamos los resultados\n",
    "#                     runType = f'DynaQ-{policy.__name__}'\n",
    "#                     runPar = f'its-{max_iter}_eps-{episodes_to_run}'\n",
    "#                     runHPar = f'a-{hyperparameters[\"alpha\"]}_g-{hyperparameters[\"gamma\"]}_e-{hyperparameters[\"epsilon\"]}_s-{hyperparameters[\"steps\"]}'\n",
    "\n",
    "#                     with open(f'Outputs/Lab2/{runType}_{runPar}_{runHPar}.csv', 'w') as f:\n",
    "#                         f.write('Goal\\tDrop\\tEarly\\tTotal\\n')\n",
    "#                         f.write(f'{ep_goal}\\t{ep_drop}\\t{ep_early}\\t{episodes_to_run}\\n')\n",
    "\n",
    "#                         f.write(f'Wall Time[s]\\t{WallTime}\\n')\n",
    "                        \n",
    "#                         f.write('State\\tAction\\tQ-value\\n')\n",
    "#                         for s in range(37):\n",
    "#                             for a in range(4):\n",
    "#                                 f.write(f'{s}\\t{a}\\t{q[(s,a)]:.6f}\\n')\n",
    "\n",
    "#                         f.write('Return\\tTimeSteps\\n')\n",
    "#                         for i in range(episodes_to_run):\n",
    "#                             f.write(f'{ep_return[i]:.0f}\\t{ep_timesteps[i]:.0f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# File = pd.read_csv(f'Outputs/Lab2/SARSA-epGreedy_its-2000_eps-100000_a-0.05_g-1_e-0.005.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "\n",
    "# axs[0].plot(mean_evol(File[:,0]))#, label=rf'$\\gamma$ = {p_g}')\n",
    "# axs[1].plot(mean_evol(File[:,1]))#, label=rf'$\\gamma$ = {p_g}')\n",
    "\n",
    "# # axs[0].set_ylim(-18, -12)\n",
    "# # axs[1].set_ylim(12, 18)\n",
    "\n",
    "# # axs[0].axhline(-13, color='red', linestyle=':', label='Óptimo')\n",
    "# # axs[0].axhline(-17, color='blue', linestyle=':', label='Seguro')\n",
    "\n",
    "# axs[0].set_xlabel('Episodios')\n",
    "# axs[1].set_xlabel('Episodios')\n",
    "\n",
    "# # axs[1].axhline(13, color='red', linestyle=':', label='Óptimo')\n",
    "# # axs[1].axhline(17, color='blue', linestyle=':', label='Seguro')\n",
    "\n",
    "# axs[0].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "# axs[1].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "\n",
    "# axs[0].set_ylabel('Retorno promedio')\n",
    "# axs[1].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "# axs[0].legend(loc='lower right', frameon=False, fancybox=True, title='Camino')\n",
    "# axs[1].legend(loc='upper right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "# plt.savefig('Outputs/Lab2/SARSA-epGreedy_its-2000_eps-100_a-0.05_g-1_e-0.005.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6Ci7JfDdaPs/g38IjJj7A",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
