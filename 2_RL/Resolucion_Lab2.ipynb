{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
    "\n",
    "## Aprendizaje por refuerzos\n",
    "\n",
    "### Trabajo práctico entregable 2/2 (materia completa)\n",
    "\n",
    "**Estudiante:**\n",
    "- [Chevallier-Boutell, Ignacio José.](https://www.linkedin.com/in/nachocheva/)\n",
    "\n",
    "**Docentes:**\n",
    "- Palombarini, Jorge (Mercado Libre).\n",
    "- Barsce, Juan Cruz (Mercado Libre).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import time\n",
    "\n",
    "# from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Traemos las funciones ya definidas y utilizadas en el primer laboratorio\n",
    "from utyls_RL import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los videos de las ejecuciones hay que tener instalado ffmpeg (`apt-get install ffmpeg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio 1\n",
    "\n",
    "Crear un entorno propio y entrenar agentes de RL en el mismo, utilizando diferentes algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheva's Odyssey: reglas del juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mapa del juego consiste en una grilla 6x6, como se muestra a continuación. Al comenzar un episodio, el agente se ubica en la posición $S$ (elemento [5, 5]) y debe llegar hasta $G$ (elemento [0, 2]) para terminar dicho episodio. El agente debe realizar tantos movimientos como sean necesarios para llegar a la meta y finalizar el episodio.\n",
    "\n",
    "|J|A|G|A|~|~|\n",
    "|-|-|-|-|-|-|\n",
    "|~|A|A|A|~|~|\n",
    "|~|~|~|~|~|~|\n",
    "|~|~|~|~|~|~|\n",
    "|A|A|~|~|~|~|\n",
    "|P|A|~|~|~|S|\n",
    "\n",
    "El espacio de estados $\\mathcal{S}$ tiene 36 elementos. Para calcular el valor del estado asociado a cada elemento del mapa debemos calcular\n",
    "    $$\\text{Fila Actual} \\times \\text{Número de Columnas} + \\text{Columna Actual}$$\n",
    "\n",
    "donde debemos contar desde 0. De esta manera, el estado inicial $S$ es el estado 35 y el estado terminal $G$ es el estado 2.\n",
    "\n",
    "El espacio de acciones $\\mathcal{A}$ tiene 4 elementos para todos los estados que no están en el borde del mapa:\n",
    "- 0 $\\Rightarrow$ Se mueve hacia arriba.\n",
    "- 1 $\\Rightarrow$ Se mueve hacia la derecha.\n",
    "- 2 $\\Rightarrow$ Se mueve hacia abajo.\n",
    "- 3 $\\Rightarrow$ Se mueve hacia la izquierda.\n",
    "\n",
    "Para los estados que están en los bordes, sólo se puede elegir entre 2 acciones si están en los vértices o entre 3 si están en las aristas, según corresponda. Observamos que se cumple que $\\mathcal{A}, \\mathcal{S} \\in \\mathbb{N}$.\n",
    "\n",
    "Además de la salida $S$ y la meta $G$, tenemos otros elementos en el mapa. Los elementos vacíos (~) representan pasto, mientras que los elementos con $A$ son agua. El elemento $J$ es un jetpack y el elemento $P$ representa un premio extra. A partir de esto, la función de recompensa es tal que el agente recibe:\n",
    "- $-1$ cuando ingresa en a un elemento con pasto o cuando busca el jetpack o el premio extra. Sus efectos no se pierden, pero tampoco se acumulan.\n",
    "- $-8$ cuando ingresa en a un elemento con agua. En caso de contar con el jetpack, el costo por pasar por el agua se reduce a $-2$.\n",
    "- $+0$ si alcanza la meta sin el premio extra y $+24$ cuando la alcanza con el premio extra.\n",
    "\n",
    "Hay principalmente 5 caminos relevantes:\n",
    "- **SG:** Ir directo a la meta requiere 8 pasos temporales, otorgando -14 puntos.\n",
    "- **SJG:** Buscar el jetpack e ir a la meta requiere 12 pasos temporales, otorgando -12 puntos.\n",
    "- **SPG:** Buscar el premio extra e ir a la meta requiere 12 pasos temporales, otorgando -8 puntos.\n",
    "- **SPJG:** Buscar el premio extra, luego el jetpack e ir a la meta requiere 12 pasos temporales, otorgando -2 puntos.\n",
    "- **SJPG:** Buscar el jetpack, luego el premio extra e ir a la meta requiere 22 pasos temporales, otorgando 0 puntos (puntuación máxima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la clase del agente asociado al juego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChevasOdyssey(gym.Env):\n",
    "    # Tipo de renderizado posible, además de None.\n",
    "    metadata = {\"render.modes\": [\"console\"]}\n",
    "\n",
    "    # Definimos los valores de las acciones\n",
    "    UP = 0\n",
    "    RIGHT = 1\n",
    "    DOWN = 2\n",
    "    LEFT = 3\n",
    "\n",
    "    # Efectos del Jetpack y el premio\n",
    "    JETPACK = 1\n",
    "    PREMIO = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ChevasOdyssey, self).__init__()\n",
    "\n",
    "        # Tamaño del mapa\n",
    "        self.grid_shape = (6, 6)\n",
    "\n",
    "        # Espacio de acciones\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Espacio de estados\n",
    "        self.observation_space = spaces.Discrete(36)\n",
    "\n",
    "        # Inicializamos en agente en la salida\n",
    "        self.agent_pos = 35\n",
    "\n",
    "        # Ubicamos el jetpack\n",
    "        self.jetpack_pos = 0\n",
    "\n",
    "        # Ubicamos el premio extra\n",
    "        self.extra_pos = 30\n",
    "\n",
    "        # Ubicamos la meta\n",
    "        self.goal_pos = 2\n",
    "\n",
    "        # Ubicamos el agua\n",
    "        self.water_pos = [1, 3, 7, 8, 9, 24, 25, 31]\n",
    "\n",
    "    def reset(self, seed=None) -> Tuple[np.array, dict]:\n",
    "        \"\"\"\n",
    "        Reinicia el ambiente y devuelve la observación inicial\n",
    "        \"\"\"\n",
    "\n",
    "        # Inicializamos en agente en el punto de partida\n",
    "        self.agent_pos = 35\n",
    "\n",
    "        self.JETPACK = 1\n",
    "        \n",
    "        self.PREMIO = 0\n",
    "\n",
    "        return (self.agent_pos, {})\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.UP:\n",
    "            self.agent_pos -= 6\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        elif action == self.DOWN:\n",
    "            self.agent_pos += 6\n",
    "        elif action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Se recibió una acción inválida={action} que no es parte del\\\n",
    "                    espacio de acciones\"\n",
    "            )\n",
    "\n",
    "        # Evitamos que el agente se salga de los límites de la grilla\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 36)\n",
    "\n",
    "        # Consigue el jetpack\n",
    "        if self.agent_pos == self.jetpack_pos:\n",
    "            self.JETPACK = 4\n",
    "\n",
    "        # Consigue el premio extra\n",
    "        if self.agent_pos == self.extra_pos:\n",
    "            self.PREMIO = 24\n",
    "\n",
    "        # Llegada a la meta\n",
    "        terminated = bool(self.agent_pos == self.goal_pos)\n",
    "        truncated = False\n",
    "\n",
    "        # Asignación de recompensa\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = self.PREMIO\n",
    "        elif self.agent_pos in  self.water_pos:\n",
    "            reward = -8 / self.JETPACK\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # Información adicional\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            self.agent_pos,\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        print('Not implemented yet.')\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validamos que el ambiente cumpla con la interfaz de gym. Si el entorno no cumple con la interfaz, se lanzará una excepción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = ChevasOdyssey()\n",
    "# check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaciones\n",
    "\n",
    "Se utilizan las 3 combinaciones que mejores resultados arrojaron en el primer laboratoio:\n",
    "* SARSA con SoftMax.\n",
    "* Q-learning con $\\epsilon$-greedy.\n",
    "* Dyna-Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChevasOdyssey()\n",
    "actions = range(env.action_space.n)\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA con SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = SoftMax\n",
    "\n",
    "# Me quedo con 0.1\n",
    "Sw_a = [0.1] # [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "# Me quedo con 0.45\n",
    "Sw_g = [0.45] # [0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.9, 1]\n",
    "# Me quedo con 0.03. Me tira prob Nan con 0.005 y 0.01\n",
    "Sw_t = [0.03] # [0.03, 0.05, 0.1, 0.5, 1]\n",
    "# Me quedo con 100k donde ya estabiliza\n",
    "Sw_eps = [100000] # [20000, 50000, 100000, 150000, 200000]\n",
    "\n",
    "for p_eps in Sw_eps:\n",
    "    episodes_to_run = p_eps\n",
    "    for p_a in Sw_a:\n",
    "        for p_g in Sw_g:\n",
    "            for p_t in Sw_t:\n",
    "                hyperparameters = {\n",
    "                    \"alpha\": p_a,\n",
    "                    \"gamma\": p_g,\n",
    "                    \"tau\": p_t,\n",
    "                }\n",
    "\n",
    "                random_state = np.random.RandomState(1994)\n",
    "\n",
    "                start = time.time()\n",
    "                ep_timesteps, ep_return, ep_goal, ep_drop, ep_early, q = run_SARSA(\n",
    "                    policy,\n",
    "                    hyperparameters,\n",
    "                    episodes_to_run,\n",
    "                    env,\n",
    "                    actions,\n",
    "                    random_state,\n",
    "                    max_iter\n",
    "                )\n",
    "                WallTime = time.time() - start\n",
    "\n",
    "                env.close()\n",
    "\n",
    "                # Guardamos los resultados\n",
    "                runType = f'SARSA-{policy.__name__}'\n",
    "                runPar = f'its-{max_iter}_eps-{episodes_to_run}'\n",
    "                runHPar = f'a-{hyperparameters[\"alpha\"]}_g-{hyperparameters[\"gamma\"]}_t-{hyperparameters[\"tau\"]}'\n",
    "\n",
    "                with open(f'Outputs/Lab2/{runType}/{runPar}_{runHPar}.csv', 'w') as f:\n",
    "                    f.write('Goal\\tDrop\\tEarly\\tTotal\\n')\n",
    "                    f.write(f'{ep_goal}\\t{ep_drop}\\t{ep_early}\\t{episodes_to_run}\\n')\n",
    "\n",
    "                    f.write(f'Wall Time[s]\\t{WallTime}\\n')\n",
    "                    \n",
    "                    f.write('State\\tAction\\tQ-value\\n')\n",
    "                    for s in range(37):\n",
    "                        for a in range(4):\n",
    "                            f.write(f'{s}\\t{a}\\t{q[(s,a)]:.6f}\\n')\n",
    "\n",
    "                    f.write('Return\\tTimeSteps\\n')\n",
    "                    for i in range(episodes_to_run):\n",
    "                        f.write(f'{ep_return[i]:.0f}\\t{ep_timesteps[i]:.0f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# for p_a in [0.05, 0.1, 0.15, 0.2, 0.25]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2/SARSA-SoftMax/Alpha/its-1000_eps-20000_a-{p_a}_g-1_t-0.05.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "\n",
    "# for p_g in [0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.9, 1]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2/Q-learning-EpGreedy/Gamma/its-1000_eps-20000_a-0.1_g-{p_g}_t-0.05.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\gamma$ = {p_g}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\gamma$ = {p_g}')\n",
    "\n",
    "# for p_t in [0.03, 0.05, 0.1, 0.5, 1]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2/SARSA-SoftMax/Tau/its-1000_eps-20000_a-0.1_g-0.45_t-{p_t}.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\tau$ = {p_t}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\tau$ = {p_t}')\n",
    "\n",
    "for eps in [200000, 150000, 100000, 50000, 20000]:\n",
    "    File = pd.read_csv(f'Outputs/Lab2/SARSA-SoftMax/Episodios/its-1000_eps-{eps}_a-0.1_g-0.45_t-0.03.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "    axs[0].plot(mean_evol(File[:,0]))\n",
    "    axs[1].plot(mean_evol(File[:,1]))\n",
    "\n",
    "axs[0].set_ylim(-30, 1)\n",
    "axs[1].set_ylim(7, 38)\n",
    "\n",
    "axs[0].axhline(-14, color='red', linestyle=':', label='SG')\n",
    "axs[0].axhline(-12, color='blue', linestyle=':', label='SJG')\n",
    "axs[0].axhline(-8, color='red', linestyle='-.', label='SPG')\n",
    "axs[0].axhline(-2, color='blue', linestyle='-.', label='SPJG')\n",
    "axs[0].axhline(0, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].set_xlabel('Episodios')\n",
    "axs[1].set_xlabel('Episodios')\n",
    "\n",
    "axs[1].axhline(8, color='red', linestyle=':', label='SG')\n",
    "axs[1].axhline(12, color='blue', linestyle=':', label='SJG | SPG | SPJG')\n",
    "axs[1].axhline(22, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "axs[1].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "\n",
    "axs[0].set_ylabel('Retorno promedio')\n",
    "axs[1].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "axs[0].legend(loc='lower right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "axs[1].legend(loc='upper right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "\n",
    "# plt.savefig('Outputs/Lab2/SARSA-SoftMax/Alpha/its-1000_eps-20000_g-1_t-0.05.png')\n",
    "# plt.savefig('Outputs/Lab2/SARSA-SoftMax/Gamma/its-1000_eps-20000_a-0.1_t-0.05.png')\n",
    "# plt.savefig('Outputs/Lab2/SARSA-SoftMax/Tau/its-1000_eps-20000_a-0.1_g-0.45.png')\n",
    "plt.savefig('Outputs/Lab2/SARSA-SoftMax/Episodios/its-1000_a-0.1_g-0.45_t-0.03.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q y Q-learning con $\\epsilon$-greedy\n",
    "\n",
    "Cuando se quiera hacer Q-learning con $\\epsilon$-greedy, se usa Dyna-Q con 0 pasos de planificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning con $\\epsilon$-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = epGreedy\n",
    "\n",
    "# Me quedo con 0.1\n",
    "Sw_a = [0.1] # [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "# Me quedo con 0.65\n",
    "Sw_g = [0.65] # [0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.9, 1]\n",
    "# Me quedo con 0.001\n",
    "Sw_e = [0.001] # [0.001, 0.005, 0.01, 0.05]\n",
    "# Me quedo con 100k donde ya estabiliza\n",
    "Sw_eps = [100000] # [20000, 50000, 100000, 150000, 200000]\n",
    "Sw_s = [0]\n",
    "\n",
    "for p_eps in Sw_eps:\n",
    "    episodes_to_run = p_eps\n",
    "    for p_a in Sw_a:\n",
    "        for p_g in Sw_g:\n",
    "            for p_e in Sw_e:\n",
    "                for p_s in Sw_s:\n",
    "                    hyperparameters = {\n",
    "                        \"alpha\": p_a,\n",
    "                        \"gamma\": p_g,\n",
    "                        \"epsilon\": p_e,\n",
    "                        \"steps\": p_s\n",
    "                    }\n",
    "                    print(hyperparameters)\n",
    "\n",
    "                    random_state = np.random.RandomState(1994)\n",
    "\n",
    "                    start = time.time()\n",
    "                    ep_timesteps, ep_return, ep_goal, ep_drop, ep_early, q = run_DynaQ(\n",
    "                        policy,\n",
    "                        hyperparameters,\n",
    "                        episodes_to_run,\n",
    "                        env,\n",
    "                        actions,\n",
    "                        random_state,\n",
    "                        max_iter\n",
    "                    )\n",
    "                    WallTime = time.time() - start\n",
    "\n",
    "                    env.close()\n",
    "\n",
    "                    # Guardamos los resultados\n",
    "                    runType = 'Q-learning-EpGreedy'\n",
    "                    runPar = f'its-{max_iter}_eps-{episodes_to_run}'\n",
    "                    runHPar = f'a-{hyperparameters[\"alpha\"]}_g-{hyperparameters[\"gamma\"]}_e-{hyperparameters[\"epsilon\"]}'\n",
    "\n",
    "                    with open(f'Outputs/Lab2/{runType}/{runPar}_{runHPar}.csv', 'w') as f:\n",
    "                        f.write('Goal\\tDrop\\tEarly\\tTotal\\n')\n",
    "                        f.write(f'{ep_goal}\\t{ep_drop}\\t{ep_early}\\t{episodes_to_run}\\n')\n",
    "\n",
    "                        f.write(f'Wall Time[s]\\t{WallTime}\\n')\n",
    "                        \n",
    "                        f.write('State\\tAction\\tQ-value\\n')\n",
    "                        for s in range(37):\n",
    "                            for a in range(4):\n",
    "                                f.write(f'{s}\\t{a}\\t{q[(s,a)]:.6f}\\n')\n",
    "\n",
    "                        f.write('Return\\tTimeSteps\\n')\n",
    "                        for i in range(episodes_to_run):\n",
    "                            f.write(f'{ep_return[i]:.0f}\\t{ep_timesteps[i]:.0f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].set_ylim(-30, 1)\n",
    "axs[1].set_ylim(7, 38)\n",
    "\n",
    "axs[0].axhline(-14, color='red', linestyle=':', label='SG')\n",
    "axs[0].axhline(-12, color='blue', linestyle=':', label='SJG')\n",
    "axs[0].axhline(-8, color='red', linestyle='-.', label='SPG')\n",
    "axs[0].axhline(-2, color='blue', linestyle='-.', label='SPJG')\n",
    "axs[0].axhline(0, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].set_xlabel('Episodios')\n",
    "axs[1].set_xlabel('Episodios')\n",
    "\n",
    "axs[1].axhline(8, color='red', linestyle=':', label='SG')\n",
    "axs[1].axhline(12, color='blue', linestyle=':', label='SJG | SPG | SPJG')\n",
    "axs[1].axhline(22, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "axs[1].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "\n",
    "axs[0].set_ylabel('Retorno promedio')\n",
    "axs[1].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "# for p_a in [0.05, 0.1, 0.15, 0.2, 0.25]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2/Q-learning-EpGreedy/Alpha/its-1000_eps-20000_a-{p_a}_g-1_e-0.05.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "\n",
    "# for p_g in [0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.9, 1]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2/Q-learning-EpGreedy/Gamma/its-1000_eps-20000_a-0.1_g-{p_g}_e-0.05.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\gamma$ = {p_g}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\gamma$ = {p_g}')\n",
    "\n",
    "# for p_e in [0.05, 0.01, 0.005, 0.001]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2/Q-learning-EpGreedy/Epsilon/its-1000_eps-20000_a-0.1_g-0.65_e-{p_e}.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\epsilon$ = {p_e}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\epsilon$ = {p_e}')\n",
    "\n",
    "for eps in [200000, 150000, 100000, 50000, 20000]:\n",
    "    File = pd.read_csv(f'Outputs/Lab2/Q-learning-EpGreedy/Episodios/its-1000_eps-{eps}_a-0.1_g-0.65_e-0.001.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "    axs[0].plot(mean_evol(File[:,0]))\n",
    "    axs[1].plot(mean_evol(File[:,1]))\n",
    "\n",
    "axs[0].legend(loc='lower right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "axs[1].legend(loc='upper right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "\n",
    "# plt.savefig('Outputs/Lab2/Q-learning-EpGreedy/Alpha/its-1000_eps-20000_g-1_e-0.05.png')\n",
    "# plt.savefig('Outputs/Lab2/Q-learning-EpGreedy/Gamma/its-1000_eps-20000_a-0.1_e-0.05.png')\n",
    "# plt.savefig('Outputs/Lab2/Q-learning-EpGreedy/Epsilon/its-1000_eps-20000_a-0.1_g-0.65.png')\n",
    "plt.savefig('Outputs/Lab2/Q-learning-EpGreedy/Episodios/its-1000_a-0.1_g-0.65_e-0.001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dyna-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = epGreedy\n",
    "\n",
    "# Me quedo con 10\n",
    "Sw_s = [10] # [10, 20, 30, 40, 50, 60, 70]\n",
    "# Me quedo con 0.1\n",
    "Sw_a = [0.1] # [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "# Me quedo con 0.9. 0.55 y 0.65 no funciona\n",
    "Sw_g = [0.9] # [0.75, 0.85, 0.9, 1]\n",
    "# Me quedo con 0.05. 0.001 no funciona 0.005 tampoco 0.01 tampoco. con 0.5 también falla bastante\n",
    "Sw_e = [0.05] # [0.05, 0.1]\n",
    "# Me quedo con \n",
    "Sw_eps = [20000, 50000, 100000, 150000, 200000]\n",
    "\n",
    "for p_eps in Sw_eps:\n",
    "    episodes_to_run = p_eps\n",
    "    for p_a in Sw_a:\n",
    "        for p_g in Sw_g:\n",
    "            for p_e in Sw_e:\n",
    "                for p_s in Sw_s:\n",
    "                    hyperparameters = {\n",
    "                        \"alpha\": p_a,\n",
    "                        \"gamma\": p_g,\n",
    "                        \"epsilon\": p_e,\n",
    "                        \"steps\": p_s\n",
    "                    }\n",
    "                    print(hyperparameters)\n",
    "\n",
    "                    random_state = np.random.RandomState(1994)\n",
    "\n",
    "                    start = time.time()\n",
    "                    ep_timesteps, ep_return, ep_goal, ep_drop, ep_early, q = run_DynaQ(\n",
    "                        policy,\n",
    "                        hyperparameters,\n",
    "                        episodes_to_run,\n",
    "                        env,\n",
    "                        actions,\n",
    "                        random_state,\n",
    "                        max_iter\n",
    "                    )\n",
    "                    WallTime = time.time() - start\n",
    "\n",
    "                    env.close()\n",
    "\n",
    "                    # Guardamos los resultados\n",
    "                    runType = f'Dyna-Q'\n",
    "                    runPar = f'its-{max_iter}_eps-{episodes_to_run}'\n",
    "                    runHPar = f'a-{hyperparameters[\"alpha\"]}_g-{hyperparameters[\"gamma\"]}_e-{hyperparameters[\"epsilon\"]}_s-{hyperparameters[\"steps\"]}'\n",
    "\n",
    "                    with open(f'Outputs/Lab2/{runType}/{runPar}_{runHPar}.csv', 'w') as f:\n",
    "                        f.write('Goal\\tDrop\\tEarly\\tTotal\\n')\n",
    "                        f.write(f'{ep_goal}\\t{ep_drop}\\t{ep_early}\\t{episodes_to_run}\\n')\n",
    "\n",
    "                        f.write(f'Wall Time[s]\\t{WallTime}\\n')\n",
    "                        \n",
    "                        f.write('State\\tAction\\tQ-value\\n')\n",
    "                        for s in range(37):\n",
    "                            for a in range(4):\n",
    "                                f.write(f'{s}\\t{a}\\t{q[(s,a)]:.6f}\\n')\n",
    "\n",
    "                        f.write('Return\\tTimeSteps\\n')\n",
    "                        for i in range(episodes_to_run):\n",
    "                            f.write(f'{ep_return[i]:.0f}\\t{ep_timesteps[i]:.0f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# for p_s in [10, 20, 30, 40, 50, 60, 70]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2/Dyna-Q/Steps/its-1000_eps-20000_a-0.1_g-1_e-0.05_s-{p_s}.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'Plan = {p_s}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'Plan = {p_s}')\n",
    "\n",
    "# for p_a in [0.05, 0.1, 0.15, 0.2, 0.25]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2/Dyna-Q/Alpha/its-1000_eps-20000_a-{p_a}_g-1_e-0.05_s-10.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "\n",
    "# for p_g in [0.75, 0.85, 0.9, 1]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2/Dyna-Q/Gamma/its-1000_eps-20000_a-0.1_g-{p_g}_e-0.05_s-10.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\gamma$ = {p_g}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\gamma$ = {p_g}')\n",
    "\n",
    "# for p_e in [0.05, 0.1]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2/Dyna-Q/Epsilon/its-1000_eps-20000_a-0.1_g-0.9_e-{p_e}_s-10.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\epsilon$ = {p_e}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\epsilon$ = {p_e}')\n",
    "\n",
    "for eps in [200000, 150000, 100000, 50000, 20000]:\n",
    "    File = pd.read_csv(f'Outputs/Lab2/Dyna-Q/Episodios/its-1000_eps-{eps}_a-0.1_g-0.9_e-0.05_s-10.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "    axs[0].plot(mean_evol(File[:,0]))\n",
    "    axs[1].plot(mean_evol(File[:,1]))\n",
    "\n",
    "# axs[0].set_ylim(-30, 1)\n",
    "# axs[1].set_ylim(7, 38)\n",
    "axs[0].set_ylim(-15, 1)\n",
    "axs[1].set_ylim(7, 25)\n",
    "\n",
    "axs[0].axhline(-14, color='red', linestyle=':', label='SG')\n",
    "axs[0].axhline(-12, color='blue', linestyle=':', label='SJG')\n",
    "axs[0].axhline(-8, color='red', linestyle='-.', label='SPG')\n",
    "axs[0].axhline(-2, color='blue', linestyle='-.', label='SPJG')\n",
    "axs[0].axhline(0, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].set_xlabel('Episodios')\n",
    "axs[1].set_xlabel('Episodios')\n",
    "\n",
    "axs[1].axhline(8, color='red', linestyle=':', label='SG')\n",
    "axs[1].axhline(12, color='blue', linestyle=':', label='SJG | SPG | SPJG')\n",
    "axs[1].axhline(22, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "axs[1].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "\n",
    "axs[0].set_ylabel('Retorno promedio')\n",
    "axs[1].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "axs[0].legend(loc='lower right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "axs[1].legend(loc='upper right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "\n",
    "# plt.savefig('Outputs/Lab2/Dyna-Q/Steps/its-1000_eps-20000_a-0.1_g-1_e-0.05.png')\n",
    "# plt.savefig('Outputs/Lab2/Dyna-Q/Alpha/its-1000_eps-20000_g-1_e-0.05_s-10.png')\n",
    "# plt.savefig('Outputs/Lab2/Dyna-Q/Gamma/its-1000_eps-20000_a-0.1_e-0.05_s-10.png')\n",
    "# plt.savefig('Outputs/Lab2/Dyna-Q/Epsilon/its-1000_eps-20000_a-0.1_g-0.9_s-10.png')\n",
    "plt.savefig('Outputs/Lab2/Dyna-Q/Episodios/its-1000_a-0.1_g-0.9_e-0.05_s-10.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "\n",
    "# Dyna-Q\n",
    "File = pd.read_csv(f'Outputs/Lab2/Dyna-Q/Episodios/its-1000_eps-100000_a-0.1_g-0.9_e-0.05_s-10.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "axs[0].plot(mean_evol(File[:,0]), label='Dyna-Q')\n",
    "axs[1].plot(mean_evol(File[:,1]), label='Dyna-Q')\n",
    "\n",
    "# Q-learning + Eps Greedy\n",
    "File = pd.read_csv(f'Outputs/Lab2/Q-learning-EpGreedy/Episodios/its-1000_eps-100000_a-0.1_g-0.65_e-0.001.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "axs[0].plot(mean_evol(File[:,0]), label='Q-learning+EpsGreedy')\n",
    "axs[1].plot(mean_evol(File[:,1]), label='Q-learning+EpsGreedy')\n",
    "\n",
    "# SARSA + SoftMax\n",
    "File = pd.read_csv(f'Outputs/Lab2/SARSA-SoftMax/Episodios/its-1000_eps-100000_a-0.1_g-0.45_t-0.03.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "axs[0].plot(mean_evol(File[:,0]), label='SARSA+SoftMax')\n",
    "axs[1].plot(mean_evol(File[:,1]), label='SARSA+SoftMax')\n",
    "\n",
    "axs[0].set_ylim(-15, 1)\n",
    "axs[1].set_ylim(7, 25)\n",
    "\n",
    "axs[0].axhline(-14, color='red', linestyle=':', label='SG')\n",
    "axs[0].axhline(-12, color='blue', linestyle=':', label='SJG')\n",
    "axs[0].axhline(-8, color='red', linestyle='-.', label='SPG')\n",
    "axs[0].axhline(-2, color='blue', linestyle='-.', label='SPJG')\n",
    "axs[0].axhline(0, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].set_xlabel('Episodios')\n",
    "axs[1].set_xlabel('Episodios')\n",
    "\n",
    "axs[1].axhline(8, color='red', linestyle=':', label='SG')\n",
    "axs[1].axhline(12, color='blue', linestyle=':', label='SJG | SPG | SPJG')\n",
    "axs[1].axhline(22, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "axs[1].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "\n",
    "axs[0].set_ylabel('Retorno promedio')\n",
    "axs[1].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "axs[0].legend(loc='lower right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "axs[1].legend(loc='upper right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "\n",
    "plt.savefig('Outputs/Lab2/Ej_1_ComparacionFinal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio 2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6Ci7JfDdaPs/g38IjJj7A",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
