{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
    "\n",
    "## Aprendizaje por refuerzos\n",
    "\n",
    "### Trabajo práctico entregable 2/2 (materia completa)\n",
    "\n",
    "**Estudiante:**\n",
    "- [Chevallier-Boutell, Ignacio José.](https://www.linkedin.com/in/nachocheva/)\n",
    "\n",
    "**Docentes:**\n",
    "- Palombarini, Jorge (Mercado Libre).\n",
    "- Barsce, Juan Cruz (Mercado Libre).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los videos de las ejecuciones hay que tener instalado ffmpeg (`apt-get install ffmpeg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio 1\n",
    "\n",
    "Crear un entorno propio y entrenar agentes de RL en el mismo, utilizando diferentes algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheva's Odyssey: reglas del juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mapa del juego consiste en una grilla 6x6, como se muestra a continuación. Al comenzar un episodio, el agente se ubica en la posición $S$ (elemento [5, 5]) y debe llegar hasta $G$ (elemento [0, 2]) para terminar dicho episodio. El agente debe realizar tantos movimientos como sean necesarios para llegar a la meta y finalizar el episodio.\n",
    "\n",
    "|J|A|G|A|~|~|\n",
    "|-|-|-|-|-|-|\n",
    "|~|A|A|A|~|~|\n",
    "|~|~|~|~|~|~|\n",
    "|~|~|~|~|~|~|\n",
    "|A|A|~|~|~|~|\n",
    "|P|A|~|~|~|S|\n",
    "\n",
    "El espacio de estados $\\mathcal{S}$ tiene 36 elementos. Para calcular el valor del estado asociado a cada elemento del mapa debemos calcular\n",
    "    $$\\text{Fila Actual} \\times \\text{Número de Columnas} + \\text{Columna Actual}$$\n",
    "\n",
    "donde debemos contar desde 0. De esta manera, el estado inicial $S$ es el estado 35 y el estado terminal $G$ es el estado 2.\n",
    "\n",
    "El espacio de acciones $\\mathcal{A}$ tiene 4 elementos para todos los estados que no están en el borde del mapa:\n",
    "- 0 $\\Rightarrow$ Se mueve hacia arriba.\n",
    "- 1 $\\Rightarrow$ Se mueve hacia la derecha.\n",
    "- 2 $\\Rightarrow$ Se mueve hacia abajo.\n",
    "- 3 $\\Rightarrow$ Se mueve hacia la izquierda.\n",
    "\n",
    "Para los estados que están en los bordes, sólo se puede elegir entre 2 acciones si están en los vértices o entre 3 si están en las aristas, según corresponda. Observamos que se cumple que $\\mathcal{A}, \\mathcal{S} \\in \\mathbb{N}$.\n",
    "\n",
    "Además de la salida $S$ y la meta $G$, tenemos otros elementos en el mapa. Los elementos vacíos (~) representan pasto, mientras que los elementos con $A$ son agua. El elemento $J$ es un jetpack y el elemento $P$ representa un premio extra. A partir de esto, la función de recompensa es tal que el agente recibe:\n",
    "- $-1$ cuando ingresa en a un elemento con pasto o cuando busca el jetpack o el premio extra. Sus efectos no se pierden, pero tampoco se acumulan.\n",
    "- $-8$ cuando ingresa en a un elemento con agua. En caso de contar con el jetpack, el costo por pasar por el agua se reduce a $-2$.\n",
    "- $+0$ si alcanza la meta sin el premio extra y $+24$ cuando la alcanza con el premio extra.\n",
    "\n",
    "Hay principalmente 5 caminos relevantes:\n",
    "- **SG:** Ir directo a la meta requiere 8 pasos temporales, otorgando -14 puntos.\n",
    "- **SJG:** Buscar el jetpack e ir a la meta requiere 12 pasos temporales, otorgando -12 puntos.\n",
    "- **SPG:** Buscar el premio extra e ir a la meta requiere 12 pasos temporales, otorgando -8 puntos.\n",
    "- **SPJG:** Buscar el premio extra, luego el jetpack e ir a la meta requiere 12 pasos temporales, otorgando -2 puntos.\n",
    "- **SJPG:** Buscar el jetpack, luego el premio extra e ir a la meta requiere 22 pasos temporales, otorgando 0 puntos (puntuación máxima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChevasOdyssey(gym.Env):\n",
    "    # Tipo de renderizado posible, además de None.\n",
    "    metadata = {\"render.modes\": [\"console\"]}\n",
    "\n",
    "    # Definimos los valores de las acciones\n",
    "    UP = 0\n",
    "    RIGHT = 1\n",
    "    DOWN = 2\n",
    "    LEFT = 3\n",
    "\n",
    "    # Efectos del Jetpack y el premio\n",
    "    JETPACK = 1\n",
    "    PREMIO = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ChevasOdyssey, self).__init__()\n",
    "\n",
    "        # Tamaño del mapa\n",
    "        self.grid_shape = (6, 6)\n",
    "\n",
    "        # Espacio de acciones\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Espacio de estados\n",
    "        self.observation_space = spaces.Discrete(np.prod(self.grid_shape))\n",
    "\n",
    "        # Inicializamos en agente en la salida\n",
    "        self.agent_pos = 35\n",
    "\n",
    "        # Ubicamos el jetpack\n",
    "        self.jetpack_pos = 0\n",
    "\n",
    "        # Ubicamos el premio extra\n",
    "        self.extra_pos = 30\n",
    "\n",
    "        # Ubicamos la meta\n",
    "        self.goal_pos = 2\n",
    "\n",
    "        # Ubicamos el agua\n",
    "        self.water_pos = [1, 3, 7, 8, 9, 24, 25, 31]\n",
    "\n",
    "    def reset(self, seed=None) -> Tuple[np.array, dict]:\n",
    "        \"\"\"\n",
    "        Reinicia el ambiente y devuelve la observación inicial\n",
    "        \"\"\"\n",
    "\n",
    "        # Inicializamos en agente en el punto de partida\n",
    "        self.agent_pos = 35\n",
    "\n",
    "        return (np.array([self.agent_pos]).astype(int), {})\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.UP:\n",
    "            self.agent_pos -= 6\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        elif action == self.DOWN:\n",
    "            self.agent_pos += 6\n",
    "        elif action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Se recibió una acción inválida={action} que no es parte del\\\n",
    "                    espacio de acciones\"\n",
    "            )\n",
    "\n",
    "        # Evitamos que el agente se salga de los límites de la grilla\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 36)\n",
    "\n",
    "        # Consigue el jetpack\n",
    "        if self.agent_pos == self.jetpack_pos:\n",
    "            self.JETPACK = 4\n",
    "\n",
    "        # Consigue el premio extra\n",
    "        if self.agent_pos == self.extra_pos:\n",
    "            self.PREMIO = 24\n",
    "\n",
    "        # Llegada a la meta\n",
    "        terminated = bool(self.agent_pos == self.goal_pos)\n",
    "        truncated = False\n",
    "\n",
    "        # Asignación de recompensa\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = self.PREMIO\n",
    "        elif self.agent_pos in  self.water_pos:\n",
    "            reward = -8 / self.JETPACK\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # Información adicional\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(int),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        print('Not implemented yet.')\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6Ci7JfDdaPs/g38IjJj7A",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
