{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
    "\n",
    "## Aprendizaje por refuerzos\n",
    "\n",
    "### Trabajo práctico entregable 2/2 (materia completa)\n",
    "\n",
    "**Estudiante:**\n",
    "- [Chevallier-Boutell, Ignacio José.](https://www.linkedin.com/in/nachocheva/)\n",
    "\n",
    "**Docentes:**\n",
    "- Palombarini, Jorge (Mercado Libre).\n",
    "- Barsce, Juan Cruz (Mercado Libre).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import time\n",
    "\n",
    "# from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Traemos las funciones ya definidas y utilizadas en el primer laboratorio\n",
    "from utyls_RL import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio 1\n",
    "\n",
    "Crear un entorno propio y entrenar agentes de RL en el mismo, utilizando diferentes algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheva's Odyssey: reglas del juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mapa del juego consiste en una grilla 6x6, como se muestra a continuación. Al comenzar un episodio, el agente se ubica en la posición $S$ (elemento [5, 5]) y debe llegar hasta $G$ (elemento [0, 2]) para terminar dicho episodio. El agente debe realizar tantos movimientos como sean necesarios para llegar a la meta y finalizar el episodio.\n",
    "\n",
    "|J|A|G|A|~|~|\n",
    "|-|-|-|-|-|-|\n",
    "|~|A|A|A|~|~|\n",
    "|~|~|~|~|~|~|\n",
    "|~|~|~|~|~|~|\n",
    "|A|A|~|~|~|~|\n",
    "|P|A|~|~|~|S|\n",
    "\n",
    "El espacio de estados $\\mathcal{S}$ tiene 36 elementos. Para calcular el valor del estado asociado a cada elemento del mapa debemos calcular\n",
    "    $$\\text{Fila Actual} \\times \\text{Número de Columnas} + \\text{Columna Actual}$$\n",
    "\n",
    "donde debemos contar desde 0. De esta manera, el estado inicial $S$ es el estado 35 y el estado terminal $G$ es el estado 2.\n",
    "\n",
    "El espacio de acciones $\\mathcal{A}$ tiene 4 elementos para todos los estados:\n",
    "- 0 $\\Rightarrow$ Se mueve hacia arriba.\n",
    "- 1 $\\Rightarrow$ Se mueve hacia la derecha.\n",
    "- 2 $\\Rightarrow$ Se mueve hacia abajo.\n",
    "- 3 $\\Rightarrow$ Se mueve hacia la izquierda.\n",
    "\n",
    "Observamos que se cumple que $\\mathcal{A}, \\mathcal{S} \\in \\mathbb{N}$.\n",
    "\n",
    "Además de la salida $S$ y la meta $G$, tenemos otros elementos en el mapa. Los elementos vacíos (~) representan pasto, mientras que los elementos con $A$ son agua. El elemento $J$ es un jetpack y el elemento $P$ representa un premio extra. A partir de esto, la función de recompensa es tal que el agente recibe:\n",
    "- $-1$ cuando ingresa en a un elemento con pasto o cuando busca el jetpack o el premio extra. Sus efectos no se pierden, pero tampoco se acumulan.\n",
    "- $-8$ cuando ingresa en a un elemento con agua. En caso de contar con el jetpack, el costo por pasar por el agua se reduce a $-2$.\n",
    "- $+0$ si alcanza la meta sin el premio extra y $+24$ cuando la alcanza con el premio extra.\n",
    "\n",
    "Hay principalmente 5 caminos relevantes:\n",
    "- **SG:** Ir directo a la meta requiere 8 pasos temporales, otorgando -14 puntos.\n",
    "- **SJG:** Buscar el jetpack e ir a la meta requiere 12 pasos temporales, otorgando -12 puntos.\n",
    "- **SPG:** Buscar el premio extra e ir a la meta requiere 12 pasos temporales, otorgando -8 puntos.\n",
    "- **SPJG:** Buscar el premio extra, luego el jetpack e ir a la meta requiere 12 pasos temporales, otorgando -2 puntos.\n",
    "- **SJPG:** Buscar el jetpack, luego el premio extra e ir a la meta requiere 22 pasos temporales, otorgando 0 puntos (puntuación máxima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la clase del agente asociado al juego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChevasOdyssey(gym.Env):\n",
    "    # Tipo de renderizado posible, además de None.\n",
    "    metadata = {\"render.modes\": [\"console\"]}\n",
    "\n",
    "    # Definimos los valores de las acciones\n",
    "    UP = 0\n",
    "    RIGHT = 1\n",
    "    DOWN = 2\n",
    "    LEFT = 3\n",
    "\n",
    "    # Efectos del Jetpack y el premio\n",
    "    JETPACK = 1\n",
    "    PREMIO = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ChevasOdyssey, self).__init__()\n",
    "\n",
    "        # Tamaño del mapa\n",
    "        self.grid_shape = (6, 6)\n",
    "\n",
    "        # Espacio de acciones\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Espacio de estados\n",
    "        self.observation_space = spaces.Discrete(36)\n",
    "\n",
    "        # Inicializamos en agente en la salida\n",
    "        self.agent_pos = 35\n",
    "\n",
    "        # Ubicamos el jetpack\n",
    "        self.jetpack_pos = 0\n",
    "\n",
    "        # Ubicamos el premio extra\n",
    "        self.extra_pos = 30\n",
    "\n",
    "        # Ubicamos la meta\n",
    "        self.goal_pos = 2\n",
    "\n",
    "        # Ubicamos el agua\n",
    "        self.water_pos = [1, 3, 7, 8, 9, 24, 25, 31]\n",
    "\n",
    "    def reset(self, seed=None) -> Tuple[np.array, dict]:\n",
    "        \"\"\"\n",
    "        Reinicia el ambiente y devuelve la observación inicial\n",
    "        \"\"\"\n",
    "\n",
    "        # Inicializamos en agente en el punto de partida\n",
    "        self.agent_pos = 35\n",
    "\n",
    "        self.JETPACK = 1\n",
    "        \n",
    "        self.PREMIO = 0\n",
    "\n",
    "        return (self.agent_pos, {})\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.UP:\n",
    "            self.agent_pos -= 6\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        elif action == self.DOWN:\n",
    "            self.agent_pos += 6\n",
    "        elif action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Se recibió una acción inválida={action} que no es parte del\\\n",
    "                    espacio de acciones\"\n",
    "            )\n",
    "\n",
    "        # Evitamos que el agente se salga de los límites de la grilla\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 36)\n",
    "\n",
    "        # Consigue el jetpack\n",
    "        if self.agent_pos == self.jetpack_pos:\n",
    "            self.JETPACK = 4\n",
    "\n",
    "        # Consigue el premio extra\n",
    "        if self.agent_pos == self.extra_pos:\n",
    "            self.PREMIO = 24\n",
    "\n",
    "        # Llegada a la meta\n",
    "        terminated = bool(self.agent_pos == self.goal_pos)\n",
    "        truncated = False\n",
    "\n",
    "        # Asignación de recompensa\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = self.PREMIO\n",
    "        elif self.agent_pos in  self.water_pos:\n",
    "            reward = -8 / self.JETPACK\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        # Información adicional\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            self.agent_pos,\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        print('Not implemented yet.')\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validamos que el ambiente cumpla con la interfaz de gym. Si el entorno no cumple con la interfaz, se lanzará una excepción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = ChevasOdyssey()\n",
    "# check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaciones\n",
    "\n",
    "Se utilizan las 3 combinaciones que mejores resultados arrojaron en el primer laboratoio:\n",
    "* SARSA con SoftMax.\n",
    "* Q-learning con $\\epsilon$-greedy.\n",
    "* Dyna-Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChevasOdyssey()\n",
    "actions = range(env.action_space.n)\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA con SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = SoftMax\n",
    "\n",
    "# Me quedo con 0.1\n",
    "Sw_a = [0.1] # [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "# Me quedo con 0.45\n",
    "Sw_g = [0.45] # [0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.9, 1]\n",
    "# Me quedo con 0.03. Me tira prob Nan con 0.005 y 0.01\n",
    "Sw_t = [0.03] # [0.03, 0.05, 0.1, 0.5, 1]\n",
    "# Me quedo con 100k donde ya estabiliza\n",
    "Sw_eps = [100000] # [20000, 50000, 100000, 150000, 200000]\n",
    "\n",
    "for p_eps in Sw_eps:\n",
    "    episodes_to_run = p_eps\n",
    "    for p_a in Sw_a:\n",
    "        for p_g in Sw_g:\n",
    "            for p_t in Sw_t:\n",
    "                hyperparameters = {\n",
    "                    \"alpha\": p_a,\n",
    "                    \"gamma\": p_g,\n",
    "                    \"tau\": p_t,\n",
    "                }\n",
    "\n",
    "                random_state = np.random.RandomState(1994)\n",
    "\n",
    "                start = time.time()\n",
    "                ep_timesteps, ep_return, ep_goal, ep_drop, ep_early, q = run_SARSA(\n",
    "                    policy,\n",
    "                    hyperparameters,\n",
    "                    episodes_to_run,\n",
    "                    env,\n",
    "                    actions,\n",
    "                    random_state,\n",
    "                    max_iter\n",
    "                )\n",
    "                WallTime = time.time() - start\n",
    "\n",
    "                env.close()\n",
    "\n",
    "                # Guardamos los resultados\n",
    "                runType = f'SARSA-{policy.__name__}'\n",
    "                runPar = f'its-{max_iter}_eps-{episodes_to_run}'\n",
    "                runHPar = f'a-{hyperparameters[\"alpha\"]}_g-{hyperparameters[\"gamma\"]}_t-{hyperparameters[\"tau\"]}'\n",
    "\n",
    "                with open(f'Outputs/Lab2_Ej1/{runType}/{runPar}_{runHPar}.csv', 'w') as f:\n",
    "                    f.write('Goal\\tDrop\\tEarly\\tTotal\\n')\n",
    "                    f.write(f'{ep_goal}\\t{ep_drop}\\t{ep_early}\\t{episodes_to_run}\\n')\n",
    "\n",
    "                    f.write(f'Wall Time[s]\\t{WallTime}\\n')\n",
    "                    \n",
    "                    f.write('State\\tAction\\tQ-value\\n')\n",
    "                    for s in range(37):\n",
    "                        for a in range(4):\n",
    "                            f.write(f'{s}\\t{a}\\t{q[(s,a)]:.6f}\\n')\n",
    "\n",
    "                    f.write('Return\\tTimeSteps\\n')\n",
    "                    for i in range(episodes_to_run):\n",
    "                        f.write(f'{ep_return[i]:.0f}\\t{ep_timesteps[i]:.0f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# for p_a in [0.05, 0.1, 0.15, 0.2, 0.25]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2_Ej1/SARSA-SoftMax/Alpha/its-1000_eps-20000_a-{p_a}_g-1_t-0.05.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "\n",
    "# for p_g in [0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.9, 1]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2_Ej1/Q-learning-EpGreedy/Gamma/its-1000_eps-20000_a-0.1_g-{p_g}_t-0.05.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\gamma$ = {p_g}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\gamma$ = {p_g}')\n",
    "\n",
    "# for p_t in [0.03, 0.05, 0.1, 0.5, 1]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2_Ej1/SARSA-SoftMax/Tau/its-1000_eps-20000_a-0.1_g-0.45_t-{p_t}.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\tau$ = {p_t}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\tau$ = {p_t}')\n",
    "\n",
    "for eps in [200000, 150000, 100000, 50000, 20000]:\n",
    "    File = pd.read_csv(f'Outputs/Lab2_Ej1/SARSA-SoftMax/Episodios/its-1000_eps-{eps}_a-0.1_g-0.45_t-0.03.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "    axs[0].plot(mean_evol(File[:,0]))\n",
    "    axs[1].plot(mean_evol(File[:,1]))\n",
    "\n",
    "axs[0].set_ylim(-30, 1)\n",
    "axs[1].set_ylim(7, 38)\n",
    "\n",
    "axs[0].axhline(-14, color='red', linestyle=':', label='SG')\n",
    "axs[0].axhline(-12, color='blue', linestyle=':', label='SJG')\n",
    "axs[0].axhline(-8, color='red', linestyle='-.', label='SPG')\n",
    "axs[0].axhline(-2, color='blue', linestyle='-.', label='SPJG')\n",
    "axs[0].axhline(0, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].set_xlabel('Episodios')\n",
    "axs[1].set_xlabel('Episodios')\n",
    "\n",
    "axs[1].axhline(8, color='red', linestyle=':', label='SG')\n",
    "axs[1].axhline(12, color='blue', linestyle=':', label='SJG | SPG | SPJG')\n",
    "axs[1].axhline(22, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "axs[1].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "\n",
    "axs[0].set_ylabel('Retorno promedio')\n",
    "axs[1].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "axs[0].legend(loc='lower right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "axs[1].legend(loc='upper right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "\n",
    "# plt.savefig('Outputs/Lab2_Ej1/SARSA-SoftMax/Alpha/its-1000_eps-20000_g-1_t-0.05.png')\n",
    "# plt.savefig('Outputs/Lab2_Ej1/SARSA-SoftMax/Gamma/its-1000_eps-20000_a-0.1_t-0.05.png')\n",
    "# plt.savefig('Outputs/Lab2_Ej1/SARSA-SoftMax/Tau/its-1000_eps-20000_a-0.1_g-0.45.png')\n",
    "plt.savefig('Outputs/Lab2_Ej1/SARSA-SoftMax/Episodios/its-1000_a-0.1_g-0.45_t-0.03.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dyna-Q y Q-learning con $\\epsilon$-greedy\n",
    "\n",
    "Cuando se quiera hacer Q-learning con $\\epsilon$-greedy, se usa Dyna-Q con 0 pasos de planificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning con $\\epsilon$-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = epGreedy\n",
    "\n",
    "# Me quedo con 0.1\n",
    "Sw_a = [0.1] # [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "# Me quedo con 0.65\n",
    "Sw_g = [0.65] # [0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.9, 1]\n",
    "# Me quedo con 0.001\n",
    "Sw_e = [0.001] # [0.001, 0.005, 0.01, 0.05]\n",
    "# Me quedo con 100k donde ya estabiliza\n",
    "Sw_eps = [100000] # [20000, 50000, 100000, 150000, 200000]\n",
    "Sw_s = [0]\n",
    "\n",
    "for p_eps in Sw_eps:\n",
    "    episodes_to_run = p_eps\n",
    "    for p_a in Sw_a:\n",
    "        for p_g in Sw_g:\n",
    "            for p_e in Sw_e:\n",
    "                for p_s in Sw_s:\n",
    "                    hyperparameters = {\n",
    "                        \"alpha\": p_a,\n",
    "                        \"gamma\": p_g,\n",
    "                        \"epsilon\": p_e,\n",
    "                        \"steps\": p_s\n",
    "                    }\n",
    "                    print(hyperparameters)\n",
    "\n",
    "                    random_state = np.random.RandomState(1994)\n",
    "\n",
    "                    start = time.time()\n",
    "                    ep_timesteps, ep_return, ep_goal, ep_drop, ep_early, q = run_DynaQ(\n",
    "                        policy,\n",
    "                        hyperparameters,\n",
    "                        episodes_to_run,\n",
    "                        env,\n",
    "                        actions,\n",
    "                        random_state,\n",
    "                        max_iter\n",
    "                    )\n",
    "                    WallTime = time.time() - start\n",
    "\n",
    "                    env.close()\n",
    "\n",
    "                    # Guardamos los resultados\n",
    "                    runType = 'Q-learning-EpGreedy'\n",
    "                    runPar = f'its-{max_iter}_eps-{episodes_to_run}'\n",
    "                    runHPar = f'a-{hyperparameters[\"alpha\"]}_g-{hyperparameters[\"gamma\"]}_e-{hyperparameters[\"epsilon\"]}'\n",
    "\n",
    "                    with open(f'Outputs/Lab2_Ej1/{runType}/{runPar}_{runHPar}.csv', 'w') as f:\n",
    "                        f.write('Goal\\tDrop\\tEarly\\tTotal\\n')\n",
    "                        f.write(f'{ep_goal}\\t{ep_drop}\\t{ep_early}\\t{episodes_to_run}\\n')\n",
    "\n",
    "                        f.write(f'Wall Time[s]\\t{WallTime}\\n')\n",
    "                        \n",
    "                        f.write('State\\tAction\\tQ-value\\n')\n",
    "                        for s in range(37):\n",
    "                            for a in range(4):\n",
    "                                f.write(f'{s}\\t{a}\\t{q[(s,a)]:.6f}\\n')\n",
    "\n",
    "                        f.write('Return\\tTimeSteps\\n')\n",
    "                        for i in range(episodes_to_run):\n",
    "                            f.write(f'{ep_return[i]:.0f}\\t{ep_timesteps[i]:.0f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].set_ylim(-30, 1)\n",
    "axs[1].set_ylim(7, 38)\n",
    "\n",
    "axs[0].axhline(-14, color='red', linestyle=':', label='SG')\n",
    "axs[0].axhline(-12, color='blue', linestyle=':', label='SJG')\n",
    "axs[0].axhline(-8, color='red', linestyle='-.', label='SPG')\n",
    "axs[0].axhline(-2, color='blue', linestyle='-.', label='SPJG')\n",
    "axs[0].axhline(0, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].set_xlabel('Episodios')\n",
    "axs[1].set_xlabel('Episodios')\n",
    "\n",
    "axs[1].axhline(8, color='red', linestyle=':', label='SG')\n",
    "axs[1].axhline(12, color='blue', linestyle=':', label='SJG | SPG | SPJG')\n",
    "axs[1].axhline(22, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "axs[1].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "\n",
    "axs[0].set_ylabel('Retorno promedio')\n",
    "axs[1].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "# for p_a in [0.05, 0.1, 0.15, 0.2, 0.25]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2_Ej1/Q-learning-EpGreedy/Alpha/its-1000_eps-20000_a-{p_a}_g-1_e-0.05.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "\n",
    "# for p_g in [0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.9, 1]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2_Ej1/Q-learning-EpGreedy/Gamma/its-1000_eps-20000_a-0.1_g-{p_g}_e-0.05.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\gamma$ = {p_g}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\gamma$ = {p_g}')\n",
    "\n",
    "# for p_e in [0.05, 0.01, 0.005, 0.001]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2_Ej1/Q-learning-EpGreedy/Epsilon/its-1000_eps-20000_a-0.1_g-0.65_e-{p_e}.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\epsilon$ = {p_e}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\epsilon$ = {p_e}')\n",
    "\n",
    "for eps in [200000, 150000, 100000, 50000, 20000]:\n",
    "    File = pd.read_csv(f'Outputs/Lab2_Ej1/Q-learning-EpGreedy/Episodios/its-1000_eps-{eps}_a-0.1_g-0.65_e-0.001.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "    axs[0].plot(mean_evol(File[:,0]))\n",
    "    axs[1].plot(mean_evol(File[:,1]))\n",
    "\n",
    "axs[0].legend(loc='lower right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "axs[1].legend(loc='upper right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "\n",
    "# plt.savefig('Outputs/Lab2_Ej1/Q-learning-EpGreedy/Alpha/its-1000_eps-20000_g-1_e-0.05.png')\n",
    "# plt.savefig('Outputs/Lab2_Ej1/Q-learning-EpGreedy/Gamma/its-1000_eps-20000_a-0.1_e-0.05.png')\n",
    "# plt.savefig('Outputs/Lab2_Ej1/Q-learning-EpGreedy/Epsilon/its-1000_eps-20000_a-0.1_g-0.65.png')\n",
    "plt.savefig('Outputs/Lab2_Ej1/Q-learning-EpGreedy/Episodios/its-1000_a-0.1_g-0.65_e-0.001.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dyna-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = epGreedy\n",
    "\n",
    "# Me quedo con 10\n",
    "Sw_s = [10] # [10, 20, 30, 40, 50, 60, 70]\n",
    "# Me quedo con 0.1\n",
    "Sw_a = [0.1] # [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "# Me quedo con 0.9. 0.55 y 0.65 no funciona\n",
    "Sw_g = [0.9] # [0.75, 0.85, 0.9, 1]\n",
    "# Me quedo con 0.05. 0.001 no funciona 0.005 tampoco 0.01 tampoco. con 0.5 también falla bastante\n",
    "Sw_e = [0.05] # [0.05, 0.1]\n",
    "# Me quedo con \n",
    "Sw_eps = [20000, 50000, 100000, 150000, 200000]\n",
    "\n",
    "for p_eps in Sw_eps:\n",
    "    episodes_to_run = p_eps\n",
    "    for p_a in Sw_a:\n",
    "        for p_g in Sw_g:\n",
    "            for p_e in Sw_e:\n",
    "                for p_s in Sw_s:\n",
    "                    hyperparameters = {\n",
    "                        \"alpha\": p_a,\n",
    "                        \"gamma\": p_g,\n",
    "                        \"epsilon\": p_e,\n",
    "                        \"steps\": p_s\n",
    "                    }\n",
    "                    print(hyperparameters)\n",
    "\n",
    "                    random_state = np.random.RandomState(1994)\n",
    "\n",
    "                    start = time.time()\n",
    "                    ep_timesteps, ep_return, ep_goal, ep_drop, ep_early, q = run_DynaQ(\n",
    "                        policy,\n",
    "                        hyperparameters,\n",
    "                        episodes_to_run,\n",
    "                        env,\n",
    "                        actions,\n",
    "                        random_state,\n",
    "                        max_iter\n",
    "                    )\n",
    "                    WallTime = time.time() - start\n",
    "\n",
    "                    env.close()\n",
    "\n",
    "                    # Guardamos los resultados\n",
    "                    runType = f'Dyna-Q'\n",
    "                    runPar = f'its-{max_iter}_eps-{episodes_to_run}'\n",
    "                    runHPar = f'a-{hyperparameters[\"alpha\"]}_g-{hyperparameters[\"gamma\"]}_e-{hyperparameters[\"epsilon\"]}_s-{hyperparameters[\"steps\"]}'\n",
    "\n",
    "                    with open(f'Outputs/Lab2_Ej1/{runType}/{runPar}_{runHPar}.csv', 'w') as f:\n",
    "                        f.write('Goal\\tDrop\\tEarly\\tTotal\\n')\n",
    "                        f.write(f'{ep_goal}\\t{ep_drop}\\t{ep_early}\\t{episodes_to_run}\\n')\n",
    "\n",
    "                        f.write(f'Wall Time[s]\\t{WallTime}\\n')\n",
    "                        \n",
    "                        f.write('State\\tAction\\tQ-value\\n')\n",
    "                        for s in range(37):\n",
    "                            for a in range(4):\n",
    "                                f.write(f'{s}\\t{a}\\t{q[(s,a)]:.6f}\\n')\n",
    "\n",
    "                        f.write('Return\\tTimeSteps\\n')\n",
    "                        for i in range(episodes_to_run):\n",
    "                            f.write(f'{ep_return[i]:.0f}\\t{ep_timesteps[i]:.0f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# for p_s in [10, 20, 30, 40, 50, 60, 70]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2_Ej1/Dyna-Q/Steps/its-1000_eps-20000_a-0.1_g-1_e-0.05_s-{p_s}.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'Plan = {p_s}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'Plan = {p_s}')\n",
    "\n",
    "# for p_a in [0.05, 0.1, 0.15, 0.2, 0.25]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2_Ej1/Dyna-Q/Alpha/its-1000_eps-20000_a-{p_a}_g-1_e-0.05_s-10.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "\n",
    "# for p_g in [0.75, 0.85, 0.9, 1]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2_Ej1/Dyna-Q/Gamma/its-1000_eps-20000_a-0.1_g-{p_g}_e-0.05_s-10.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\gamma$ = {p_g}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\gamma$ = {p_g}')\n",
    "\n",
    "# for p_e in [0.05, 0.1]:\n",
    "#     File = pd.read_csv(f'Outputs/Lab2_Ej1/Dyna-Q/Epsilon/its-1000_eps-20000_a-0.1_g-0.9_e-{p_e}_s-10.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "#     axs[0].plot(mean_evol(File[:,0]), label=rf'$\\epsilon$ = {p_e}')\n",
    "#     axs[1].plot(mean_evol(File[:,1]), label=rf'$\\epsilon$ = {p_e}')\n",
    "\n",
    "for eps in [200000, 150000, 100000, 50000, 20000]:\n",
    "    File = pd.read_csv(f'Outputs/Lab2_Ej1/Dyna-Q/Episodios/its-1000_eps-{eps}_a-0.1_g-0.9_e-0.05_s-10.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "    axs[0].plot(mean_evol(File[:,0]))\n",
    "    axs[1].plot(mean_evol(File[:,1]))\n",
    "\n",
    "# axs[0].set_ylim(-30, 1)\n",
    "# axs[1].set_ylim(7, 38)\n",
    "axs[0].set_ylim(-15, 1)\n",
    "axs[1].set_ylim(7, 25)\n",
    "\n",
    "axs[0].axhline(-14, color='red', linestyle=':', label='SG')\n",
    "axs[0].axhline(-12, color='blue', linestyle=':', label='SJG')\n",
    "axs[0].axhline(-8, color='red', linestyle='-.', label='SPG')\n",
    "axs[0].axhline(-2, color='blue', linestyle='-.', label='SPJG')\n",
    "axs[0].axhline(0, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].set_xlabel('Episodios')\n",
    "axs[1].set_xlabel('Episodios')\n",
    "\n",
    "axs[1].axhline(8, color='red', linestyle=':', label='SG')\n",
    "axs[1].axhline(12, color='blue', linestyle=':', label='SJG | SPG | SPJG')\n",
    "axs[1].axhline(22, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "axs[1].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "\n",
    "axs[0].set_ylabel('Retorno promedio')\n",
    "axs[1].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "axs[0].legend(loc='lower right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "axs[1].legend(loc='upper right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "\n",
    "# plt.savefig('Outputs/Lab2_Ej1/Dyna-Q/Steps/its-1000_eps-20000_a-0.1_g-1_e-0.05.png')\n",
    "# plt.savefig('Outputs/Lab2_Ej1/Dyna-Q/Alpha/its-1000_eps-20000_g-1_e-0.05_s-10.png')\n",
    "# plt.savefig('Outputs/Lab2_Ej1/Dyna-Q/Gamma/its-1000_eps-20000_a-0.1_e-0.05_s-10.png')\n",
    "# plt.savefig('Outputs/Lab2_Ej1/Dyna-Q/Epsilon/its-1000_eps-20000_a-0.1_g-0.9_s-10.png')\n",
    "plt.savefig('Outputs/Lab2_Ej1/Dyna-Q/Episodios/its-1000_a-0.1_g-0.9_e-0.05_s-10.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cierre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "\n",
    "# Dyna-Q\n",
    "File = pd.read_csv(f'Outputs/Lab2_Ej1/Dyna-Q/Episodios/its-1000_eps-100000_a-0.1_g-0.9_e-0.05_s-10.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "axs[0].plot(mean_evol(File[:,0]), label='Dyna-Q')\n",
    "axs[1].plot(mean_evol(File[:,1]), label='Dyna-Q')\n",
    "\n",
    "# Q-learning + Eps Greedy\n",
    "File = pd.read_csv(f'Outputs/Lab2_Ej1/Q-learning-EpGreedy/Episodios/its-1000_eps-100000_a-0.1_g-0.65_e-0.001.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "axs[0].plot(mean_evol(File[:,0]), label='Q-learning+EpsGreedy')\n",
    "axs[1].plot(mean_evol(File[:,1]), label='Q-learning+EpsGreedy')\n",
    "\n",
    "# SARSA + SoftMax\n",
    "File = pd.read_csv(f'Outputs/Lab2_Ej1/SARSA-SoftMax/Episodios/its-1000_eps-100000_a-0.1_g-0.45_t-0.03.csv', header=None, sep='\\t').iloc[153:, :2].to_numpy().astype(int)\n",
    "axs[0].plot(mean_evol(File[:,0]), label='SARSA+SoftMax')\n",
    "axs[1].plot(mean_evol(File[:,1]), label='SARSA+SoftMax')\n",
    "\n",
    "axs[0].set_ylim(-15, 1)\n",
    "axs[1].set_ylim(7, 25)\n",
    "\n",
    "axs[0].axhline(-14, color='red', linestyle=':', label='SG')\n",
    "axs[0].axhline(-12, color='blue', linestyle=':', label='SJG')\n",
    "axs[0].axhline(-8, color='red', linestyle='-.', label='SPG')\n",
    "axs[0].axhline(-2, color='blue', linestyle='-.', label='SPJG')\n",
    "axs[0].axhline(0, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].set_xlabel('Episodios')\n",
    "axs[1].set_xlabel('Episodios')\n",
    "\n",
    "axs[1].axhline(8, color='red', linestyle=':', label='SG')\n",
    "axs[1].axhline(12, color='blue', linestyle=':', label='SJG | SPG | SPJG')\n",
    "axs[1].axhline(22, color='black', linestyle='--', label='SJPG')\n",
    "\n",
    "axs[0].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "axs[1].ticklabel_format(style='sci', axis='x', scilimits=(3,3))\n",
    "\n",
    "axs[0].set_ylabel('Retorno promedio')\n",
    "axs[1].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "axs[0].legend(loc='lower right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "axs[1].legend(loc='upper right', frameon=False, fancybox=True, title='Camino', ncol=2)\n",
    "\n",
    "plt.savefig('Outputs/Lab2_Ej1/Ej_1_ComparacionFinal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio 2\n",
    "\n",
    "Utilizar stable-baselines3 para aplicar DQN y PPO al entorno LunarLander."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander: descripción del juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El juego consiste en aterrizar la nave sobre la superficie lunar. La zona de aterrizaje objetivo se encuentra en el origen de coordenadas, pudiendo aterrizarse en otro lugar. El combustible es infinito. La posición inicial de la nave es arriba al centro, con una fuerza inicial aleatoria aplicada sobre su centro de masa.\n",
    "\n",
    "El espacio de acciones $\\mathcal{A}$ tiene 4 elementos para todos los estados que no están en el borde del mapa:\n",
    "- 0 $\\Rightarrow$ Motor apagado.\n",
    "- 1 $\\Rightarrow$ Motor izquierdo encendido.\n",
    "- 2 $\\Rightarrow$ Motor central encendido.\n",
    "- 3 $\\Rightarrow$ Motor derecho encendido.\n",
    "\n",
    "\n",
    "El espacio de estados $\\mathcal{S}$ es un vector de 8 dimensiones, cuyas coordenadas corresponden a:\n",
    "- Coordenadas $(x, y)$ de la nave.\n",
    "- Velocidad lineal $(v_x, v_y)$ de la nave.\n",
    "- Ángulo $\\theta$ de la nave respecto al suelo.\n",
    "- Velocidad angular de la nave $(\\omega_x, \\omega_y)$.\n",
    "- Dos booleanos que representan el contacto de las patas de la nave con el suelo.\n",
    "\n",
    "Observamos que $\\mathcal{A}$ es discreto, pero $\\mathcal{S}$ es continuo.\n",
    "\n",
    "Luego de cada iteración, se otorga una recompensa según:\n",
    "* Aumenta/disminuye al acercarse/alejarse de la zona de aterrizaje.\n",
    "* Aumenta/disminuye cuanto más lento/rápido se mueva la nave.\n",
    "* Disminuye cuanto más inclinada ($\\theta$) se encuentre la nave respecto al suelo.\n",
    "* Suma 10 puntos por cada pierna sobre el suelo.\n",
    "* Resta 0.03 puntos por cada vez que prende los motores laterales.\n",
    "* Resta 0.3 puntos por cada vez que prende el motor central.\n",
    "* Suma $\\pm100$ puntos según aterriza bien o choca, respectivamente.\n",
    "\n",
    "El episodio se considera una solución satisfactoria al problema cuando se alcanzan al menos 200 puntos. Se considera que un episodio termina cuando:\n",
    "* La nave choca (toca el suelo con alguna parte de la nave que no sean las patas).\n",
    "* La nave escapa del campo disponible ($x>1$).\n",
    "* La nave *no está despierta* (no se mueve ni choca con nada)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f92ad551d5742f0ad49502d13fb2753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps/s-2000000_a-0.001_g-0.99_e-0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.99 min\n"
     ]
    }
   ],
   "source": [
    "Dir = 'Steps'\n",
    "\n",
    "# Etapa de exploración\n",
    "epsilon = [0.3] # [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# Tasa de aprendizaje\n",
    "alpha = [0.001] # [0.01, 0.1] # [0.001, 0.00001, 0.000001] # [0.0001, 0.001, 0.00001]\n",
    "# Factor de descuento\n",
    "gamma = [0.99] # [0.9, 0.999, 0.9999, 0.85] # [0.99, 0.9, 0.999]\n",
    "# Cantidad de pasos\n",
    "nSteps = [2000000] # [50000, 100000, 300000, 1000000, 2000000, 4000000] # [600000]\n",
    "\n",
    "for p_e in epsilon:\n",
    "    for p_a in alpha:\n",
    "        for p_g in gamma:\n",
    "            for p_nS in nSteps:\n",
    "                # Nombre del file\n",
    "                log = f'{Dir}/s-{p_nS}_a-{p_a}_g-{p_g}_e-{p_e}'\n",
    "                print(log)\n",
    "                start = time.time()\n",
    "\n",
    "                # Creación del entorno.\n",
    "                env = gym.make(\"LunarLander-v2\", gravity = -10.0)\n",
    "\n",
    "                # Instanciamos el modelo con MlpPolicy\n",
    "                model = DQN(\"MlpPolicy\", env, learning_rate=p_a, gamma=p_g, \n",
    "                            seed=1994, exploration_fraction=p_e, \n",
    "                            tensorboard_log=f\"Outputs/Lab2_Ej2/DQN/\")\n",
    "\n",
    "                # # Continuamos un aprendizaje ya guardado\n",
    "                # model_path = f\"Outputs/Lab2_Ej2/DQN/Model/{log}\"\n",
    "                # log_path = f\"Outputs/Lab2_Ej2/DQN/{log}\"\n",
    "                # model = DQN.load(model_path, tensorboard_log=log_path)\n",
    "                # model.set_env(env)\n",
    "\n",
    "                # Entrenamos el agente\n",
    "                model.learn(total_timesteps=p_nS, tb_log_name=log, progress_bar=True)\n",
    "\n",
    "                # Guardamos el modelo entrenado\n",
    "                model.save(f\"Outputs/Lab2_Ej2/DQN/Model/{log}\")\n",
    "\n",
    "                Walltime = time.time() - start\n",
    "                print(f'{Walltime/60:.2f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps/s-2000000_a-0.0003_g-0.99_e-50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7952b5bcda854a698313c31c19a756b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414.81 min\n"
     ]
    }
   ],
   "source": [
    "Dir = 'Steps'\n",
    "\n",
    "# Cantidad de épocas\n",
    "epoch = [50] # [10, 25, 50, 75, 100, 150, 200]\n",
    "# Tasa de aprendizaje\n",
    "alpha = [0.0003] # [0.0003, 0.001, 0.01, 0.0001, 0.00001]\n",
    "# Factor de descuento\n",
    "gamma = [0.99] # [0.9, 0.999, 0.9999, 0.99999]\n",
    "# Cantidad de pasos\n",
    "nSteps = [2000000] # [100000]\n",
    "\n",
    "for p_e in epoch:\n",
    "    for p_a in alpha:\n",
    "        for p_g in gamma:\n",
    "            for p_nS in nSteps:\n",
    "                # Nombre del file\n",
    "                log = f'{Dir}/s-{p_nS}_a-{p_a}_g-{p_g}_e-{p_e}'\n",
    "                print(log)\n",
    "                start = time.time()\n",
    "\n",
    "                # Creación del entorno.\n",
    "                env = gym.make(\"LunarLander-v2\", gravity = -10.0)\n",
    "\n",
    "                # Instanciamos el modelo con MlpPolicy\n",
    "                model = PPO(\"MlpPolicy\", env, learning_rate=p_a, gamma=p_g, \n",
    "                            seed=1994, n_epochs=p_e, \n",
    "                            tensorboard_log=f\"Outputs/Lab2_Ej2/PPO/\")\n",
    "\n",
    "                # # Continuamos un aprendizaje ya guardado\n",
    "                # model_path = f\"Outputs/Lab2_Ej2/PPO/Model/{log}\"\n",
    "                # log_path = f\"Outputs/Lab2_Ej2/PPO/{log}\"\n",
    "                # model = PPO.load(model_path, tensorboard_log=log_path)\n",
    "                # model.set_env(env)\n",
    "\n",
    "                # Entrenamos el agente\n",
    "                model.learn(total_timesteps=p_nS, tb_log_name=log, progress_bar=True)\n",
    "\n",
    "                # Guardamos el modelo entrenado\n",
    "                model.save(f\"Outputs/Lab2_Ej2/PPO/Model/{log}\")\n",
    "\n",
    "                Walltime = time.time() - start\n",
    "                print(f'{Walltime/60:.2f} min')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6Ci7JfDdaPs/g38IjJj7A",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
