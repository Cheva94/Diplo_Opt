{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
    "\n",
    "## Aprendizaje por refuerzos\n",
    "\n",
    "### Trabajo práctico entregable 2/2 (materia completa)\n",
    "\n",
    "**Estudiante:**\n",
    "- [Chevallier-Boutell, Ignacio José.](https://www.linkedin.com/in/nachocheva/)\n",
    "\n",
    "**Docentes:**\n",
    "- Palombarini, Jorge (Mercado Libre).\n",
    "- Barsce, Juan Cruz (Mercado Libre).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los videos de las ejecuciones hay que tener instalado ffmpeg (`apt-get install ffmpeg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio 1\n",
    "\n",
    "Crear un entorno propio y entrenar agentes de RL en el mismo, utilizando diferentes algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheva's Odyssey: reglas del juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mapa del juego consiste en una grilla 6x6, como se muestra a continuación. Al comenzar un episodio, el agente se ubica en la posición $S$ (elemento [5, 5]) y debe llegar hasta $G$ (elemento [0, 2]) para terminar dicho episodio. El agente debe realizar tantos movimientos como sean necesarios para llegar a la meta y finalizar el episodio.\n",
    "\n",
    "|J|A|G|A|~|~|\n",
    "|-|-|-|-|-|-|\n",
    "|~|A|A|A|~|~|\n",
    "|~|~|~|~|~|~|\n",
    "|~|~|~|~|~|~|\n",
    "|A|A|~|~|~|~|\n",
    "|P|A|~|~|~|S|\n",
    "\n",
    "El espacio de estados $\\mathcal{S}$ tiene 36 elementos. Para calcular el valor del estado asociado a cada elemento del mapa debemos calcular\n",
    "    $$\\text{Fila Actual} \\times \\text{Número de Columnas} + \\text{Columna Actual}$$\n",
    "\n",
    "donde debemos contar desde 0. De esta manera, el estado inicial $S$ es el estado 35 y el estado terminal $G$ es el estado 2.\n",
    "\n",
    "El espacio de acciones $\\mathcal{A}$ tiene 4 elementos para todos los estados que no están en el borde del mapa:\n",
    "- 0 $\\Rightarrow$ Se mueve hacia arriba.\n",
    "- 1 $\\Rightarrow$ Se mueve hacia la derecha.\n",
    "- 2 $\\Rightarrow$ Se mueve hacia abajo.\n",
    "- 3 $\\Rightarrow$ Se mueve hacia la izquierda.\n",
    "\n",
    "Para los estados que están en los bordes, sólo se puede elegir entre 2 acciones si están en los vértices o entre 3 si están en las aristas, según corresponda. Observamos que se cumple que $\\mathcal{A}, \\mathcal{S} \\in \\mathbb{N}$.\n",
    "\n",
    "Además de la salida $S$ y la meta $G$, tenemos otros elementos en el mapa. Los elementos vacíos (~) representan pasto, mientras que los elementos con $A$ son agua. El elemento $J$ es un jetpack y el elemento $P$ representa un premio extra. A partir de esto, la función de recompensa es tal que el agente recibe:\n",
    "- $-1$ cuando ingresa en a un elemento con pasto o cuando busca el jetpack o el premio extra. Sus efectos no se pierden, pero tampoco se acumulan.\n",
    "- $-8$ cuando ingresa en a un elemento con agua. En caso de contar con el jetpack, el costo por pasar por el agua se reduce a $-2$.\n",
    "- $+0$ si alcanza la meta sin el premio extra y $+24$ cuando la alcanza con el premio extra.\n",
    "\n",
    "Hay principalmente 5 caminos relevantes:\n",
    "- **SG:** Ir directo a la meta requiere 8 pasos temporales, otorgando -14 puntos.\n",
    "- **SJG:** Buscar el jetpack e ir a la meta requiere 12 pasos temporales, otorgando -12 puntos.\n",
    "- **SPG:** Buscar el premio extra e ir a la meta requiere 12 pasos temporales, otorgando -8 puntos.\n",
    "- **SPJG:** Buscar el premio extra, luego el jetpack e ir a la meta requiere 12 pasos temporales, otorgando -2 puntos.\n",
    "- **SJPG:** Buscar el jetpack, luego el premio extra e ir a la meta requiere 22 pasos temporales, otorgando 0 puntos (puntuación máxima)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChevasOdyssey(gym.Env):\n",
    "    # Tipo de renderizado posible, además de None.\n",
    "    metadata = {\"render.modes\": [\"console\"]}\n",
    "\n",
    "    # Definimos los valores de las acciones\n",
    "    UP = 0\n",
    "    RIGHT = 1\n",
    "    DOWN = 2\n",
    "    LEFT = 3\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ChevasOdyssey, self).__init__()\n",
    "\n",
    "        # Tamaño del mapa\n",
    "        self.grid_shape = (6, 6)\n",
    "\n",
    "        # Inicializamos en agente en el punto de partida\n",
    "        self.agent_pos = 35\n",
    "\n",
    "        # Espacio de acciones\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Espacio de estados\n",
    "        self.state_space = spaces.Discrete(36)\n",
    "\n",
    "\n",
    "    def reset(self, seed=None) -> Tuple[np.array, dict]:\n",
    "        \"\"\"\n",
    "        Reinicia el ambiente y devuelve la observación inicial\n",
    "        \"\"\"\n",
    "        # Inicializamos en agente en el punto de partida\n",
    "        self.agent_pos = 35\n",
    "\n",
    "        # convertimos con astype a float32 (numpy) para hacer más general\n",
    "        # el agente (en caso de que querramos usar acciones continuas)\n",
    "        return (np.array([self.agent_pos]).astype(int), {})\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.UP:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        elif action == self.DOWN:\n",
    "            self.agent_pos += 1\n",
    "        elif action == self.LEFT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Se recibió una acción inválida={action} que no es parte del\\\n",
    "                    espacio de acciones\"\n",
    "            )\n",
    "\n",
    "        # Evitamos que el agente se salga de los límites de la grilla\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "        # Llegó el agente a su estado objetivo (izquierda) de la grilla?\n",
    "        terminated = bool(self.agent_pos == 0)\n",
    "        truncated = False  # no limitamos la duración de los episodios\n",
    "\n",
    "        # Asignamos recompensa sólo cuando el agente llega a su objetivo\n",
    "        # (recompensa = 0 en todos los demás estados)\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        # gym también nos permite devolver información adicional, ej. en Atari:\n",
    "        # las vidas restantes del agente (no usaremos esto por ahora)\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        if mode != \"console\":\n",
    "            raise NotImplementedError()\n",
    "        # en nuestra interfaz de consola, representamos el agente como una\n",
    "        # cruz, y el resto como un punto\n",
    "        print(\".\" * self.agent_pos, end=\"\")\n",
    "        print(\"x\", end=\"\")\n",
    "        print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nS = np.prod((6,6))\n",
    "nS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isd = np.zeros(nS)\n",
    "isd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isd[np.ravel_multi_index((3,0), (6,6))] = 1.0\n",
    "isd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6Ci7JfDdaPs/g38IjJj7A",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
