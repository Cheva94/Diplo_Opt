{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
    "\n",
    "## Aprendizaje por refuerzos\n",
    "\n",
    "### Trabajo práctico entregable 1/2 (materia completa)\n",
    "\n",
    "**Estudiante:**\n",
    "- [Chevallier-Boutell, Ignacio José.](https://www.linkedin.com/in/nachocheva/)\n",
    "\n",
    "**Docentes:**\n",
    "- Palombarini, Jorge (Mercado Libre).\n",
    "- Barsce, Juan Cruz (Mercado Libre).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los videos de las ejecuciones hay que tener instalado ffmpeg (`apt-get install ffmpeg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones predefinidas de ploteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_per_episode(reward_ep) -> None:\n",
    "    episode_rewards = np.array(reward_ep)\n",
    "\n",
    "    # se suaviza la curva de convergencia\n",
    "    episode_number = np.linspace(\n",
    "        1, len(episode_rewards) + 1, len(episode_rewards) + 1)\n",
    "    acumulated_rewards = np.cumsum(episode_rewards)\n",
    "\n",
    "    reward_per_episode = [\n",
    "        acumulated_rewards[i] / episode_number[i]\n",
    "        for i in range(len(acumulated_rewards))\n",
    "    ]\n",
    "\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.title(\"Recompensa acumulada por episodio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_per_episode(timesteps_ep) -> None:\n",
    "    # se muestra la curva de aprendizaje de los pasos por episodio\n",
    "    episode_steps = np.array(timesteps_ep)\n",
    "    plt.plot(np.array(range(0, len(episode_steps))), episode_steps)\n",
    "    plt.title(\"Pasos (timesteps) por episodio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_per_episode_smooth(timesteps_ep) -> None:\n",
    "    episode_steps = np.array(timesteps_ep)\n",
    "\n",
    "    # se suaviza la curva de aprendizaje\n",
    "    episode_number = np.linspace(\n",
    "        1, len(episode_steps) + 1, len(episode_steps) + 1)\n",
    "    acumulated_steps = np.cumsum(episode_steps)\n",
    "\n",
    "    steps_per_episode = [\n",
    "        acumulated_steps[i] / episode_number[i] for i in range(\n",
    "            len(acumulated_steps))\n",
    "    ]\n",
    "\n",
    "    plt.plot(steps_per_episode)\n",
    "    plt.title(\"Pasos (timesteps) acumulados por episodio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_value_matrix(q) -> None:\n",
    "    n_rows = 4\n",
    "    n_columns = 12\n",
    "    n_actions = 4\n",
    "\n",
    "    # se procede con los cálculos previos a la graficación de la matriz de valor\n",
    "    q_value_matrix = np.empty((n_rows, n_columns))\n",
    "    for row in range(n_rows):\n",
    "        for column in range(n_columns):\n",
    "            state_values = []\n",
    "\n",
    "            for action in range(n_actions):\n",
    "                state_values.append(\n",
    "                    q.get((row * n_columns + column, action), -100))\n",
    "\n",
    "            maximum_value = max(\n",
    "                state_values\n",
    "            )  # determinamos la acción que arroja máximo valor\n",
    "\n",
    "            q_value_matrix[row, column] = maximum_value\n",
    "\n",
    "    # el valor del estado objetivo se asigna en -1 (reward recibido al llegar)\n",
    "    # para que se coloree de forma apropiada\n",
    "    q_value_matrix[3, 11] = -1\n",
    "\n",
    "    # se grafica la matriz de valor\n",
    "    plt.imshow(q_value_matrix, cmap=plt.cm.RdYlGn)\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "\n",
    "    for row, column in itertools.product(\n",
    "        range(q_value_matrix.shape[0]), range(q_value_matrix.shape[1])\n",
    "    ):\n",
    "        left_action = q.get((row * n_columns + column, 3), -1000)\n",
    "        down_action = q.get((row * n_columns + column, 2), -1000)\n",
    "        right_action = q.get((row * n_columns + column, 1), -1000)\n",
    "        up_action = q.get((row * n_columns + column, 0), -1000)\n",
    "\n",
    "        arrow_direction = \"D\"\n",
    "        best_action = down_action\n",
    "\n",
    "        if best_action < right_action:\n",
    "            arrow_direction = \"R\"\n",
    "            best_action = right_action\n",
    "        if best_action < left_action:\n",
    "            arrow_direction = \"L\"\n",
    "            best_action = left_action\n",
    "        if best_action < up_action:\n",
    "            arrow_direction = \"U\"\n",
    "            best_action = up_action\n",
    "        if best_action == -1:\n",
    "            arrow_direction = \"\"\n",
    "\n",
    "        # notar que column, row están invertidos en orden en la línea de abajo\n",
    "        # porque representan a x,y del plot\n",
    "        plt.text(column, row, arrow_direction, horizontalalignment=\"center\")\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n Matriz de mejor acción-valor (en números): \\n\\n\", q_value_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cliff walking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema\n",
    "\n",
    "Cliff walking es un juego que involucra moverse sobre un mapa (grilla 4x12) desde un punto inicial (Start, S) hasta un punto final (Goal, G), evitando caer por el precipicio (The Cliff).\n",
    "\n",
    "![](https://github.com/GIDISIA/RLDiplodatos/blob/master/images/cliffwalking.png?raw=1)\n",
    "\n",
    "Imagen: Sutton y Barto, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comenzar un episodio, el jugador se ubica en el elemento [3, 0] de la grilla y debe llegar hasta el elemento [3, 11] para terminar dicho episodio. El precipio se ubica en [3, 1:11]. Cuando el jugador llega a uno de estos elementos, *se cae por el precipicio* y vuelve al punto inicial [3, 0]. El jugador debe realizar tantos movimientos como sean necesarios para llegar a la meta y finalizar el episodio.\n",
    "\n",
    "El espacio de acciones $\\mathcal{A}$ tiene 4 elementos:\n",
    "- 0 $\\Rightarrow$ Se mueve hacia arriba\n",
    "- 1 $\\Rightarrow$ Se mueve hacia la derecha\n",
    "- 2 $\\Rightarrow$ Se mueve hacia abajo\n",
    "- 3 $\\Rightarrow$ Se mueve hacia la izquierda\n",
    "\n",
    "A pesar de que la grilla tiene 48 elementos, el jugador no puede estar en el precipicio, ya que vuelve al punto S, y tampoco puede estar en la meta, sino sólo llegar a ésta, ya que es el estado terminal del episodio. Esto resulta en que el espacio de estados $\\mathcal{S}$ tiene 37 elementos.\n",
    "\n",
    "Se cumple que $\\mathcal{A}, \\mathcal{S} \\in \\mathbb{N}$, teniendo al cero como primer elemento.\n",
    "\n",
    "La función de recompensa es tal que el jugador recibe:\n",
    "- $-100$ cuando cae el precipicio.\n",
    "- $-1$ en todos los demás casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
    "actions = range(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementos del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heurísticas de selección de acciones\n",
    "\n",
    "Se definen los métodos para decidir qué acciones tomar, pudiendo ser $\\epsilon$-greedy o SoftMax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_e_greedy(state: int, \n",
    "                           actions: range, \n",
    "                           q: dict,\n",
    "                           hyperparameters: dict,\n",
    "                           random_state: np.random.RandomState\n",
    "                           ) -> int:\n",
    "    \"\"\"\n",
    "    Elije una acción de acuerdo al aprendizaje realizado previamente\n",
    "    usando una política de exploración épsilon-greedy\n",
    "    \"\"\"\n",
    "    # ej: para 4 acciones inicializa en [0,0,0,0]\n",
    "    # q_values = [q.get((state, a), 0.0) for a in actions]\n",
    "    q_values = [q.get(a) for a in actions]\n",
    "    print(q_values)\n",
    "    max_q = max(q_values)\n",
    "    # sorteamos un número: es menor a épsilon?\n",
    "    if random_state.uniform() < hyperparameters['epsilon']:\n",
    "        # sí: se selecciona una acción aleatoria\n",
    "        return random_state.choice(actions)\n",
    "\n",
    "    count = q_values.count(max_q)\n",
    "\n",
    "    # hay más de un máximo valor de estado-acción?\n",
    "    if count > 1:\n",
    "        # sí: seleccionamos uno de ellos aleatoriamente\n",
    "        best = [i for i in range(len(actions)) if q_values[i] == max_q]\n",
    "        i = random_state.choice(best)\n",
    "    else:\n",
    "        # no: seleccionamos el máximo valor de estado-acción\n",
    "        i = q_values.index(max_q)\n",
    "\n",
    "    return actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
    "actions = range(env.action_space.n)\n",
    "\n",
    "q = {'arriba': 1,\n",
    "     'derecha': 5,\n",
    "     'abajo': 99,\n",
    "     'izquierda': -20}\n",
    "\n",
    "for a in actions:\n",
    "    print(q.get(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m random_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mRandomState(\u001b[39m42\u001b[39m) \u001b[39m# Semilla aleatoria\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m state, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m action \u001b[39m=\u001b[39m choose_action_e_greedy(state, actions, q, hyperparameters, random_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;32m/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m q_values \u001b[39m=\u001b[39m [q\u001b[39m.\u001b[39mget(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m actions]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(q_values)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m max_q \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(q_values)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# sorteamos un número: es menor a épsilon?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m random_state\u001b[39m.\u001b[39muniform() \u001b[39m<\u001b[39m hyperparameters[\u001b[39m'\u001b[39m\u001b[39mepsilon\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# sí: se selecciona una acción aleatoria\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "q = {'arriba': 1,\n",
    "     'derecha': 5,\n",
    "     'abajo': 99,\n",
    "     'izquierda': -20}\n",
    "hyperparameters = {\"alpha\": 0.5, \n",
    "                   \"gamma\": 1, \n",
    "                   \"epsilon\": 0.1, \n",
    "                   \"tau\": 25}\n",
    "random_state = np.random.RandomState(42) # Semilla aleatoria\n",
    "\n",
    "state, _ = env.reset()\n",
    "action = choose_action_e_greedy(state, actions, q, hyperparameters, random_state)\n",
    "print(action)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_softmax() -> int:\n",
    "    \"\"\"\n",
    "    Elije una acción de acuerdo al aprendizaje realizado previamente\n",
    "    usando una política softmax\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implementar\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje por diferencias temporales\n",
    "\n",
    "Se definen los métodos de aprendizaje, los cuales toman una transición y cambian el dict de los valores de Q de acuerdo a Sarsa o Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_SARSA(\n",
    "        state: Any, # COMPLETAR tipo de cada parámetro\n",
    "        action: Any,\n",
    "        reward: Any,\n",
    "        next_state: Any,\n",
    "        next_action: Any,\n",
    "        hyperparameters: Any,\n",
    "        q: Any,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Realiza una actualización según el algoritmo SARSA, para una transición\n",
    "    de estado dada\n",
    "    Args:\n",
    "        state: COMPLETAR\n",
    "        action: COMPLETAR\n",
    "        reward: COMPLETAR\n",
    "        next_state: COMPLETAR\n",
    "        next_action: COMPLETAR\n",
    "        hyperparameters: COMPLETAR\n",
    "        q: COMPLETAR\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO - completa con tu código aquí\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completar argumentos de la función para hacer una actualización Q-learning\n",
    "def learn_Q_learning(\n",
    "        # COMPLETAR\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Realiza una actualización según el algoritmo Q-learning, para una transición\n",
    "    de estado dada\n",
    "    Args: COMPLETAR\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO - completa con tu código aquí\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteraciones\n",
    "\n",
    "Definimos la manera en la que itera el algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(learning_function: Callable, \n",
    "        hyperparameters: dict, \n",
    "        episodes_to_run: int, \n",
    "        env: gym.Env, \n",
    "        actions: range, \n",
    "        q: dict, \n",
    "        random_state: np.random.RandomState\n",
    "        ) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "\n",
    "    \"\"\"\n",
    "    Corre el algoritmo de RL para un dado entorno.\n",
    "    Args:\n",
    "        learning_function: función de actualización de algoritmo de aprendizaje\n",
    "        hyperparameters: hiperparámetros del algoritmo de aprendizaje\n",
    "        episodes_to_run: cantidad de episodios a ejecutar\n",
    "        env: entorno de Gymnasium\n",
    "        actions: lista de acciones posibles\n",
    "        q: diccionario de valores de estado-acción\n",
    "        random_state: generador de números aleatorios\n",
    "    \"\"\"\n",
    "\n",
    "    # Registra la cantidad de pasos de cada episodio\n",
    "    timesteps_of_episode = []\n",
    "\n",
    "    # Registra el retorno de cada episodio\n",
    "    reward_of_episode = []\n",
    "\n",
    "    # Loop sobre los episodios\n",
    "    # for _ in range(episodes_to_run):\n",
    "    for k in range(episodes_to_run):\n",
    "        print(f'Episodio {k}')\n",
    "        # Instancea un nuevo agente en cada episodio\n",
    "        # Fin del episodio: llegar a la salida o superar los 2000 pasos\n",
    "\n",
    "        # Reinicia el entorno, obteniendo el estado inicial del mismo\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # Contador de recompensa\n",
    "        episode_reward = 0\n",
    "\n",
    "        # Contador de pasos temporales\n",
    "        t = 0\n",
    "\n",
    "        # Flag de finalización de iteración actual\n",
    "        done = False\n",
    "\n",
    "        # elige una acción basado en el estado actual.\n",
    "        # Filtra el primer elemento de state, que es el estado en sí mismo\n",
    "        action = choose_action_e_greedy(\n",
    "            state, actions, q, hyperparameters, random_state)\n",
    "\n",
    "        while not done:\n",
    "            # el agente ejecuta la acción elegida y obtiene los resultados\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            next_action = choose_action_e_greedy(\n",
    "                next_state, actions, q, hyperparameters, random_state)\n",
    "\n",
    "            episode_reward += reward\n",
    "            learning_function(\n",
    "                state,\n",
    "                action,\n",
    "                reward,\n",
    "                next_state,\n",
    "                next_action,\n",
    "                hyperparameters,\n",
    "                q\n",
    "            )\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # if the algorithm does not converge, it stops after 2000 timesteps\n",
    "            if not done and t < 10:#2000:\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            else:\n",
    "                # el algoritmo no ha podido llegar a la meta antes de dar 2000 pasos\n",
    "                done = True  # se establece manualmente la bandera done\n",
    "                timesteps_of_episode = np.append(timesteps_of_episode, [int(t + 1)])\n",
    "                reward_of_episode = np.append(\n",
    "                    reward_of_episode, max(episode_reward, -100)\n",
    "                )\n",
    "\n",
    "            t += 1\n",
    "\n",
    "    return reward_of_episode.mean(), timesteps_of_episode, reward_of_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa + $\\epsilon$-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_function = learn_SARSA\n",
    "\n",
    "hyperparameters = {\"alpha\": 0.5, \n",
    "                   \"gamma\": 1, \n",
    "                   \"epsilon\": 0.1, \n",
    "                   \"tau\": 25}\n",
    "\n",
    "episodes_to_run = 3\n",
    "\n",
    "q = {} # Contiene los valores de Q para cada tupla (estado, acción)\n",
    "\n",
    "random_state = np.random.RandomState(42) # Semilla aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m avg_steps_per_episode, timesteps_ep, reward_ep \u001b[39m=\u001b[39m run(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     learning_function,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     hyperparameters,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     episodes_to_run,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     env,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     actions,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     q,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     random_state\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "\u001b[1;32m/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb Cell 26\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m action \u001b[39m=\u001b[39m choose_action_e_greedy(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     state, actions, q, hyperparameters, random_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m# el agente ejecuta la acción elegida y obtiene los resultados\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     next_action \u001b[39m=\u001b[39m choose_action_e_greedy(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         next_state, actions, q, hyperparameters, random_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cheva/Diplo_Opt/2_RL/Resolucion_Lab1.ipynb#X35sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     episode_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/toy_text/cliffwalking.py:181\u001b[0m, in \u001b[0;36mCliffWalkingEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    182\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/toy_text/cliffwalking.py:206\u001b[0m, in \u001b[0;36mCliffWalkingEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    205\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/toy_text/cliffwalking.py:293\u001b[0m, in \u001b[0;36mCliffWalkingEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    291\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m    292\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    294\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# rgb_array\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[1;32m    296\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    297\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "avg_steps_per_episode, timesteps_ep, reward_ep = run(\n",
    "    learning_function,\n",
    "    hyperparameters,\n",
    "    episodes_to_run,\n",
    "    env,\n",
    "    actions,\n",
    "    q,\n",
    "    random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La interfaz principal de los ambientes de gym es la interfaz Env. La misma posee cinco métodos principales:\n",
    "\n",
    "* ```reset(self, seed)``` : Reinicia el estado del entorno, a su estado inicial, devolviendo una observación de dicho estado. Opcionalmente, establece la semilla aleatoria del generador de números aleatorios del presente entorno.\n",
    "\n",
    "* ```step(self, action)``` : \"Avanza\" un timestep del entorno. Devuelve: ```observation, reward, terminated, truncated, info```.\n",
    "\n",
    "* ```render(self)``` : Muestra en pantalla una parte del entorno.\n",
    "\n",
    "* ```close(self)``` : Finaliza con la instancia del agente.\n",
    "\n",
    "\n",
    "Por otra parte, cada entorno posee los siguientes tres atributos principales:\n",
    "\n",
    "* ```action_space``` : El objeto de tipo Space correspondiente al espacio de acciones válidas.\n",
    "\n",
    "* ```observation_space``` : El objeto de tipo Space correspondiente a todos los rangos posibles de observaciones.\n",
    "\n",
    "* ```reward_range``` : Tupla que contiene los valores mínimo y máximo de recompensa posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa + SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning + $\\epsilon$-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning + SoftMax"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6Ci7JfDdaPs/g38IjJj7A",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
