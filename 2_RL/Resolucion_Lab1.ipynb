{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
    "\n",
    "## Aprendizaje por refuerzos\n",
    "\n",
    "### Trabajo práctico entregable 1/2 (materia completa)\n",
    "\n",
    "**Estudiante:**\n",
    "- [Chevallier-Boutell, Ignacio José.](https://www.linkedin.com/in/nachocheva/)\n",
    "\n",
    "**Docentes:**\n",
    "- Palombarini, Jorge (Mercado Libre).\n",
    "- Barsce, Juan Cruz (Mercado Libre).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los videos de las ejecuciones hay que tener instalado ffmpeg (`apt-get install ffmpeg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones predefinidas de ploteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_per_episode(reward_ep) -> None:\n",
    "    episode_rewards = np.array(reward_ep)\n",
    "\n",
    "    # se suaviza la curva de convergencia\n",
    "    episode_number = np.linspace(\n",
    "        1, len(episode_rewards) + 1, len(episode_rewards) + 1)\n",
    "    acumulated_rewards = np.cumsum(episode_rewards)\n",
    "\n",
    "    reward_per_episode = [\n",
    "        acumulated_rewards[i] / episode_number[i]\n",
    "        for i in range(len(acumulated_rewards))\n",
    "    ]\n",
    "\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.title(\"Recompensa acumulada por episodio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_per_episode(timesteps_ep) -> None:\n",
    "    # se muestra la curva de aprendizaje de los pasos por episodio\n",
    "    episode_steps = np.array(timesteps_ep)\n",
    "    plt.plot(np.array(range(0, len(episode_steps))), episode_steps)\n",
    "    plt.title(\"Pasos (timesteps) por episodio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_per_episode_smooth(timesteps_ep) -> None:\n",
    "    episode_steps = np.array(timesteps_ep)\n",
    "\n",
    "    # se suaviza la curva de aprendizaje\n",
    "    episode_number = np.linspace(\n",
    "        1, len(episode_steps) + 1, len(episode_steps) + 1)\n",
    "    acumulated_steps = np.cumsum(episode_steps)\n",
    "\n",
    "    steps_per_episode = [\n",
    "        acumulated_steps[i] / episode_number[i] for i in range(\n",
    "            len(acumulated_steps))\n",
    "    ]\n",
    "\n",
    "    plt.plot(steps_per_episode)\n",
    "    plt.title(\"Pasos (timesteps) acumulados por episodio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_value_matrix(q) -> None:\n",
    "    n_rows = 4\n",
    "    n_columns = 12\n",
    "    n_actions = 4\n",
    "\n",
    "    # se procede con los cálculos previos a la graficación de la matriz de valor\n",
    "    q_value_matrix = np.empty((n_rows, n_columns))\n",
    "    for row in range(n_rows):\n",
    "        for column in range(n_columns):\n",
    "            state_values = []\n",
    "\n",
    "            for action in range(n_actions):\n",
    "                state_values.append(\n",
    "                    q.get((row * n_columns + column, action), -100))\n",
    "\n",
    "            maximum_value = max(\n",
    "                state_values\n",
    "            )  # determinamos la acción que arroja máximo valor\n",
    "\n",
    "            q_value_matrix[row, column] = maximum_value\n",
    "\n",
    "    # el valor del estado objetivo se asigna en -1 (reward recibido al llegar)\n",
    "    # para que se coloree de forma apropiada\n",
    "    q_value_matrix[3, 11] = -1\n",
    "\n",
    "    # se grafica la matriz de valor\n",
    "    plt.imshow(q_value_matrix, cmap=plt.cm.RdYlGn)\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "\n",
    "    for row, column in itertools.product(\n",
    "        range(q_value_matrix.shape[0]), range(q_value_matrix.shape[1])\n",
    "    ):\n",
    "        left_action = q.get((row * n_columns + column, 3), -1000)\n",
    "        down_action = q.get((row * n_columns + column, 2), -1000)\n",
    "        right_action = q.get((row * n_columns + column, 1), -1000)\n",
    "        up_action = q.get((row * n_columns + column, 0), -1000)\n",
    "\n",
    "        arrow_direction = \"D\"\n",
    "        best_action = down_action\n",
    "\n",
    "        if best_action < right_action:\n",
    "            arrow_direction = \"R\"\n",
    "            best_action = right_action\n",
    "        if best_action < left_action:\n",
    "            arrow_direction = \"L\"\n",
    "            best_action = left_action\n",
    "        if best_action < up_action:\n",
    "            arrow_direction = \"U\"\n",
    "            best_action = up_action\n",
    "        if best_action == -1:\n",
    "            arrow_direction = \"\"\n",
    "\n",
    "        # notar que column, row están invertidos en orden en la línea de abajo\n",
    "        # porque representan a x,y del plot\n",
    "        plt.text(column, row, arrow_direction, horizontalalignment=\"center\")\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n Matriz de mejor acción-valor (en números): \\n\\n\", q_value_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cliff walking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del problema\n",
    "\n",
    "Cliff walking es un juego que involucra moverse sobre un mapa (grilla 4x12) desde un punto inicial (Start, S) hasta un punto final (Goal, G), evitando caer por el precipicio (The Cliff).\n",
    "\n",
    "![](https://github.com/GIDISIA/RLDiplodatos/blob/master/images/cliffwalking.png?raw=1)\n",
    "\n",
    "Imagen: Sutton y Barto, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comenzar un episodio, el jugador se ubica en el elemento [3, 0] de la grilla y debe llegar hasta el elemento [3, 11] para terminar dicho episodio. El precipio se ubica en [3, 1:11]. Cuando el jugador llega a uno de estos elementos, *se cae por el precipicio* y vuelve al punto inicial [3, 0]. El jugador debe realizar tantos movimientos como sean necesarios para llegar a la meta y finalizar el episodio.\n",
    "\n",
    "El espacio de acciones $\\mathcal{A}$ tiene 4 elementos:\n",
    "- 0 $\\Rightarrow$ Se mueve hacia arriba\n",
    "- 1 $\\Rightarrow$ Se mueve hacia la derecha\n",
    "- 2 $\\Rightarrow$ Se mueve hacia abajo\n",
    "- 3 $\\Rightarrow$ Se mueve hacia la izquierda\n",
    "\n",
    "A pesar de que la grilla tiene 48 elementos, el jugador no puede estar en el precipicio, ya que vuelve al punto S, y tampoco puede estar en la meta, sino sólo llegar a ésta, ya que es el estado terminal del episodio. Esto resulta en que el espacio de estados $\\mathcal{S}$ tiene 37 elementos.\n",
    "\n",
    "Se cumple que $\\mathcal{A}, \\mathcal{S} \\in \\mathbb{N}$, teniendo al cero como primer elemento.\n",
    "\n",
    "La función de recompensa es tal que el jugador recibe:\n",
    "- $-100$ cuando cae el precipicio.\n",
    "- $-1$ en todos los demás casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\")\n",
    "actions = range(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementos del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heurísticas de selección de acciones\n",
    "\n",
    "Se definen los métodos para decidir qué acciones tomar, pudiendo ser $\\epsilon$-greedy o SoftMax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_e_greedy(\n",
    "        state: int,\n",
    "        actions: range,\n",
    "        q: dict,\n",
    "        hyperparameters: dict,\n",
    "        random_state: np.random.RandomState,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Elije una acción de acuerdo al aprendizaje realizado previamente\n",
    "    usando una política de exploración épsilon-greedy\n",
    "    \"\"\"\n",
    "    # ej: para 4 acciones inicializa en [0,0,0,0]\n",
    "    q_values = [q.get((state, a), 0.0) for a in actions]\n",
    "    max_q = max(q_values)\n",
    "    # sorteamos un número: es menor a épsilon?\n",
    "    if random_state.uniform() < hyperparameters['epsilon']:\n",
    "        # sí: se selecciona una acción aleatoria\n",
    "        return random_state.choice(actions)\n",
    "\n",
    "    count = q_values.count(max_q)\n",
    "\n",
    "    # hay más de un máximo valor de estado-acción?\n",
    "    if count > 1:\n",
    "        # sí: seleccionamos uno de ellos aleatoriamente\n",
    "        best = [i for i in range(len(actions)) if q_values[i] == max_q]\n",
    "        i = random_state.choice(best)\n",
    "    else:\n",
    "        # no: seleccionamos el máximo valor de estado-acción\n",
    "        i = q_values.index(max_q)\n",
    "\n",
    "    return actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_softmax() -> int:\n",
    "    \"\"\"\n",
    "    Elije una acción de acuerdo al aprendizaje realizado previamente\n",
    "    usando una política softmax\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: implementar\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje por diferencias temporales\n",
    "\n",
    "Se definen los métodos de aprendizaje, los cuales toman una transición y cambian el dict de los valores de Q de acuerdo a Sarsa o Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_SARSA(\n",
    "        state: Any, # COMPLETAR tipo de cada parámetro\n",
    "        action: Any,\n",
    "        reward: Any,\n",
    "        next_state: Any,\n",
    "        next_action: Any,\n",
    "        hyperparameters: Any,\n",
    "        q: Any,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Realiza una actualización según el algoritmo SARSA, para una transición\n",
    "    de estado dada\n",
    "    Args:\n",
    "        state: COMPLETAR\n",
    "        action: COMPLETAR\n",
    "        reward: COMPLETAR\n",
    "        next_state: COMPLETAR\n",
    "        next_action: COMPLETAR\n",
    "        hyperparameters: COMPLETAR\n",
    "        q: COMPLETAR\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO - completa con tu código aquí\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completar argumentos de la función para hacer una actualización Q-learning\n",
    "def learn_Q_learning(\n",
    "        # COMPLETAR\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Realiza una actualización según el algoritmo Q-learning, para una transición\n",
    "    de estado dada\n",
    "    Args: COMPLETAR\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO - completa con tu código aquí\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteraciones\n",
    "\n",
    "Definimos la manera en la que itera el algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(learning_function: Callable,\n",
    "        hyperparameters: dict,\n",
    "        episodes_to_run: int,\n",
    "        env: gym.Env,\n",
    "        actions: range,\n",
    "        q: dict,\n",
    "        random_state: np.random.RandomState\n",
    "        ) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Corre el algoritmo de RL para el ambiente CliffWalking-v0.\n",
    "    Args:\n",
    "        learning_function: función de actualización de algoritmo de aprendizaje\n",
    "        hyperparameters: hiperparámetros del algoritmo de aprendizaje\n",
    "        episodes_to_run: cantidad de episodios a ejecutar\n",
    "        env: ambiente de Gymnasium\n",
    "        actions: lista de acciones posibles\n",
    "        q: diccionario de valores de estado-acción\n",
    "        random_state: generador de números aleatorios\n",
    "    \"\"\"\n",
    "\n",
    "    # Registra la cantidad de pasos de cada episodio\n",
    "    timesteps_of_episode = []\n",
    "\n",
    "    # Registra el retorno de cada episodio\n",
    "    reward_of_episode = []\n",
    "\n",
    "    # Loop sobre los episodios\n",
    "    for _ in range(episodes_to_run):\n",
    "        # Instancea un nuevo agente en cada episodio\n",
    "        # Fin del episodio: llegar a la salida o superar los 2000 pasos\n",
    "\n",
    "        # Reinicia el entorno, obteniendo el estado inicial del mismo\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        t = 0\n",
    "\n",
    "        # elige una acción basado en el estado actual.\n",
    "        # Filtra el primer elemento de state, que es el estado en sí mismo\n",
    "        action = choose_action_e_greedy(\n",
    "            state, actions, q, hyperparameters, random_state)\n",
    "\n",
    "        while not done:\n",
    "            # el agente ejecuta la acción elegida y obtiene los resultados\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            next_action = choose_action_e_greedy(\n",
    "                next_state, actions, q, hyperparameters, random_state)\n",
    "\n",
    "            episode_reward += reward\n",
    "            learning_function(\n",
    "                state,\n",
    "                action,\n",
    "                reward,\n",
    "                next_state,\n",
    "                next_action,\n",
    "                hyperparameters,\n",
    "                q\n",
    "            )\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # if the algorithm does not converge, it stops after 2000 timesteps\n",
    "            if not done and t < 2000:\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            else:\n",
    "                # el algoritmo no ha podido llegar a la meta antes de dar 2000 pasos\n",
    "                done = True  # se establece manualmente la bandera done\n",
    "                timesteps_of_episode = np.append(timesteps_of_episode, [int(t + 1)])\n",
    "                reward_of_episode = np.append(\n",
    "                    reward_of_episode, max(episode_reward, -100)\n",
    "                )\n",
    "\n",
    "            t += 1\n",
    "\n",
    "    return reward_of_episode.mean(), timesteps_of_episode, reward_of_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa + $\\epsilon$-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_function = learn_SARSA\n",
    "\n",
    "hyperparameters = {\"alpha\": 0.5, \n",
    "                   \"gamma\": 1, \n",
    "                   \"epsilon\": 0.1, \n",
    "                   \"tau\": 25}\n",
    "\n",
    "episodes_to_run = 500\n",
    "\n",
    "\n",
    "\n",
    "q = {} # Contiene los valores de Q para cada tupla (estado, acción)\n",
    "\n",
    "random_state = np.random.RandomState(42) # Semilla aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_steps_per_episode, timesteps_ep, reward_ep = run(\n",
    "    learning_function,\n",
    "    hyperparameters,\n",
    "    episodes_to_run,\n",
    "    env,\n",
    "    actions,\n",
    "    q,\n",
    "    random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La interfaz principal de los ambientes de gym es la interfaz Env. La misma posee cinco métodos principales:\n",
    "\n",
    "* ```reset(self, seed)``` : Reinicia el estado del entorno, a su estado inicial, devolviendo una observación de dicho estado. Opcionalmente, establece la semilla aleatoria del generador de números aleatorios del presente entorno.\n",
    "\n",
    "* ```step(self, action)``` : \"Avanza\" un timestep del ambiente. Devuelve: ```observation, reward, terminated, truncated, info```.\n",
    "\n",
    "* ```render(self)``` : Muestra en pantalla una parte del ambiente.\n",
    "\n",
    "* ```close(self)``` : Finaliza con la instancia del agente.\n",
    "\n",
    "\n",
    "Por otra parte, cada entorno posee los siguientes tres atributos principales:\n",
    "\n",
    "* ```action_space``` : El objeto de tipo Space correspondiente al espacio de acciones válidas.\n",
    "\n",
    "* ```observation_space``` : El objeto de tipo Space correspondiente a todos los rangos posibles de observaciones.\n",
    "\n",
    "* ```reward_range``` : Tupla que contiene los valores mínimo y máximo de recompensa posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa + SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning + $\\epsilon$-greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning + SoftMax"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6Ci7JfDdaPs/g38IjJj7A",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
