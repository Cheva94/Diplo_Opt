{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
    "\n",
    "## Aprendizaje por refuerzos\n",
    "\n",
    "### Trabajo práctico entregable 1/2 (materia completa)\n",
    "\n",
    "**Estudiante:**\n",
    "- [Chevallier-Boutell, Ignacio José.](https://www.linkedin.com/in/nachocheva/)\n",
    "\n",
    "**Docentes:**\n",
    "- Palombarini, Jorge (Mercado Libre).\n",
    "- Barsce, Juan Cruz (Mercado Libre).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry shell\n",
    "import itertools\n",
    "from typing import Any, Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OpenAI Gym](https://gym.openai.com/) (Brockman et. al., 2016) es una librería de OpenAI que ofrece entornos y una interfaz estándar con la cuál probar nuestros agentes. Su objetivo es proveer benchmarks unificados para ver el desempeño de algoritmos en el entorno y así poder saber con facilidad cómo es su desempeño comparado con los demás. Parte de la siguiente sección está basada en la [documentación oficial de OpenAI](https://gym.openai.com/docs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La interfaz principal de los ambientes de gym es la interfaz Env. La misma posee cinco métodos principales:\n",
    "\n",
    "* ```reset(self, seed)``` : Reinicia el estado del entorno, a su estado inicial, devolviendo una observación de dicho estado. Opcionalmente, establece la semilla aleatoria del generador de números aleatorios del presente entorno.\n",
    "\n",
    "* ```step(self, action)``` : \"Avanza\" un timestep del ambiente. Devuelve: ```observation, reward, terminated, truncated, info```.\n",
    "\n",
    "* ```render(self)``` : Muestra en pantalla una parte del ambiente.\n",
    "\n",
    "* ```close(self)``` : Finaliza con la instancia del agente.\n",
    "\n",
    "\n",
    "Por otra parte, cada entorno posee los siguientes tres atributos principales:\n",
    "\n",
    "* ```action_space``` : El objeto de tipo Space correspondiente al espacio de acciones válidas.\n",
    "\n",
    "* ```observation_space``` : El objeto de tipo Space correspondiente a todos los rangos posibles de observaciones.\n",
    "\n",
    "* ```reward_range``` : Tupla que contiene los valores mínimo y máximo de recompensa posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas de las ejecuciones contienen videos. Para poder verlos se necesita previamente instalar la librería ffmpeg; para instalarla desde Linux ejecutar en consola\n",
    "\n",
    "```apt-get install ffmpeg```\n",
    "\n",
    "desde Mac, reemplazar *apt-get* por *brew*\n",
    "\n",
    "desde Windows, descargarla desde\n",
    "\n",
    "[https://ffmpeg.org/download.html](https://ffmpeg.org/download.html)\n",
    "\n",
    "(Nota: las animaciones son a modo ilustrativo, si no se desea instalar la librería se puede directamente eliminar la línea de código donde se llama al método ``env.render(mode='human')``)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones predefinidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_per_episode(reward_ep) -> None:\n",
    "    episode_rewards = np.array(reward_ep)\n",
    "\n",
    "    # se suaviza la curva de convergencia\n",
    "    episode_number = np.linspace(\n",
    "        1, len(episode_rewards) + 1, len(episode_rewards) + 1)\n",
    "    acumulated_rewards = np.cumsum(episode_rewards)\n",
    "\n",
    "    reward_per_episode = [\n",
    "        acumulated_rewards[i] / episode_number[i]\n",
    "        for i in range(len(acumulated_rewards))\n",
    "    ]\n",
    "\n",
    "    plt.plot(reward_per_episode)\n",
    "    plt.title(\"Recompensa acumulada por episodio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_per_episode(timesteps_ep) -> None:\n",
    "    # se muestra la curva de aprendizaje de los pasos por episodio\n",
    "    episode_steps = np.array(timesteps_ep)\n",
    "    plt.plot(np.array(range(0, len(episode_steps))), episode_steps)\n",
    "    plt.title(\"Pasos (timesteps) por episodio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_per_episode_smooth(timesteps_ep) -> None:\n",
    "    episode_steps = np.array(timesteps_ep)\n",
    "\n",
    "    # se suaviza la curva de aprendizaje\n",
    "    episode_number = np.linspace(\n",
    "        1, len(episode_steps) + 1, len(episode_steps) + 1)\n",
    "    acumulated_steps = np.cumsum(episode_steps)\n",
    "\n",
    "    steps_per_episode = [\n",
    "        acumulated_steps[i] / episode_number[i] for i in range(\n",
    "            len(acumulated_steps))\n",
    "    ]\n",
    "\n",
    "    plt.plot(steps_per_episode)\n",
    "    plt.title(\"Pasos (timesteps) acumulados por episodio\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_value_matrix(q) -> None:\n",
    "    n_rows = 4\n",
    "    n_columns = 12\n",
    "    n_actions = 4\n",
    "\n",
    "    # se procede con los cálculos previos a la graficación de la matriz de valor\n",
    "    q_value_matrix = np.empty((n_rows, n_columns))\n",
    "    for row in range(n_rows):\n",
    "        for column in range(n_columns):\n",
    "            state_values = []\n",
    "\n",
    "            for action in range(n_actions):\n",
    "                state_values.append(\n",
    "                    q.get((row * n_columns + column, action), -100))\n",
    "\n",
    "            maximum_value = max(\n",
    "                state_values\n",
    "            )  # determinamos la acción que arroja máximo valor\n",
    "\n",
    "            q_value_matrix[row, column] = maximum_value\n",
    "\n",
    "    # el valor del estado objetivo se asigna en -1 (reward recibido al llegar)\n",
    "    # para que se coloree de forma apropiada\n",
    "    q_value_matrix[3, 11] = -1\n",
    "\n",
    "    # se grafica la matriz de valor\n",
    "    plt.imshow(q_value_matrix, cmap=plt.cm.RdYlGn)\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "\n",
    "    for row, column in itertools.product(\n",
    "        range(q_value_matrix.shape[0]), range(q_value_matrix.shape[1])\n",
    "    ):\n",
    "        left_action = q.get((row * n_columns + column, 3), -1000)\n",
    "        down_action = q.get((row * n_columns + column, 2), -1000)\n",
    "        right_action = q.get((row * n_columns + column, 1), -1000)\n",
    "        up_action = q.get((row * n_columns + column, 0), -1000)\n",
    "\n",
    "        arrow_direction = \"D\"\n",
    "        best_action = down_action\n",
    "\n",
    "        if best_action < right_action:\n",
    "            arrow_direction = \"R\"\n",
    "            best_action = right_action\n",
    "        if best_action < left_action:\n",
    "            arrow_direction = \"L\"\n",
    "            best_action = left_action\n",
    "        if best_action < up_action:\n",
    "            arrow_direction = \"U\"\n",
    "            best_action = up_action\n",
    "        if best_action == -1:\n",
    "            arrow_direction = \"\"\n",
    "\n",
    "        # notar que column, row están invertidos en orden en la línea de abajo\n",
    "        # porque representan a x,y del plot\n",
    "        plt.text(column, row, arrow_direction, horizontalalignment=\"center\")\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n Matriz de mejor acción-valor (en números): \\n\\n\", q_value_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cliff walking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El problema\n",
    "\n",
    "Cliff walking es un juego que involucra moverse sobre un mapa (grilla 4x12) desde un punto inicial (Start, S) hasta un punto final (Goal, G), evitando caer por el precipicio (The Cliff).\n",
    "\n",
    "![](https://github.com/GIDISIA/RLDiplodatos/blob/master/images/cliffwalking.png?raw=1)\n",
    "\n",
    "Imagen: Sutton y Barto, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comenzar un episodio, el jugador se ubica en el elemento [3, 0] de la grilla y debe llegar hasta el elemento [3, 11] para terminar dicho episodio. El precipio se ubica en [3, 1:11]. Cuando el jugador llega a uno de estos elementos, *se cae por el precipicio* y vuelve al punto inicial [3, 0]. El jugador debe realizar tantos movimientos como sean necesarios para llegar a la meta y finalizar el episodio.\n",
    "\n",
    "El espacio de acciones $\\mathcal{A}$ tiene 4 elementos:\n",
    "- 0 $\\Rightarrow$ Se mueve hacia arriba\n",
    "- 1 $\\Rightarrow$ Se mueve hacia la derecha\n",
    "- 2 $\\Rightarrow$ Se mueve hacia abajo\n",
    "- 3 $\\Rightarrow$ Se mueve hacia la izquierda\n",
    "\n",
    "A pesar de que la grilla tiene 48 elementos, el jugador no puede estar en el precipicio, ya que vuelve al punto S, y tampoco puede estar en la meta, sino sólo llegar a ésta, ya que es el estado terminal del episodio. Esto resulta en que el espacio de estados $\\mathcal{S}$ tiene 37 elementos.\n",
    "\n",
    "Se cumple que $\\mathcal{A}, \\mathcal{S} \\in \\mathbb{N}$, teniendo al cero como primer elemento.\n",
    "\n",
    "La función de recompensa es tal que el jugador recibe:\n",
    "- $-100$ cuando cae el precipicio.\n",
    "- $-1$ en todos los demás casos."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6Ci7JfDdaPs/g38IjJj7A",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
