{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
    "\n",
    "## Aprendizaje por refuerzos\n",
    "\n",
    "### Trabajo práctico entregable 1/2 (materia completa)\n",
    "\n",
    "**Estudiante:**\n",
    "- [Chevallier-Boutell, Ignacio José.](https://www.linkedin.com/in/nachocheva/)\n",
    "\n",
    "**Docentes:**\n",
    "- Palombarini, Jorge (Mercado Libre).\n",
    "- Barsce, Juan Cruz (Mercado Libre).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver los videos de las ejecuciones hay que tener instalado ffmpeg (`apt-get install ffmpeg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cliff walking: descripción del problema\n",
    "\n",
    "Cliff walking es un juego que involucra moverse sobre un mapa (grilla 4x12) desde un punto inicial (Start, S) hasta un punto final (Goal, G), evitando caer por el precipicio (The Cliff).\n",
    "\n",
    "![](https://github.com/GIDISIA/RLDiplodatos/blob/master/images/cliffwalking.png?raw=1)\n",
    "\n",
    "Imagen: Sutton y Barto, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comenzar un episodio, el jugador se ubica en el elemento [3, 0] de la grilla y debe llegar hasta el elemento [3, 11] para terminar dicho episodio. El precipio se ubica en [3, 1:11]. Cuando el jugador llega a uno de estos elementos, *se cae por el precipicio* y vuelve al punto inicial [3, 0]. El jugador debe realizar tantos movimientos como sean necesarios para llegar a la meta y finalizar el episodio.\n",
    "\n",
    "El espacio de acciones $\\mathcal{A}$ tiene 4 elementos:\n",
    "- 0 $\\Rightarrow$ Se mueve hacia arriba\n",
    "- 1 $\\Rightarrow$ Se mueve hacia la derecha\n",
    "- 2 $\\Rightarrow$ Se mueve hacia abajo\n",
    "- 3 $\\Rightarrow$ Se mueve hacia la izquierda\n",
    "\n",
    "A pesar de que la grilla tiene 48 elementos, el jugador no puede estar en el precipicio, ya que vuelve al punto S, y tampoco puede estar en la meta, sino sólo llegar a ésta, ya que es el estado terminal del episodio. Esto resulta en que el espacio de estados $\\mathcal{S}$ tiene 37 elementos. Para calcular el valor del estado asociado a cada elemento del mapa debemos calcular\n",
    "    $$\\text{Fila Actual} \\times \\text{Número de Columnas} + \\text{Columna Actual}$$\n",
    "\n",
    "donde debemos contar desde 0. De esta manera, el estado inicial es el estado 36.\n",
    "\n",
    "Se cumple que $\\mathcal{A}, \\mathcal{S} \\in \\mathbb{N}$, teniendo al cero como primer elemento.\n",
    "\n",
    "La función de recompensa es tal que el jugador recibe:\n",
    "- $-100$ cuando cae el precipicio.\n",
    "- $-1$ en todos los demás casos.\n",
    "\n",
    "Recorrer el camino óptimo (Optimal path) requiere de 13 pasos temporales, obteniendo un retorno de -13. Por otro lado, recorrer el camino seguro (Safer path) requiere de 17 pasos temporales, obteniendo un retorno de -17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Ejercicio: SARSA + $\\epsilon$-greedy\n",
    "\n",
    "En esta actividad se implementa y ejecuta el algoritmo SARSA con una política $\\epsilon$-greedy. Se estudia además el efecto de los hiperparámetros sobre la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heurística de selección de acciones: política\n",
    "\n",
    "Se define a $\\epsilon$-greedy como el métodos para decidir qué acciones tomar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epGreedy(\n",
    "    state: int,\n",
    "    actions: range,\n",
    "    q: dict,\n",
    "    hyperparameters: dict,\n",
    "    random_state: np.random.RandomState\n",
    ") -> int:\n",
    "\n",
    "    \"\"\"\n",
    "    Elije una acción de acuerdo a una política de exploración-explotación \n",
    "    épsilon-greedy.\n",
    "    Args:\n",
    "        state: estado actual del agente\n",
    "        actions: lista de acciones posibles\n",
    "        q: diccionario de valores de estado-acción\n",
    "        hyperparameters: hiperparámetros del algoritmo de aprendizaje\n",
    "        random_state: generador de números aleatorios\n",
    "    \"\"\"\n",
    "\n",
    "    # Lista de valores q asociados a un estado-acción\n",
    "    q_values = [q.get((state, a)) for a in actions]\n",
    "\n",
    "    # Toma el valor máximo\n",
    "    max_q = max(q_values)\n",
    "\n",
    "    # Puede haber más de un valor máximo\n",
    "    count = q_values.count(max_q)\n",
    "\n",
    "    # Sortemaos un número aleatorio y comparamos con épsilon\n",
    "    if random_state.uniform() < hyperparameters['epsilon']:\n",
    "        # Exploramos, seleccionando una acción aleatoriamente\n",
    "        return random_state.choice(actions)\n",
    "\n",
    "    # Al no cumplirse la condición, explotamos\n",
    "    elif count > 1:\n",
    "        # Hay más de un valor máximo. Sorteamos alguno de ellos\n",
    "        best = [i for i in range(len(actions)) if q_values[i] == max_q]\n",
    "        i = random_state.choice(best)\n",
    "    \n",
    "    else:\n",
    "        # Hay un único valor máximo, eligiendo el correspondiente estado-acción\n",
    "        i = q_values.index(max_q)\n",
    "\n",
    "    return actions[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje por diferencias temporales\n",
    "\n",
    "Se define a SARSA como el método de aprendizaje, tomando una transición y cambiando el diccionario de los valores de Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(\n",
    "    state: int,\n",
    "    action: int,\n",
    "    reward: int,\n",
    "    next_state: int,\n",
    "    next_action: int,\n",
    "    hyperparameters: dict,\n",
    "    q: dict\n",
    ") -> Tuple[int, int]:\n",
    "\n",
    "    \"\"\"\n",
    "    Realiza una actualización según el algoritmo SARSA, para una transición de \n",
    "    estado dada.\n",
    "    Args:\n",
    "        state: estado actual del agente\n",
    "        action: acción actual ejecutada por el agente\n",
    "        reward: recompensa recibida al ejecutar la acción\n",
    "        next_state: próximo estado del agente\n",
    "        next_action: próxima acción del agente\n",
    "        hyperparameters: hiperparámetros del algoritmo de aprendizaje\n",
    "        q: diccionario de valores de estado-acción\n",
    "    \"\"\"\n",
    "\n",
    "    # Actualizo el valor del estado-acción\n",
    "    Target = reward + hyperparameters['gamma']  * q[(next_state, next_action)]\n",
    "    TD_error = Target - q[(state, action)]\n",
    "    q[(state, action)] += hyperparameters['alpha'] * TD_error\n",
    "\n",
    "    # Actualizo el estado\n",
    "    state = next_state\n",
    "\n",
    "    # Actualizo la acción\n",
    "    action = next_action\n",
    "\n",
    "    return state, action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteraciones\n",
    "\n",
    "Se define la manera en la que itera el algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    policy: Callable,\n",
    "    learning_function: Callable,\n",
    "    hyperparameters: dict,\n",
    "    episodes_to_run: int,\n",
    "    env: gym.Env,\n",
    "    actions: range,\n",
    "    random_state: np.random.RandomState,\n",
    "    max_iter: int\n",
    ") -> Tuple[np.ndarray, np.ndarray, int, int, int, dict]:\n",
    "\n",
    "    \"\"\"\n",
    "    Corre el algoritmo de RL.\n",
    "    Args:\n",
    "        policy: huerística de selección de acciones\n",
    "        learning_function: función de actualización de algoritmo de aprendizaje\n",
    "        hyperparameters: hiperparámetros del algoritmo de aprendizaje\n",
    "        episodes_to_run: cantidad de episodios a ejecutar\n",
    "        env: entorno de Gymnasium\n",
    "        actions: lista de acciones posibles\n",
    "        random_state: generador de números aleatorios\n",
    "        max_iter: cantidad máxima de pasos temporales\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicialización del diccionario de valores de estado-acción\n",
    "    q = {}\n",
    "    for s in range(49):\n",
    "        for a in range(4):\n",
    "            q[(s, a)] = 0.0\n",
    "\n",
    "    # Registra la cantidad de pasos de cada episodio\n",
    "    timesteps_of_episode = []\n",
    "\n",
    "    # Registra el retorno de cada episodio\n",
    "    return_of_episode = []\n",
    "\n",
    "    # Casuísticas de finalización\n",
    "    goal, drop, early = 0, 0, 0\n",
    "\n",
    "    # Loop sobre los episodios\n",
    "    for _ in range(episodes_to_run):\n",
    "        # Instancea un nuevo agente en cada episodio\n",
    "        # Fin del episodio: llegar a la salida o superar max_iter\n",
    "\n",
    "        # Reinicia el entorno, obteniendo el estado inicial del mismo\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # Retorno del episodio\n",
    "        episode_return = 0\n",
    "\n",
    "        # Contador de pasos temporales\n",
    "        t = 0\n",
    "\n",
    "        # Flag de finalización de iteración actual\n",
    "        done = False\n",
    "\n",
    "        # Elige la primera acción a ejecutar\n",
    "        action = policy(\n",
    "            state, actions, q, hyperparameters, random_state)\n",
    "\n",
    "        while not done:\n",
    "            # El agente ejecuta la acción elegida y obtiene los resultados\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Acumulamos recompensa\n",
    "            episode_return += reward\n",
    "\n",
    "            # Elige la nueva acción a ejecutar partiendo desde el nuevo estado\n",
    "            next_action = policy(\n",
    "                next_state, actions, q, hyperparameters, random_state)\n",
    "\n",
    "            # Mecanismo de aprendizaje por TD\n",
    "            state, action = learning_function(\n",
    "                state, action, reward, next_state,next_action, hyperparameters, q)\n",
    "\n",
    "            # Análisis de convergencia\n",
    "            if terminated:\n",
    "                # El agente alcanzó el objetivo\n",
    "                goal += 1\n",
    "                timesteps_of_episode = np.append(\n",
    "                    timesteps_of_episode, [int(t + 1)])\n",
    "                return_of_episode = np.append(return_of_episode, episode_return)\n",
    "                done = True\n",
    "\n",
    "            elif truncated:\n",
    "                # El agente ejecutó una acción inválida\n",
    "                drop += 1\n",
    "                timesteps_of_episode = np.append(\n",
    "                    timesteps_of_episode, [int(t + 1)])\n",
    "                return_of_episode = np.append(return_of_episode, episode_return)\n",
    "                done = True\n",
    "\n",
    "            elif t >= max_iter:\n",
    "                # Early stopping\n",
    "                early += 1\n",
    "                timesteps_of_episode = np.append(\n",
    "                    timesteps_of_episode, [int(t + 1)])\n",
    "                return_of_episode = np.append(return_of_episode, episode_return)\n",
    "                done = True\n",
    "\n",
    "            t += 1\n",
    "\n",
    "    return timesteps_of_episode, return_of_episode, goal, drop, early, q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación\n",
    "\n",
    "Se definen el entorno, los parámetros e hiperparámetros deseados para la corrida. Luego corre y, finalmente, guarda los resultados. Se pueden ejecutar barridas de parámetros gracias a los for anidados. A continuación se tienen celdas que grafican comparativamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")#, render_mode=\"human\")\n",
    "actions = range(env.action_space.n)\n",
    "policy = epGreedy\n",
    "learning_function = SARSA\n",
    "\n",
    "max_iter = 2000\n",
    "Sw_eps = [10000] # [10000, 20000, 30000, 40000, 50000, 100000, 150000, 200000] # [500, 5000, 10000, 20000]\n",
    "Sw_a = [0.05] # [0.005, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1]\n",
    "Sw_g = [0.65] # [0.1, 0.25, 0.5, 0.6, 0.65, 0.7, 0.75, 0.9, 1]\n",
    "Sw_e = [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1] # [0.001]\n",
    "\n",
    "for p_eps in Sw_eps:\n",
    "    episodes_to_run = p_eps\n",
    "    for p_a in Sw_a:\n",
    "        for p_g in Sw_g:\n",
    "            for p_e in Sw_e:\n",
    "                hyperparameters = {\n",
    "                    \"alpha\": p_a,\n",
    "                    \"gamma\": p_g,\n",
    "                    \"epsilon\": p_e,\n",
    "                }\n",
    "\n",
    "                random_state = np.random.RandomState(1994)\n",
    "                # random_state = np.random.RandomState(p_eps)\n",
    "\n",
    "                start = time.time()\n",
    "                ep_timesteps, ep_return, ep_goal, ep_drop, ep_early, q = run(\n",
    "                    policy, \n",
    "                    learning_function,\n",
    "                    hyperparameters,\n",
    "                    episodes_to_run,\n",
    "                    env,\n",
    "                    actions,\n",
    "                    random_state,\n",
    "                    max_iter\n",
    "                )\n",
    "                WallTime = time.time() - start\n",
    "\n",
    "                env.close()\n",
    "\n",
    "                # Guardamos los resultados\n",
    "                runType = f'{learning_function.__name__}-{policy.__name__}'\n",
    "                runPar = f'its-{max_iter}_eps-{episodes_to_run}'\n",
    "                runHPar = f'a-{hyperparameters[\"alpha\"]}_g-{hyperparameters[\"gamma\"]}_e-{hyperparameters[\"epsilon\"]}'\n",
    "\n",
    "                with open(f'Outputs/Lab1/{runType}_{runPar}_{runHPar}.csv', 'w') as f:\n",
    "                    f.write('Goal\\tDrop\\tEarly\\tTotal\\n')\n",
    "                    f.write(f'{ep_goal}\\t{ep_drop}\\t{ep_early}\\t{episodes_to_run}\\n')\n",
    "\n",
    "                    f.write(f'Wall Time[s]\\t{WallTime}\\n')\n",
    "                    \n",
    "                    f.write('State\\tAction\\tQ-value\\n')\n",
    "                    for s in range(49):\n",
    "                        for a in range(4):\n",
    "                            f.write(f'{s}\\t{a}\\t{q[(s,a)]:.6f}\\n')\n",
    "\n",
    "                    f.write('Return\\tTimeSteps\\n')\n",
    "                    for i in range(episodes_to_run):\n",
    "                        f.write(f'{ep_return[i]:.0f}\\t{ep_timesteps[i]:.0f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promediación de episodios y pasos temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_evol(Arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Suaviza la curva de aprendizaje o de recompensa.\n",
    "    Args:\n",
    "        Arr: array a suavizar\n",
    "    \"\"\"\n",
    "\n",
    "    eps_val = np.linspace(1, len(Arr) + 1, len(Arr) + 1)\n",
    "    cum_vals = np.cumsum(Arr)\n",
    "\n",
    "    val_per_eps = [cum_vals[i] / eps_val[i] for i in range(len(cum_vals))]\n",
    "\n",
    "    return np.array(val_per_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figura para comparar lo que ocurre al cambiar la cantidad de episodios y, además, ver que es determinista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for p_eps in [20000, 10000, 5000, 500]:\n",
    "\n",
    "    # Semillas aleatorias (se usa el número de episodios p_eps correspondiente para reproducibilidad en la \"aleatoriedad\")\n",
    "    File = pd.read_csv(f'Outputs/Lab1/Determinista/SARSA-epGreedy_its-2000_eps-{p_eps}_a-0.5_g-1_e-0.1_randomSeed.csv', header=None, sep='\\t').iloc[201:, :2].to_numpy().astype(int)\n",
    "    axs[0, 0].plot(mean_evol(File[:,0]))\n",
    "    axs[1, 0].plot(mean_evol(File[:,1]))\n",
    "\n",
    "    # Semillas fija (1994)\n",
    "    File = pd.read_csv(f'Outputs/Lab1/Determinista/SARSA-epGreedy_its-2000_eps-{p_eps}_a-0.5_g-1_e-0.1_fixedSeed.csv', header=None, sep='\\t').iloc[201:, :2].to_numpy().astype(int)\n",
    "    axs[0, 1].plot(mean_evol(File[:,0]))\n",
    "    axs[1, 1].plot(mean_evol(File[:,1]))\n",
    "\n",
    "axs[0, 0].set_title('Semillas aleatorias')\n",
    "axs[0, 1].set_title('Semilla fija')\n",
    "\n",
    "for k in range(2):\n",
    "    axs[0, k].axhline(-13, color='red', linestyle=':', label='Óptimo')\n",
    "    axs[0, k].axhline(-17, color='blue', linestyle=':', label='Seguro')\n",
    "    axs[0, k].legend(loc='lower right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "    axs[0, k].set_xlabel('Episodios')\n",
    "    axs[0, k].set_ylabel('Retorno promedio')\n",
    "    axs[0, k].set_ylim(-50, -10)\n",
    "\n",
    "    axs[1, k].axhline(13, color='red', linestyle=':', label='Óptimo')\n",
    "    axs[1, k].axhline(17, color='blue', linestyle=':', label='Seguro')\n",
    "    axs[1, k].legend(loc='upper right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "    axs[1, k].set_xlabel('Episodios')\n",
    "    axs[1, k].set_ylabel('Pasos temporales promedio')\n",
    "    axs[1, k].set_ylim(10, 50)\n",
    "\n",
    "plt.savefig('Outputs/Lab1/Determinista/SARSA-epGreedy_its-2000_a-0.5_g-1_e-0.1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figura para comparar barrido de $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for p_a in [0.005, 0.01, 0.05, 0.1, 0.25, 0.5]:\n",
    "\n",
    "    File = pd.read_csv(f'Outputs/Lab1/Alpha/SARSA-epGreedy_its-2000_eps-10000_a-{p_a}_g-1_e-0.1.csv', header=None, sep='\\t').iloc[201:, :2].to_numpy().astype(int)\n",
    "    \n",
    "    axs[0, 0].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "    axs[0, 1].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "    axs[0, 2].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "\n",
    "    axs[1, 0].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "    axs[1, 1].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "    axs[1, 2].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "\n",
    "for p_a in [0.75, 0.9, 1]:\n",
    "\n",
    "    File = pd.read_csv(f'Outputs/Lab1/Alpha/SARSA-epGreedy_its-2000_eps-10000_a-{p_a}_g-1_e-0.1.csv', header=None, sep='\\t').iloc[201:, :2].to_numpy().astype(int)\n",
    "    \n",
    "    axs[0, 0].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "    axs[0, 1].plot(mean_evol(File[:,0]), label=rf'$\\alpha$ = {p_a}')\n",
    "\n",
    "    axs[1, 0].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "    axs[1, 1].plot(mean_evol(File[:,1]), label=rf'$\\alpha$ = {p_a}')\n",
    "\n",
    "axs[0, 0].set_ylim(-1000, 0)\n",
    "axs[0, 1].set_ylim(-100, -10)\n",
    "axs[0, 2].set_ylim(-50, -10)\n",
    "\n",
    "axs[1, 0].set_ylim(0, 400)\n",
    "axs[1, 1].set_ylim(10, 70)\n",
    "axs[1, 2].set_ylim(10, 35)\n",
    "\n",
    "for k in range(3):\n",
    "    axs[0, k].axhline(-13, color='red', linestyle=':', label='Óptimo')\n",
    "    axs[0, k].axhline(-17, color='blue', linestyle=':', label='Seguro')\n",
    "    axs[0, k].legend(loc='lower right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "    axs[0, k].set_xlabel('Episodios')\n",
    "    axs[0, k].set_ylabel('Retorno promedio')\n",
    "\n",
    "    axs[1, k].axhline(13, color='red', linestyle=':', label='Óptimo')\n",
    "    axs[1, k].axhline(17, color='blue', linestyle=':', label='Seguro')\n",
    "    axs[1, k].legend(loc='upper right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "    axs[1, k].set_xlabel('Episodios')\n",
    "    axs[1, k].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "plt.savefig('Outputs/Lab1/Alpha/SARSA-epGreedy_its-2000_eps-10000_g-1_e-0.1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figura para comparar barrido de $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2,figsize=(12, 10))\n",
    "\n",
    "for p_g in [1, 0.9, 0.75, 0.7, 0.65, 0.6, 0.5, 0.25, 0.1]:\n",
    "\n",
    "    File = pd.read_csv(f'Outputs/Lab1/Gamma/SARSA-epGreedy_its-2000_eps-10000_a-0.05_g-{p_g}_e-0.1.csv', header=None, sep='\\t').iloc[201:, :2].to_numpy().astype(int)\n",
    "\n",
    "    axs[0, 0].plot(mean_evol(File[:,0]), label=rf'$\\gamma$ = {p_g}')\n",
    "    axs[1, 0].plot(mean_evol(File[:,1]), label=rf'$\\gamma$ = {p_g}')\n",
    "\n",
    "for p_g in [1, 0.9, 0.75, 0.7, 0.65, 0.6]:\n",
    "\n",
    "    File = pd.read_csv(f'Outputs/Lab1/Gamma/SARSA-epGreedy_its-2000_eps-10000_a-0.05_g-{p_g}_e-0.1.csv', header=None, sep='\\t').iloc[201:, :2].to_numpy().astype(int)\n",
    "\n",
    "    axs[0, 1].plot(mean_evol(File[:,0]), label=rf'$\\gamma$ = {p_g}')\n",
    "    axs[1, 1].plot(mean_evol(File[:,1]), label=rf'$\\gamma$ = {p_g}')\n",
    "\n",
    "axs[0, 0].set_ylim(-2500, 0)\n",
    "axs[0, 1].set_ylim(-50, -10)\n",
    "\n",
    "axs[1, 0].set_ylim(0, 2000)\n",
    "axs[1, 1].set_ylim(10, 50)\n",
    "\n",
    "for k in range(2):\n",
    "    axs[0, k].axhline(-13, color='red', linestyle=':', label='Óptimo')\n",
    "    axs[0, k].axhline(-17, color='blue', linestyle=':', label='Seguro')\n",
    "    axs[0, k].legend(loc='lower right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "    axs[0, k].set_xlabel('Episodios')\n",
    "    axs[0, k].set_ylabel('Retorno promedio')\n",
    "\n",
    "    axs[1, k].axhline(13, color='red', linestyle=':', label='Óptimo')\n",
    "    axs[1, k].axhline(17, color='blue', linestyle=':', label='Seguro')\n",
    "    axs[1, k].legend(loc='upper right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "    axs[1, k].set_xlabel('Episodios')\n",
    "    axs[1, k].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "plt.savefig('Outputs/Lab1/Gamma/SARSA-epGreedy_its-2000_eps-10000_a-0.05_e-0.1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figura para comparar barrido de $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2,figsize=(12, 10))\n",
    "\n",
    "for p_e in [0, 0.001, 0.005, 0.01, 0.05, 0.1]:\n",
    "\n",
    "    File = pd.read_csv(f'Outputs/Lab1/Epsilon/SARSA-epGreedy_its-2000_eps-10000_a-0.05_g-0.65_e-{p_e}.csv', header=None, sep='\\t').iloc[201:, :2].to_numpy().astype(int)\n",
    "\n",
    "    axs[0, 0].plot(mean_evol(File[:,0]), label=rf'$\\epsilon$ = {p_e}')\n",
    "    axs[1, 0].plot(mean_evol(File[:,1]), label=rf'$\\epsilon$ = {p_e}')\n",
    "\n",
    "    axs[0, 1].plot(mean_evol(File[:,0]), label=rf'$\\epsilon$ = {p_e}')\n",
    "    axs[1, 1].plot(mean_evol(File[:,1]), label=rf'$\\epsilon$ = {p_e}')\n",
    "\n",
    "for p_e in [0.25, 0.5, 0.75, 0.9, 1]:\n",
    "\n",
    "    File = pd.read_csv(f'Outputs/Lab1/Epsilon/SARSA-epGreedy_its-2000_eps-10000_a-0.05_g-0.65_e-{p_e}.csv', header=None, sep='\\t').iloc[201:, :2].to_numpy().astype(int)\n",
    "\n",
    "    axs[0, 0].plot(mean_evol(File[:,0]), label=rf'$\\epsilon$ = {p_e}')\n",
    "    axs[1, 0].plot(mean_evol(File[:,1]), label=rf'$\\epsilon$ = {p_e}')\n",
    "\n",
    "axs[0, 0].set_ylim(-22000, 0)\n",
    "axs[1, 0].set_ylim(0, 2000)\n",
    "\n",
    "axs[0, 1].set_ylim(-50, -10)\n",
    "axs[1, 1].set_ylim(10, 50)\n",
    "\n",
    "for k in range(2):\n",
    "    axs[0, k].axhline(-13, color='red', linestyle=':', label='Óptimo')\n",
    "    axs[0, k].axhline(-17, color='blue', linestyle=':', label='Seguro')\n",
    "    axs[0, k].legend(loc='lower right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "    axs[0, k].set_xlabel('Episodios')\n",
    "    axs[0, k].set_ylabel('Retorno promedio')\n",
    "\n",
    "    axs[1, k].axhline(13, color='red', linestyle=':', label='Óptimo')\n",
    "    axs[1, k].axhline(17, color='blue', linestyle=':', label='Seguro')\n",
    "    axs[1, k].legend(loc='upper right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "    axs[1, k].set_xlabel('Episodios')\n",
    "    axs[1, k].set_ylabel('Pasos temporales promedio')\n",
    "\n",
    "plt.savefig('Outputs/Lab1/Epsilon/SARSA-epGreedy_its-2000_eps-10000_a-0.05_g-0.65.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figura para comparar lo que ocurre al cambiar la cantidad de episodios, utilizando hiperparámetros óptimos y dejando fija o libre la semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for p_eps in [200000, 150000,  100000, 50000, 40000, 30000, 20000, 10000]:\n",
    "\n",
    "    # Semillas aleatorias (se usa el número de episodios p_eps correspondiente para reproducibilidad en la \"aleatoriedad\")\n",
    "    # File = pd.read_csv(f'Outputs/Lab1/Optimos/SARSA-epGreedy_its-2000_eps-{p_eps}_a-0.05_g-0.65_e-0.001_randomSeed.csv', header=None, sep='\\t').iloc[201:, :2].to_numpy().astype(int)\n",
    "    # axs[0, 0].plot(mean_evol(File[:,0]))\n",
    "    # axs[1, 0].plot(mean_evol(File[:,1]))\n",
    "\n",
    "    # # Semillas fija (1994)\n",
    "    File = pd.read_csv(f'Outputs/Lab1/Optimos/SARSA-epGreedy_its-2000_eps-{p_eps}_a-0.05_g-0.65_e-0.001_fixedSeed.csv', header=None, sep='\\t').iloc[201:, :2].to_numpy().astype(int)\n",
    "    axs[0, 1].plot(mean_evol(File[:,0]))\n",
    "    axs[1, 1].plot(mean_evol(File[:,1]))\n",
    "\n",
    "axs[0, 0].set_title('Semillas aleatorias')\n",
    "axs[0, 1].set_title('Semilla fija')\n",
    "\n",
    "for k in range(2):\n",
    "    axs[0, k].axhline(-13, color='red', linestyle=':', label='Óptimo')\n",
    "    axs[0, k].axhline(-17, color='blue', linestyle=':', label='Seguro')\n",
    "    axs[0, k].legend(loc='lower right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "    axs[0, k].set_xlabel('Episodios')\n",
    "    axs[0, k].set_ylabel('Retorno promedio')\n",
    "    axs[0, k].set_ylim(-25, -10)\n",
    "\n",
    "    axs[1, k].axhline(13, color='red', linestyle=':', label='Óptimo')\n",
    "    axs[1, k].axhline(17, color='blue', linestyle=':', label='Seguro')\n",
    "    axs[1, k].legend(loc='upper right', frameon=False, fancybox=True, title='Camino')\n",
    "\n",
    "    axs[1, k].set_xlabel('Episodios')\n",
    "    axs[1, k].set_ylabel('Pasos temporales promedio')\n",
    "    axs[1, k].set_ylim(10, 25)\n",
    "\n",
    "plt.savefig('Outputs/Lab1/Optimos/SARSA-epGreedy_its-2000_a-0.5_g-1_e-0.1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función ploteo grilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_value_matrix(q) -> None:\n",
    "    '''\n",
    "    Grafica el mapa del juego mostrando la ruta más conveniente según el\n",
    "    algoritmo de aprendizaje utilizado.\n",
    "    '''\n",
    "\n",
    "    n_rows = 4\n",
    "    n_columns = 12\n",
    "    n_actions = 4\n",
    "\n",
    "    # se procede con los cálculos previos a la graficación de la matriz de valor\n",
    "    q_value_matrix = np.empty((n_rows, n_columns))\n",
    "    for row in range(n_rows):\n",
    "        for column in range(n_columns):\n",
    "            state_values = []\n",
    "\n",
    "            for action in range(n_actions):\n",
    "                state_values.append(\n",
    "                    q.get((row * n_columns + column, action), -100))\n",
    "\n",
    "            maximum_value = max(\n",
    "                state_values\n",
    "            )  # determinamos la acción que arroja máximo valor\n",
    "\n",
    "            q_value_matrix[row, column] = maximum_value\n",
    "\n",
    "    # el valor del estado objetivo se asigna en -1 (reward recibido al llegar)\n",
    "    # para que se coloree de forma apropiada\n",
    "    q_value_matrix[3, 11] = -1\n",
    "\n",
    "    # se grafica la matriz de valor\n",
    "    plt.imshow(q_value_matrix, cmap=plt.cm.RdYlGn)\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "\n",
    "    for row, column in itertools.product(\n",
    "        range(q_value_matrix.shape[0]), range(q_value_matrix.shape[1])\n",
    "    ):\n",
    "        left_action = q.get((row * n_columns + column, 3), -1000)\n",
    "        down_action = q.get((row * n_columns + column, 2), -1000)\n",
    "        right_action = q.get((row * n_columns + column, 1), -1000)\n",
    "        up_action = q.get((row * n_columns + column, 0), -1000)\n",
    "\n",
    "        arrow_direction = \"D\"\n",
    "        best_action = down_action\n",
    "\n",
    "        if best_action < right_action:\n",
    "            arrow_direction = \"R\"\n",
    "            best_action = right_action\n",
    "        if best_action < left_action:\n",
    "            arrow_direction = \"L\"\n",
    "            best_action = left_action\n",
    "        if best_action < up_action:\n",
    "            arrow_direction = \"U\"\n",
    "            best_action = up_action\n",
    "        if best_action == -1:\n",
    "            arrow_direction = \"\"\n",
    "\n",
    "        # notar que column, row están invertidos en orden en la línea de abajo\n",
    "        # porque representan a x,y del plot\n",
    "        plt.text(column, row, arrow_direction, horizontalalignment=\"center\")\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('Ruta más conveniente')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6Ci7JfDdaPs/g38IjJj7A",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
