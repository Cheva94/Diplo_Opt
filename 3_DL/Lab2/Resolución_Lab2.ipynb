{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYg4jx_iZ3Ey"
      },
      "source": [
        "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
        "\n",
        "## Introducción al aprendizaje profundo\n",
        "\n",
        "### Trabajo práctico entregable 2/2\n",
        "\n",
        "- **Estudiantes:**\n",
        "    - [Chevallier-Boutell, Ignacio José](https://www.linkedin.com/in/nachocheva/) (materia completa).\n",
        "    - Gastelu, Gabriela (materia completa).\n",
        "    - Spano, Marcelo (materia completa).\n",
        "\n",
        "- **Docentes:**\n",
        "    - Johanna Analiz Frau (Mercado Libre).\n",
        "    - Nindiría Armenta Guerrero (fyo).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81o1pr1wZ3E2"
      },
      "source": [
        "## Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkfYanqJZ3E3"
      },
      "outputs": [],
      "source": [
        "# Importando líbrerias\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Para que todo sea reproducible\n",
        "torch.manual_seed(1994)\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlo6TbbJTaOt"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYYyDSMdIDkM"
      },
      "source": [
        "## Funciones útiles"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparación"
      ],
      "metadata": {
        "id": "hEm-xe1vW9W1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM6_yWpCYugV"
      },
      "outputs": [],
      "source": [
        "def data_preparation(BATCH_SIZE, transform):\n",
        "\n",
        "    # Download and load the training data\n",
        "    trainval_set = datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    # 40000 para entrenamiento y 10000 para validación\n",
        "    train_split = 40000\n",
        "    validate_split = 10000\n",
        "\n",
        "    train_set, val_set = random_split(trainval_set, [train_split, validate_split])\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "\n",
        "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "\n",
        "    # Download and load the test data\n",
        "    test_set = datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_set, train_loader, val_set, val_loader, test_set, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento y validación"
      ],
      "metadata": {
        "id": "5lbFL2w2XA74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
        "    print('>>> Running...')\n",
        "    # Ciclo for para el número de épocas\n",
        "    train_loss_history, train_acc_history, val_loss_history, val_acc_history = [], [], [], []\n",
        "    all_labels, all_preds = [], []\n",
        "    model = model.to(device=device)\n",
        "    start = time.time()\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        val_loss = 0.0\n",
        "        val_acc = 0.0\n",
        "\n",
        "        # Establecer el modelo en modo de entrenamiento\n",
        "        model.train()\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        # Iterar sobre los datos de entrenamiento\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
        "            optimizer.zero_grad()\n",
        "            predicted_outputs = model(inputs)\n",
        "            # Calcular la pérdida\n",
        "            loss = criterion(predicted_outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Acumular la pérdida y la precisión\n",
        "            train_loss += loss.item()\n",
        "            # La etiqueta con mayor valor será nuestra predicción\n",
        "            _, predicted = torch.max(predicted_outputs , 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Calcular la pérdida y precisión promedio del entrenamiento\n",
        "        train_loss /= len(train_loader)\n",
        "        train_loss_history.append(train_loss)\n",
        "        train_acc = accuracy_score(y_true, y_pred)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
        "                # Run the forward pass\n",
        "                predicted_outputs = model(inputs)\n",
        "                # Compute loss\n",
        "                loss = criterion(predicted_outputs, labels)\n",
        "                # Accumulate the average loss of the mini-batch\n",
        "                val_loss += loss.item()\n",
        "                # The label with the highest value will be our prediction\n",
        "                _, predicted = torch.max(predicted_outputs , 1)\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "                y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # calculate the average validation loss and accuracy\n",
        "        val_loss /= len(val_loader)\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_acc = accuracy_score(y_true, y_pred)\n",
        "        val_acc_history.append(val_acc)\n",
        "        all_labels.extend(y_true)\n",
        "        all_preds.extend(y_pred)\n",
        "\n",
        "        if (epoch % 10 == 0) and (epoch != num_epochs):\n",
        "            print(f'Voy por la época {epoch}! :)')\n",
        "            print(f'\\t Train >> Loss: {train_loss:.4f} - Acc: {train_acc:.4f}')\n",
        "            print(f'\\t   Val >> Loss: {val_loss:.4f} - Acc: {val_acc:.4f}')\n",
        "        elif epoch == num_epochs:\n",
        "            WallTime = time.time() - start\n",
        "            print(f'\\tTerminé! :D')\n",
        "            print(f'\\t Train >> Loss: {train_loss:.4f} - Acc: {train_acc:.4f}')\n",
        "            print(f'\\t   Val >> Loss: {val_loss:.4f} - Acc: {val_acc:.4f}')\n",
        "            print(f'<<< WallTime = {WallTime/60:.2f} min')\n",
        "\n",
        "    modelo = model.__class__.__name__\n",
        "    activacion = model.actfunc\n",
        "    GD = optimizer.__class__.__name__\n",
        "    LR = optimizer.param_groups[0]['lr']\n",
        "    Walltime = f'{WallTime/60:.2f}'\n",
        "    titulo = f'{modelo}_{activacion}_{GD}_LR-{LR}_Walltime-{Walltime}min'\n",
        "\n",
        "    # Guardamos métricas\n",
        "    Arr = np.array([train_loss_history, train_acc_history, val_loss_history, val_acc_history]).T\n",
        "    df = pd.DataFrame(Arr, columns=['train_loss_history', 'train_acc_history', 'val_loss_history', 'val_acc_history'])\n",
        "    df.to_csv(titulo+'_Metricas.csv', index=False)\n",
        "\n",
        "    # Guardamos etiquetas\n",
        "    Arr = np.array([all_labels, all_preds]).T\n",
        "    df = pd.DataFrame(Arr, columns=['all_labels', 'all_preds'])\n",
        "    df.to_csv(titulo+'_Etiquetas.csv', index=False)\n",
        "\n",
        "    print(titulo)"
      ],
      "metadata": {
        "id": "J2M8U1uF2fOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gráficas"
      ],
      "metadata": {
        "id": "jPEVR6DGXDkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotting_confusion_matrix(path, CIFAR_CLASSES):\n",
        "    Arr = pd.read_csv(path).to_numpy()\n",
        "\n",
        "    # Plotting Confusion Matrix\n",
        "    fig, axs = plt.subplots(1, 2, figsize = (14, 6))\n",
        "\n",
        "    # Implementing visualization of Confusion Matrix\n",
        "    c_m = confusion_matrix(Arr[:,0], Arr[:,1])\n",
        "    c_m_normalized = confusion_matrix(Arr[:,0], Arr[:,1], normalize='true').round(3)\n",
        "\n",
        "    ConfusionMatrixDisplay(c_m, display_labels=CIFAR_CLASSES).plot(cmap='Greys', xticks_rotation=25, ax=axs[0])\n",
        "    ConfusionMatrixDisplay(c_m_normalized*100, display_labels=CIFAR_CLASSES).plot(cmap='Greys', xticks_rotation=25, ax=axs[1],)\n",
        "\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    axs[0].set_title('Valores absolutos', fontsize=14)\n",
        "    axs[1].set_title('Valores porcentuales', fontsize=14)\n",
        "\n",
        "    titulo = path.split('_Etiquetas')[0]\n",
        "    if len(titulo.split('Outputs/')) == 2:\n",
        "        titulo = titulo.split('Outputs/')[1]\n",
        "    plt.suptitle(titulo, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_and_accuracy(path):\n",
        "    Arr = pd.read_csv(path).to_numpy()\n",
        "\n",
        "    xAxis = [x+1 for x in range(Arr.shape[0])]\n",
        "    xTicks = [(k+1)*10 for k in range(int(100/10))]\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize = (10, 4))\n",
        "\n",
        "    # Plot the training and validation loss\n",
        "    axs[0].plot(xAxis, Arr[:, 0], label='train loss')\n",
        "    axs[0].plot(xAxis, Arr[:, 2], label='val loss')\n",
        "    axs[0].grid()\n",
        "    axs[0].set_xlabel('Épocas')\n",
        "    axs[0].set_xticks(xTicks)\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[0].legend(loc='upper right')\n",
        "\n",
        "\n",
        "    # Plot the training and validation accuracy\n",
        "    axs[1].plot(xAxis, Arr[:, 1], label='train acc')\n",
        "    axs[1].plot(xAxis, Arr[:, 3], label='val acc')\n",
        "    axs[1].grid()\n",
        "    axs[1].set_xlabel('Épocas')\n",
        "    axs[1].set_xticks(xTicks)\n",
        "    axs[1].set_ylabel('Acc')\n",
        "    axs[1].legend(loc='lower right')\n",
        "\n",
        "    titulo = path.split('_Metricas')[0]\n",
        "    if len(titulo.split('Outputs/')) == 2:\n",
        "        titulo = titulo.split('Outputs/')[1]\n",
        "\n",
        "    plt.suptitle(titulo, fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dp3-mbQV2qKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b6ufcuoZ3E4"
      },
      "source": [
        "---\n",
        "# Descripción, carga y preprocesamiento del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1OsnyZOIDkQ"
      },
      "source": [
        "El dataset a utilizar es el **[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)**, el cual es un conjunto estándar para hacer reconocimiento de imágenes. Consta de 60000 imágenes RGB de 32x32 divididas en 10 clases mutuamente excluyentes (avión, auto, pájaro, gato, ciervo, perro, rana, caballo, barco y camión), con 6000 imágenes por clase. De estas 60000 hay 50000 imágenes de entrenamiento y las otras 10000 son de evaluación.\n",
        "\n",
        "Nuestro objetivo es entrenar una CNN que clasifique los objetos de las imágenes dentro de alguna de las 10 categorías dadas.\n",
        "\n",
        "***Observación:*** este dataset ya está incoporado dentro de las librerías de pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HURRFhXSIDkQ"
      },
      "outputs": [],
      "source": [
        "# Tamaño de lote (default)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Número de épocas (default)\n",
        "EPOCHS = 100\n",
        "\n",
        "# Función de pérdida\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Convertimos en tensor y normalizamos, tomando el valor medio del rango\n",
        "# posible, i.e. [0; 1].\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Obtenemos el conjunto de entrenamiento y de evaluación, junto a sus loaders\n",
        "train_set, train_loader, val_set, val_loader, test_set, test_loader = data_preparation(BATCH_SIZE, transform)\n",
        "\n",
        "# Clases disponibles\n",
        "CIFAR_CLASSES = test_set.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3uO5LOp3cI1"
      },
      "source": [
        "---\n",
        "# Modelo de partida: red convolucional sin padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqQ18h673cI1"
      },
      "source": [
        "## Definición del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmcwGhbv3cI2"
      },
      "outputs": [],
      "source": [
        "class NetwithoutPadding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetwithoutPadding, self).__init__()\n",
        "        # Inicializar 2 capas convolucionales\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, padding=0)\n",
        "\n",
        "        # Inicializar función de activación. Default: Sigmoide\n",
        "        self.actfunc = nn.Sigmoid()\n",
        "\n",
        "        # Inicializar una capa de agrupamiento por máximo\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Inicializar la capa totalmente conectada\n",
        "        self.fc = nn.Linear(5 * 5 * 20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolución + Activación + Agrupamiento sobre capa 1\n",
        "        x = self.actfunc(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # # Convolución + Activación + Agrupamiento sobre capa 2\n",
        "        x = self.actfunc(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Preparar la imagen para la capa totalmente conectada\n",
        "        x = x.view(-1, self.fc.in_features)\n",
        "\n",
        "        # Aplicar la capa totalmente conectada y obtener resultado\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqaaTQ2R3cI3"
      },
      "source": [
        "## Corrida default: baseline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Instanciamos el modelo\n",
        "# model = NetwithoutPadding()\n",
        "# # Definimos el optimizador\n",
        "# optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "# # Corremos el experimento\n",
        "# train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ],
      "metadata": {
        "id": "Bc9isvXLC1qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_Sigmoid_SGD_LR-0.1_Walltime-24.35min_Metricas.csv'\n",
        "plot_loss_and_accuracy(path)"
      ],
      "metadata": {
        "id": "z98stZavEe2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_Sigmoid_SGD_LR-0.1_Walltime-24.35min_Etiquetas.csv'\n",
        "plotting_confusion_matrix(path, CIFAR_CLASSES)"
      ],
      "metadata": {
        "id": "mTKc7TXKKYhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estudio de la función de activación"
      ],
      "metadata": {
        "id": "k0J1YdT8sCFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # La sigmoide es el default\n",
        "# activation_functions = [nn.Tanh(), nn.ReLU(), nn.LeakyReLU()]\n",
        "\n",
        "# for af in activation_functions:\n",
        "#     model = NetwithoutPadding()\n",
        "#     model.actfunc = af\n",
        "#     print(model.actfunc)\n",
        "#     optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "#     train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ],
      "metadata": {
        "id": "QGheBcCMsFgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_'\n",
        "for path in ['Tanh_SGD_LR-0.1_Walltime-25.00min_Metricas.csv',\n",
        "            'ReLU_SGD_LR-0.1_Walltime-24.71min_Metricas.csv',\n",
        "            'LeakyReLU_SGD_LR-0.1_Walltime-24.77min_Metricas.csv',\n",
        "            'Sigmoid_SGD_LR-0.1_Walltime-24.35min_Metricas.csv']:\n",
        "    plot_loss_and_accuracy(prefix+path)"
      ],
      "metadata": {
        "id": "jP8NYKEQsVEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_'\n",
        "for path in ['Tanh_SGD_LR-0.1_Walltime-25.00min_Etiquetas.csv',\n",
        "            'ReLU_SGD_LR-0.1_Walltime-24.71min_Etiquetas.csv',\n",
        "            'LeakyReLU_SGD_LR-0.1_Walltime-24.77min_Etiquetas.csv',\n",
        "            'Sigmoid_SGD_LR-0.1_Walltime-24.35min_Etiquetas.csv']:\n",
        "    plotting_confusion_matrix(prefix+path, CIFAR_CLASSES)"
      ],
      "metadata": {
        "id": "HH-_ykvQwGBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estudio del optimizador"
      ],
      "metadata": {
        "id": "E9QcbJbixCPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD es el default\n",
        "optimizadores = [optim.Adam, optim.RMSprop] # [optim.Adagrad, optim.RMSprop, optim.Adam]\n",
        "\n",
        "for gd in optimizadores:\n",
        "    print(gd)\n",
        "    model = NetwithoutPadding()\n",
        "    # Adagrad no funciona sin esta línea (es un issue ya reportado)\n",
        "    model = model.to(device=device)\n",
        "    # Usamos la función de activación elegida\n",
        "    model.actfunc = nn.ReLU()\n",
        "    optimizer = gd(model.parameters(), lr=1e-1)\n",
        "    train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ],
      "metadata": {
        "id": "9U68a7OWFmmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_'\n",
        "for path in ['ReLU_SGD_LR-0.1_Walltime-24.71min_Metricas.csv',\n",
        "            'ReLU_Adagrad_LR-0.1_Walltime-24.84min_Metricas.csv',\n",
        "            'ReLU_RMSprop_LR-0.1_Walltime-min_Metricas.csv',\n",
        "            'ReLU_Adam_LR-0.1_Walltime-min_Metricas.csv']:\n",
        "    plot_loss_and_accuracy(prefix+path)"
      ],
      "metadata": {
        "id": "SHw4jp_tHbhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_'\n",
        "for path in ['ReLU_SGD_LR-0.1_Walltime-24.71min_Etiquetas.csv',\n",
        "            'ReLU_Adagrad_LR-0.1_Walltime-24.84min_Etiquetas.csv',\n",
        "            'ReLU_RMSprop_LR-0.1_Walltime-min_Etiquetas.csv',\n",
        "            'ReLU_Adam_LR-0.1_Walltime-min_Etiquetas.csv']:\n",
        "    plotting_confusion_matrix(prefix+path, CIFAR_CLASSES)"
      ],
      "metadata": {
        "id": "vvsdAdCPHhZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estudio de la tasa de aprendizaje"
      ],
      "metadata": {
        "id": "oQbQg3P_xENC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 0.1 es el default\n",
        "# alphas = [1e-2, 1e-3, 1e-4]\n",
        "\n",
        "# for lr in alphas:\n",
        "#     print(lr)\n",
        "#     model = NetwithoutPadding()\n",
        "#     model.actfunc = # poner la que elegida del estudio anterior\n",
        "#     optimizer = # optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "#     optimizer = # gd(model.parameters(), lr=1e-1)\n",
        "#     train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ],
      "metadata": {
        "id": "tR8fULBnFpMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5YJVlOwPHb7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ciH6QP8UHhvf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vYg4jx_iZ3Ey",
        "7b6ufcuoZ3E4",
        "O3uO5LOp3cI1",
        "17fSO_O23cI5",
        "py-hG15c3cI6",
        "gp66JoSR3cI7",
        "tWoJ8Jog3cI8",
        "amUB47Dd3cI_",
        "ML_YjvPr_drw",
        "XTAUvFPpNAsy",
        "VBnK5nqKO_15",
        "-1OvRv9Wn6pz",
        "ddpbSAdSDwau",
        "Un9Far-eJjWN",
        "Sk1lWQt-OB2B",
        "n-a1QJEzSBIS",
        "_9DWL-u_Xwfc"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}