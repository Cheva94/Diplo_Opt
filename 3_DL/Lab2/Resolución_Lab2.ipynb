{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYg4jx_iZ3Ey"
      },
      "source": [
        "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
        "\n",
        "## Introducción al aprendizaje profundo\n",
        "\n",
        "### Trabajo práctico entregable 2/2\n",
        "\n",
        "- **Estudiantes:**\n",
        "    - [Chevallier-Boutell, Ignacio José](https://www.linkedin.com/in/nachocheva/) (materia completa).\n",
        "    - Gastelu, Gabriela (materia completa).\n",
        "    - Spano, Marcelo (materia completa).\n",
        "\n",
        "- **Docentes:**\n",
        "    - Johanna Analiz Frau (Mercado Libre).\n",
        "    - Nindiría Armenta Guerrero (fyo).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81o1pr1wZ3E2"
      },
      "source": [
        "## Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkfYanqJZ3E3"
      },
      "outputs": [],
      "source": [
        "# Importando líbrerias\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Para que todo sea reproducible\n",
        "torch.manual_seed(1994)\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlo6TbbJTaOt"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYYyDSMdIDkM"
      },
      "source": [
        "## Funciones útiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEm-xe1vW9W1"
      },
      "source": [
        "### Preparación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM6_yWpCYugV"
      },
      "outputs": [],
      "source": [
        "def data_preparation(BATCH_SIZE, transform):\n",
        "\n",
        "    # Download and load the training data\n",
        "    trainval_set = datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "\n",
        "    # 40000 para entrenamiento y 10000 para validación\n",
        "    train_split = 40000\n",
        "    validate_split = 10000\n",
        "\n",
        "    train_set, val_set = random_split(trainval_set, [train_split, validate_split])\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "\n",
        "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "\n",
        "    # Download and load the test data\n",
        "    test_set = datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_set, train_loader, val_set, val_loader, test_set, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lbFL2w2XA74"
      },
      "source": [
        "### Entrenamiento y validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2M8U1uF2fOH"
      },
      "outputs": [],
      "source": [
        "def train_val(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
        "    print('>>> Running...')\n",
        "    # Ciclo for para el número de épocas\n",
        "    train_loss_history, train_acc_history, val_loss_history, val_acc_history = [], [], [], []\n",
        "    all_labels, all_preds = [], []\n",
        "    model = model.to(device=device)\n",
        "    start = time.time()\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        val_loss = 0.0\n",
        "        val_acc = 0.0\n",
        "\n",
        "        # Establecer el modelo en modo de entrenamiento\n",
        "        model.train()\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        # Iterar sobre los datos de entrenamiento\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
        "            optimizer.zero_grad()\n",
        "            predicted_outputs = model(inputs)\n",
        "            # Calcular la pérdida\n",
        "            loss = criterion(predicted_outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Acumular la pérdida y la precisión\n",
        "            train_loss += loss.item()\n",
        "            # La etiqueta con mayor valor será nuestra predicción\n",
        "            _, predicted = torch.max(predicted_outputs , 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Calcular la pérdida y precisión promedio del entrenamiento\n",
        "        train_loss /= len(train_loader)\n",
        "        train_loss_history.append(train_loss)\n",
        "        train_acc = accuracy_score(y_true, y_pred)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n",
        "                # Run the forward pass\n",
        "                predicted_outputs = model(inputs)\n",
        "                # Compute loss\n",
        "                loss = criterion(predicted_outputs, labels)\n",
        "                # Accumulate the average loss of the mini-batch\n",
        "                val_loss += loss.item()\n",
        "                # The label with the highest value will be our prediction\n",
        "                _, predicted = torch.max(predicted_outputs , 1)\n",
        "                y_true.extend(labels.cpu().numpy())\n",
        "                y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # calculate the average validation loss and accuracy\n",
        "        val_loss /= len(val_loader)\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_acc = accuracy_score(y_true, y_pred)\n",
        "        val_acc_history.append(val_acc)\n",
        "        all_labels.extend(y_true)\n",
        "        all_preds.extend(y_pred)\n",
        "\n",
        "        if (epoch % 10 == 0) and (epoch != num_epochs):\n",
        "            print(f'Voy por la época {epoch}! :)')\n",
        "            print(f'\\t Train >> Loss: {train_loss:.4f} - Acc: {train_acc:.4f}')\n",
        "            print(f'\\t   Val >> Loss: {val_loss:.4f} - Acc: {val_acc:.4f}')\n",
        "        elif epoch == num_epochs:\n",
        "            WallTime = time.time() - start\n",
        "            print(f'\\tTerminé! :D')\n",
        "            print(f'\\t Train >> Loss: {train_loss:.4f} - Acc: {train_acc:.4f}')\n",
        "            print(f'\\t   Val >> Loss: {val_loss:.4f} - Acc: {val_acc:.4f}')\n",
        "            print(f'<<< WallTime = {WallTime/60:.2f} min')\n",
        "\n",
        "    modelo = model.__class__.__name__\n",
        "    activacion = model.actfunc\n",
        "    GD = optimizer.__class__.__name__\n",
        "    LR = optimizer.param_groups[0]['lr']\n",
        "    Walltime = f'{WallTime/60:.2f}'\n",
        "    titulo = f'{modelo}_{activacion}_{GD}_LR-{LR}_Walltime-{Walltime}min'\n",
        "\n",
        "    # Guardamos métricas\n",
        "    Arr = np.array([train_loss_history, train_acc_history, val_loss_history, val_acc_history]).T\n",
        "    df = pd.DataFrame(Arr, columns=['train_loss_history', 'train_acc_history', 'val_loss_history', 'val_acc_history'])\n",
        "    df.to_csv(titulo+'_Metricas.csv', index=False)\n",
        "\n",
        "    # Guardamos etiquetas\n",
        "    Arr = np.array([all_labels, all_preds]).T\n",
        "    df = pd.DataFrame(Arr, columns=['all_labels', 'all_preds'])\n",
        "    df.to_csv(titulo+'_Etiquetas.csv', index=False)\n",
        "\n",
        "    print(titulo)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPEVR6DGXDkD"
      },
      "source": [
        "### Gráficas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp3-mbQV2qKJ"
      },
      "outputs": [],
      "source": [
        "def plotting_confusion_matrix(path, CIFAR_CLASSES):\n",
        "    Arr = pd.read_csv(path).to_numpy()\n",
        "\n",
        "    # Plotting Confusion Matrix\n",
        "    fig, axs = plt.subplots(1, 2, figsize = (14, 6))\n",
        "\n",
        "    # Implementing visualization of Confusion Matrix\n",
        "    c_m = confusion_matrix(Arr[:,0], Arr[:,1])\n",
        "    c_m_normalized = confusion_matrix(Arr[:,0], Arr[:,1], normalize='true').round(3)\n",
        "\n",
        "    ConfusionMatrixDisplay(c_m, display_labels=CIFAR_CLASSES).plot(cmap='Greys', xticks_rotation=25, ax=axs[0])\n",
        "    ConfusionMatrixDisplay(c_m_normalized*100, display_labels=CIFAR_CLASSES).plot(cmap='Greys', xticks_rotation=25, ax=axs[1],)\n",
        "\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.yticks(fontsize=10)\n",
        "    axs[0].set_title('Valores absolutos', fontsize=14)\n",
        "    axs[1].set_title('Valores porcentuales', fontsize=14)\n",
        "\n",
        "    titulo = path.split('_Etiquetas')[0]\n",
        "    if len(titulo.split('Outputs/')) == 2:\n",
        "        titulo = titulo.split('Outputs/')[1]\n",
        "    plt.suptitle(titulo, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_and_accuracy(path):\n",
        "    Arr = pd.read_csv(path).to_numpy()\n",
        "\n",
        "    xAxis = [x+1 for x in range(Arr.shape[0])]\n",
        "    xTicks = [(k+1)*10 for k in range(int(100/10))]\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize = (10, 4))\n",
        "\n",
        "    # Plot the training and validation loss\n",
        "    axs[0].plot(xAxis, Arr[:, 0], label='train loss')\n",
        "    axs[0].plot(xAxis, Arr[:, 2], label='val loss')\n",
        "    axs[0].grid()\n",
        "    axs[0].set_xlabel('Épocas')\n",
        "    axs[0].set_xticks(xTicks)\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[0].legend(loc='upper right')\n",
        "\n",
        "\n",
        "    # Plot the training and validation accuracy\n",
        "    axs[1].plot(xAxis, Arr[:, 1], label='train acc')\n",
        "    axs[1].plot(xAxis, Arr[:, 3], label='val acc')\n",
        "    axs[1].grid()\n",
        "    axs[1].set_xlabel('Épocas')\n",
        "    axs[1].set_xticks(xTicks)\n",
        "    axs[1].set_ylabel('Acc')\n",
        "    axs[1].legend(loc='lower right')\n",
        "\n",
        "    titulo = path.split('_Metricas')[0]\n",
        "    if len(titulo.split('Outputs/')) == 2:\n",
        "        titulo = titulo.split('Outputs/')[1]\n",
        "\n",
        "    plt.suptitle(titulo, fontsize=16)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b6ufcuoZ3E4"
      },
      "source": [
        "---\n",
        "# Descripción, carga y preprocesamiento del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1OsnyZOIDkQ"
      },
      "source": [
        "El dataset a utilizar es el **[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)**, el cual es un conjunto estándar para hacer reconocimiento de imágenes. Consta de 60000 imágenes RGB de 32x32 divididas en 10 clases mutuamente excluyentes (avión, auto, pájaro, gato, ciervo, perro, rana, caballo, barco y camión), con 6000 imágenes por clase. De estas 60000 hay 50000 imágenes de entrenamiento y las otras 10000 son de evaluación.\n",
        "\n",
        "Nuestro objetivo es entrenar una CNN que clasifique los objetos de las imágenes dentro de alguna de las 10 categorías dadas.\n",
        "\n",
        "***Observación:*** este dataset ya está incoporado dentro de las librerías de pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HURRFhXSIDkQ"
      },
      "outputs": [],
      "source": [
        "# Tamaño de lote (default)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Número de épocas (default)\n",
        "EPOCHS = 100\n",
        "\n",
        "# Función de pérdida\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Convertimos en tensor y normalizamos, tomando el valor medio del rango\n",
        "# posible, i.e. [0; 1].\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Obtenemos el conjunto de entrenamiento y de evaluación, junto a sus loaders\n",
        "train_set, train_loader, val_set, val_loader, test_set, test_loader = data_preparation(BATCH_SIZE, transform)\n",
        "\n",
        "# Clases disponibles\n",
        "CIFAR_CLASSES = test_set.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3uO5LOp3cI1"
      },
      "source": [
        "---\n",
        "# Modelo de partida: red convolucional sin padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqQ18h673cI1"
      },
      "source": [
        "## Definición del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gegZm7wEV4wb"
      },
      "source": [
        "La red base que usamos consta de 2 capas convolucionales:\n",
        "- La primera con 3 canales de entrada (RGB de las imágenes) y 10 canales de salida.\n",
        "- La segunda toma estos 10 canales de salida como canales de entrada y devuelve 20 canales de saldia.\n",
        "\n",
        "En ambos casos los filtros son 5x5, el paso es de 1 y el relleno es nulo. Esto hace que la primera capa convolucional tome las imágenes 32x32x3 y devuelva un mapa de características de 28x28x10. Luego, el max pooling nos deja un mapa de características de 14x14x10. La segunda capa convolucional toma este mapa de 14x14x10, y devuelve otro mapa de 10x10x20, quedando en 5x5x20 luego del max pooling.\n",
        "\n",
        "Al aplanar esta salida, la flatten layer es un vector de 500 (=5x5x20) entradas. La capa fully-connected toma estos 500 valores y devuelve 10, uno por cada clase posible.\n",
        "\n",
        "La función de activación por defecto de esta red es la sigmoide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmcwGhbv3cI2"
      },
      "outputs": [],
      "source": [
        "class NetwithoutPadding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetwithoutPadding, self).__init__()\n",
        "        # Inicializar 2 capas convolucionales\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5, padding=0)\n",
        "\n",
        "        # Inicializar función de activación. Default: Sigmoide\n",
        "        self.actfunc = nn.Sigmoid()\n",
        "\n",
        "        # Inicializar una capa de agrupamiento por máximo\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Inicializar la capa totalmente conectada\n",
        "        self.fc = nn.Linear(5 * 5 * 20, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolución + Activación + Agrupamiento sobre capa 1\n",
        "        x = self.actfunc(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # # Convolución + Activación + Agrupamiento sobre capa 2\n",
        "        x = self.actfunc(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Preparar la imagen para la capa totalmente conectada (flatten layer)\n",
        "        x = x.view(-1, self.fc.in_features)\n",
        "\n",
        "        # Aplicar la capa totalmente conectada y obtener resultado\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqaaTQ2R3cI3"
      },
      "source": [
        "## Corrida default: baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc9isvXLC1qd"
      },
      "outputs": [],
      "source": [
        "# # Instanciamos el modelo\n",
        "# model = NetwithoutPadding()\n",
        "# # Definimos el optimizador\n",
        "# optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "# # Corremos el experimento\n",
        "# train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z98stZavEe2l"
      },
      "outputs": [],
      "source": [
        "path = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_Sigmoid_SGD_LR-0.1_Walltime-24.35min_Metricas.csv'\n",
        "plot_loss_and_accuracy(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTKc7TXKKYhq"
      },
      "outputs": [],
      "source": [
        "path = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_Sigmoid_SGD_LR-0.1_Walltime-24.35min_Etiquetas.csv'\n",
        "plotting_confusion_matrix(path, CIFAR_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0J1YdT8sCFq"
      },
      "source": [
        "## Estudio de la función de activación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGheBcCMsFgM"
      },
      "outputs": [],
      "source": [
        "# # La sigmoide es el default\n",
        "# activation_functions = [nn.Tanh(), nn.ReLU(), nn.LeakyReLU()]\n",
        "\n",
        "# for af in activation_functions:\n",
        "#     model = NetwithoutPadding()\n",
        "#     model.actfunc = af\n",
        "#     print(model.actfunc)\n",
        "#     optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "#     train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP8NYKEQsVEz"
      },
      "outputs": [],
      "source": [
        "prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_'\n",
        "for path in ['Tanh_SGD_LR-0.1_Walltime-25.00min_Metricas.csv',\n",
        "            'ReLU_SGD_LR-0.1_Walltime-24.71min_Metricas.csv',\n",
        "            'LeakyReLU_SGD_LR-0.1_Walltime-24.77min_Metricas.csv',\n",
        "            'Sigmoid_SGD_LR-0.1_Walltime-24.35min_Metricas.csv']:\n",
        "    plot_loss_and_accuracy(prefix+path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9QcbJbixCPS"
      },
      "source": [
        "## Estudio del optimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U68a7OWFmmh"
      },
      "outputs": [],
      "source": [
        "# # SGD es el default\n",
        "# optimizadores = [optim.Adam, optim.Adagrad, optim.RMSprop, optim.Adam]\n",
        "\n",
        "# for gd in optimizadores:\n",
        "#     print(gd)\n",
        "#     model = NetwithoutPadding()\n",
        "#     # Adagrad no funciona sin esta línea (es un issue ya reportado)\n",
        "#     model = model.to(device=device)\n",
        "#     # Usamos la función de activación elegida\n",
        "#     model.actfunc = nn.ReLU()\n",
        "#     optimizer = gd(model.parameters(), lr=1e-1)\n",
        "#     train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHw4jp_tHbhe"
      },
      "outputs": [],
      "source": [
        "prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_'\n",
        "for path in ['ReLU_Adam_LR-0.1_Walltime-25.39min_Metricas.csv',\n",
        "             'ReLU_RMSprop_LR-0.1_Walltime-26.22min_Metricas.csv',\n",
        "             'ReLU_SGD_LR-0.1_Walltime-24.71min_Metricas.csv',\n",
        "             'ReLU_Adagrad_LR-0.1_Walltime-24.84min_Metricas.csv']:\n",
        "    plot_loss_and_accuracy(prefix+path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQbQg3P_xENC"
      },
      "source": [
        "## Estudio de la tasa de aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR8fULBnFpMr"
      },
      "outputs": [],
      "source": [
        "# # 0.1 es el default\n",
        "# alphas = [1e-4] # [1e-2, 1e-3, 1e-4]\n",
        "\n",
        "# for lr in alphas:\n",
        "#     print(lr)\n",
        "#     model = NetwithoutPadding()\n",
        "#     # Adagrad no funciona sin esta línea (es un issue ya reportado)\n",
        "#     model = model.to(device=device)\n",
        "#     # Usamos la función de activación elegida\n",
        "#     model.actfunc = nn.ReLU()\n",
        "#     # Usamos el optimizador elegido\n",
        "#     optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
        "#     train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciH6QP8UHhvf"
      },
      "outputs": [],
      "source": [
        "prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/NetwithoutPadding_'\n",
        "for path in ['ReLU_Adagrad_LR-0.0001_Walltime-23.86min_Metricas.csv',\n",
        "             'ReLU_Adagrad_LR-0.001_Walltime-26.21min_Metricas.csv',\n",
        "             'ReLU_Adagrad_LR-0.01_Walltime-26.38min_Metricas.csv',\n",
        "             'ReLU_Adagrad_LR-0.1_Walltime-24.84min_Metricas.csv']:\n",
        "    plot_loss_and_accuracy(prefix+path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M-MAPEUPxj"
      },
      "source": [
        "Me quedo con LR=0.001 como lo mejorcito."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFsR9iRTUYU9"
      },
      "source": [
        "---\n",
        "# Modelo nuevo: afinando el filtro y profundizando la red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu3fjM-kUeAH"
      },
      "source": [
        "## Definición del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKAd9EFeYDNy"
      },
      "source": [
        "En un primer intento de mejora, generamos la siguiente red, la cual consta de 3 capas convolucionales:\n",
        "- La primera con 3 canales de entrada (RGB de las imágenes) y 10 canales de salida.\n",
        "- La segunda toma estos 10 canales de salida como canales de entrada y devuelve 20 canales de saldia.\n",
        "- La tercera toma estos 20 canales de salida como canales de entrada y devuelve 30 canales de saldia.\n",
        "\n",
        "En ambos casos los filtros son 3x3, el paso es de 1 y el relleno es de 1 (modo 'reflect'), manteniendo el tamaño de entrada en cada capa convolucional. Esto hace que la primera capa convolucional tome las imágenes 32x32x3 y devuelva un mapa de características de 32x32x10. Luego, el max pooling nos deja un mapa de características de 16x16x10. La segunda capa convolucional toma este mapa de 16x16x10, y devuelve otro mapa de 16x16x20, quedando en 8x8x20 luego del max pooling. Finalmente, la tercera capa convolucional toma este mapa de 8x8x20, devolviendo otro de 8x8x30, reduciéndose a 4x4x30 luego del max pooling.\n",
        "\n",
        "Al aplanar esta salida, la flatten layer es un vector de 480 (=4x4x30) entradas. La capa fully-connected toma estos 480 valores y devuelve 10, uno por cada clase posible.\n",
        "\n",
        "La función de activación por defecto de esta red es la sigmoide.\n",
        "\n",
        "Resumiendo, las principales diferencias con la red base son:\n",
        "- Cantidad de capas convolucionales: tiene una más.\n",
        "- El tamaño de los filtros: son más pequeños.\n",
        "- Tiene padding, reflejando los valores de los bordes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg5IJ-shUeAI"
      },
      "outputs": [],
      "source": [
        "class CNN3K3P1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN3K3P1, self).__init__()\n",
        "        # Inicializar 2 capas convolucionales\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, padding=1, padding_mode='reflect')\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1, padding_mode='reflect')\n",
        "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=30, kernel_size=3, padding=1, padding_mode='reflect')\n",
        "\n",
        "        # Inicializar función de activación. Default: Sigmoide\n",
        "        self.actfunc = nn.Sigmoid()\n",
        "\n",
        "        # Inicializar una capa de agrupamiento por máximo\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Inicializar la capa totalmente conectada\n",
        "        self.fc = nn.Linear(4 * 4 * 30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolución + Activación + Agrupamiento sobre capa 1\n",
        "        x = self.actfunc(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Convolución + Activación + Agrupamiento sobre capa 2\n",
        "        x = self.actfunc(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Convolución + Activación + Agrupamiento sobre capa 3\n",
        "        x = self.actfunc(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Preparar la imagen para la capa totalmente conectada (flatten layer)\n",
        "        x = x.view(-1, self.fc.in_features)\n",
        "\n",
        "        # Aplicar la capa totalmente conectada y obtener resultado\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FXE-dECUeAJ"
      },
      "source": [
        "## Corrida default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jQ6eZ4wUeAJ"
      },
      "outputs": [],
      "source": [
        "# # Instanciamos el modelo\n",
        "# model = CNN3K3P1()\n",
        "# # Definimos el optimizador\n",
        "# optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "# # Corremos el experimento\n",
        "# train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ETfrX93UeAJ"
      },
      "outputs": [],
      "source": [
        "path = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/CNN3K3P1_Sigmoid_SGD_LR-0.1_Walltime-24.13min_Metricas.csv'\n",
        "plot_loss_and_accuracy(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbXhhyCoUeAK"
      },
      "outputs": [],
      "source": [
        "path = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/CNN3K3P1_Sigmoid_SGD_LR-0.1_Walltime-24.13min_Etiquetas.csv'\n",
        "plotting_confusion_matrix(path, CIFAR_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVZFfo4_UeAL"
      },
      "source": [
        "## Estudio de la función de activación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfHDxIawUeAi"
      },
      "outputs": [],
      "source": [
        "# # La sigmoide es el default\n",
        "# activation_functions = [nn.LeakyReLU()] # [nn.Tanh(), nn.ReLU(), nn.LeakyReLU()]\n",
        "\n",
        "# for af in activation_functions:\n",
        "#     model = CNN3K3P1()\n",
        "#     model.actfunc = af\n",
        "#     print(model.actfunc)\n",
        "#     optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0.9)\n",
        "#     train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRb5u0LdUeAi"
      },
      "outputs": [],
      "source": [
        "prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/CNN3K3P1_'\n",
        "for path in ['Tanh_SGD_LR-0.1_Walltime-23.88min_Metricas.csv',\n",
        "             'ReLU_SGD_LR-0.1_Walltime-24.41min_Metricas.csv',\n",
        "             'LeakyReLU_SGD_LR-0.1_Walltime-24.57min_Metricas.csv',\n",
        "             'Sigmoid_SGD_LR-0.1_Walltime-24.13min_Metricas.csv']:\n",
        "    plot_loss_and_accuracy(prefix+path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dlWRcaaU53Z"
      },
      "source": [
        "me quedo con la sigmoide (default). En la leaky salía NaN en las loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfONXNBHUeAj"
      },
      "source": [
        "## Estudio del optimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppwdqT1fUeAj"
      },
      "outputs": [],
      "source": [
        "# # SGD es el default\n",
        "# optimizadores = [optim.Adam, optim.Adagrad, optim.RMSprop]\n",
        "\n",
        "# for gd in optimizadores:\n",
        "#     print(gd)\n",
        "#     model = CNN3K3P1()\n",
        "#     # Adagrad no funciona sin esta línea (es un issue ya reportado)\n",
        "#     model = model.to(device=device)\n",
        "#     optimizer = gd(model.parameters(), lr=1e-1)\n",
        "#     train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHwhBf7GUeAj"
      },
      "outputs": [],
      "source": [
        "prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/CNN3K3P1_'\n",
        "for path in ['Sigmoid_SGD_LR-0.1_Walltime-24.13min_Metricas.csv',\n",
        "             'Sigmoid_Adam_LR-0.1_Walltime-25.30min_Metricas.csv',\n",
        "             'Sigmoid_Adagrad_LR-0.1_Walltime-24.85min_Metricas.csv',\n",
        "             'Sigmoid_RMSprop_LR-0.1_Walltime-24.82min_Metricas.csv']:\n",
        "    plot_loss_and_accuracy(prefix+path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3lRRTP8Z7xo"
      },
      "source": [
        "me quedo con adagrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q53oQezMUeAk"
      },
      "source": [
        "## Estudio de la tasa de aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b1ijL89Z7xp"
      },
      "outputs": [],
      "source": [
        "# prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/CNN3K3P1_'\n",
        "# for path in ['ReLU_Adagrad_LR-0.01_Walltime-29.31min_Metricas.csv',\n",
        "#              'ReLU_Adagrad_LR-0.001_Walltime-27.44min_Metricas.csv',\n",
        "#              'ReLU_SGD_LR-0.1_Walltime-24.41min_Metricas.csv']:\n",
        "#     plot_loss_and_accuracy(prefix+path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D6UdAuTUeAk"
      },
      "outputs": [],
      "source": [
        "# 0.1 es el default\n",
        "alphas = [1e-2, 1e-3, 1e-4]\n",
        "\n",
        "for lr in alphas:\n",
        "    print(lr)\n",
        "    model = CNN3K3P1()\n",
        "    # Adagrad no funciona sin esta línea (es un issue ya reportado)\n",
        "    model = model.to(device=device)\n",
        "    # Usamos el optimizador elegido\n",
        "    optimizer = optim.Adagrad(model.parameters(), lr=lr)\n",
        "    train_val(model, criterion, optimizer, train_loader, val_loader, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjM1fGxmUeAl"
      },
      "outputs": [],
      "source": [
        "prefix = 'https://raw.githubusercontent.com/Cheva94/Diplo_Opt/cheva_dl_lab2/3_DL/Lab2/Outputs/CNN3K3P1_'\n",
        "for path in ['Sigmoid_Adagrad_LR-0.0001_Walltime-25.98min_Metricas.csv',\n",
        "             'Sigmoid_Adagrad_LR-0.001_Walltime-26.05min_Metricas.csv',\n",
        "             'Sigmoid_Adagrad_LR-0.01_Walltime-26.65min_Metricas.csv',\n",
        "             'Sigmoid_Adagrad_LR-0.1_Walltime-24.85min_Metricas.csv']:\n",
        "    plot_loss_and_accuracy(prefix+path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vYg4jx_iZ3Ey",
        "7b6ufcuoZ3E4",
        "O3uO5LOp3cI1",
        "17fSO_O23cI5",
        "py-hG15c3cI6",
        "gp66JoSR3cI7",
        "tWoJ8Jog3cI8",
        "amUB47Dd3cI_",
        "ML_YjvPr_drw",
        "XTAUvFPpNAsy",
        "VBnK5nqKO_15",
        "-1OvRv9Wn6pz",
        "ddpbSAdSDwau",
        "Un9Far-eJjWN",
        "Sk1lWQt-OB2B",
        "n-a1QJEzSBIS",
        "_9DWL-u_Xwfc"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}