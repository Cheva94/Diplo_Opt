{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYg4jx_iZ3Ey"
      },
      "source": [
        "# Diplomatura en ciencia de datos, aprendizaje automático y sus aplicaciones - Edición 2023 - FAMAF (UNC)\n",
        "\n",
        "## Introducción al aprendizaje profundo\n",
        "\n",
        "### Trabajo práctico entregable 2/2\n",
        "\n",
        "- **Estudiantes:**\n",
        "    - [Chevallier-Boutell, Ignacio José](https://www.linkedin.com/in/nachocheva/) (materia completa).\n",
        "    - Gastelu, Gabriela (materia completa).\n",
        "    - Spano, Marcelo (materia completa).\n",
        "\n",
        "- **Docentes:**\n",
        "    - Johanna Analiz Frau (Mercado Libre).\n",
        "    - Nindiría Armenta Guerrero (fyo).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81o1pr1wZ3E2"
      },
      "source": [
        "## Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkfYanqJZ3E3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "# Configuración del dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Para que todo sea reproducible\n",
        "torch.manual_seed(1994)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader#, TensorDataset\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "from torchvision import datasets\n",
        "# from torchvision.utils import make_grid\n",
        "# from torchvision.transforms import ToTensor, transforms\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# from sklearn import metrics\n",
        "# from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funciones útiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_preparation(BATCH_SIZE, transform):\n",
        "\n",
        "    # Download and load the training data\n",
        "    trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True, num_workers=2)\n",
        "\n",
        "    # Download and load the test data\n",
        "    testset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "    testloader = DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainset, trainloader, testset, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, trainloader, loss_function, optimizer, epoch, device, use_tqdm=True):\n",
        "    '''\n",
        "    Lleva a cabo el entrenamiento del modelo.\n",
        "\n",
        "    Args:\n",
        "        model: estructura de la red neuronal.\n",
        "        trainloader: cargador de datos de entrenamiento.\n",
        "        loss_function: función de costo a utilizar.\n",
        "        optimizer: tipo de descenso por gradiente a utilizar.\n",
        "        epoch: número de épocas a entrenar.\n",
        "        use_tqdm: muestra el progreso del entrenamiento.\n",
        "        device: dónde se realiza el cálculo.\n",
        "    '''\n",
        "\n",
        "    # Enviamos el modelo al dispositivo donde se realiza el cálculo\n",
        "    model.to(device)\n",
        "\n",
        "    # Activamos el modo de entrenamiento en el modelo\n",
        "    model.train()\n",
        "\n",
        "    # Inicializamos el costo acumulado de la época\n",
        "    training_loss = 0.0\n",
        "    pbar = tqdm(trainloader) if use_tqdm else trainloader\n",
        "    for step, (inputs, labels) in enumerate(pbar, 1):\n",
        "        # Tensors to gpu (if necessary)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        # Run a forward pass\n",
        "        predicted_outputs = model(inputs.view(inputs.shape[0], -1))\n",
        "        # Compute loss\n",
        "        loss = loss_function(predicted_outputs, labels.long())\n",
        "        # Backpropagation\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Accumulate the average loss of the mini-batch\n",
        "        training_loss += loss.item()\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics each 50 mini-batches\n",
        "        if use_tqdm and step % 50 == 0:\n",
        "          # Show number of epoch, step and average loss\n",
        "          pbar.set_description(f\"[{epoch}, {step}] loss: {training_loss / step:.4g}\")\n",
        "\n",
        "    epoch_training_loss = round(training_loss / len(trainloader), 4)\n",
        "\n",
        "    return epoch_training_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validation(model, valloader, loss_function, device, use_tqdm=True):\n",
        "    '''\n",
        "    Lleva a cabo la validación del modelo. Se utiliza la accuracy como métrica \n",
        "    principal y el f1-score como métrica secundaria.\n",
        "\n",
        "    Args:\n",
        "        model: estructura de la red neuronal.\n",
        "        valloader: cargador de datos de validación.\n",
        "        use_tqdm: muestra el progreso del entrenamiento.\n",
        "        device: dónde se realiza el cálculo.\n",
        "    '''\n",
        "\n",
        "    model.eval()  # Activate evaluation mode\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    validation_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "    total = 0\n",
        "    # Don't calculate gradient speed up the forward pass\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(valloader) if use_tqdm else valloader\n",
        "        for (inputs, labels) in pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # Run the forward pass\n",
        "            predicted_outputs = model(inputs.view(inputs.shape[0], -1))\n",
        "            # Compute loss\n",
        "            loss = loss_function(predicted_outputs, labels.long())\n",
        "            # Accumulate the average loss of the mini-batch\n",
        "            validation_loss += loss.item()\n",
        "\n",
        "            # The label with the highest value will be our prediction\n",
        "            _, predicted = torch.max(predicted_outputs , 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    epoch_validation_loss = round(validation_loss / len(valloader), 4)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
        "    f1 = round(metrics.f1_score(y_true, y_pred, average='macro'), 4)\n",
        "\n",
        "    return epoch_validation_loss, (accuracy, f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(model, n_epochs, trainloader, valloader, loss_function, optimizer, device, use_tqdm=True):\n",
        "    '''\n",
        "    Función de ejecución de experimentos, que entrena y valida el modelo, \n",
        "    evaluando la función de costo para cada conjunto de hiperparámetros. Guarda \n",
        "    los hiperparámetros y resultados en un diccionario.\n",
        "\n",
        "    Args:\n",
        "        model: estructura de la red neuronal.\n",
        "        n_epochs: número de épocas a entrenar.\n",
        "        trainloader: cargador de datos de entrenamiento.\n",
        "        valloader: cargador de datos de validación.\n",
        "        loss_function: función de costo a utilizar.\n",
        "        optimizer: tipo de descenso por gradiente a utilizar.\n",
        "        device: dónde se realiza el cálculo.\n",
        "        use_tqdm: muestra el progreso del entrenamiento.\n",
        "    '''\n",
        "\n",
        "    register_performance = {\n",
        "        'epoch': [],\n",
        "        'epoch_training_loss': [], 'epoch_validation_loss': [],\n",
        "        'validation_accuracy': [], 'validation_f1': []\n",
        "        }\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    print(\"Begin training...\")\n",
        "    start = time.time()\n",
        "    # Loop through the dataset multiple times\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # Train the model\n",
        "        epoch_training_loss = train(model, trainloader, loss_function, optimizer, epoch, device, use_tqdm)\n",
        "        # Validate the model\n",
        "        epoch_validation_loss, metrics = validation(model, valloader, loss_function, device, use_tqdm)\n",
        "\n",
        "        register_performance['epoch'].append(epoch)\n",
        "        register_performance['epoch_training_loss'].append(epoch_training_loss)\n",
        "        register_performance['epoch_validation_loss'].append(epoch_validation_loss)\n",
        "        register_performance['validation_accuracy'].append(metrics[0])\n",
        "        register_performance['validation_f1'].append(metrics[1])\n",
        "\n",
        "        # Save the model if the accuracy is the best\n",
        "        if metrics[0] > best_accuracy:\n",
        "            best_model = model\n",
        "            best_accuracy = metrics[0]\n",
        "\n",
        "        if (epoch % 10 == 0) and (epoch != n_epochs):\n",
        "            print(f'\\tVoy por la época {epoch}! :)')\n",
        "        elif epoch == n_epochs:\n",
        "            WallTime = time.time() - start\n",
        "            print(f'\\tTerminé! :D >>> WallTime = {WallTime/60:.2f} min')\n",
        "\n",
        "\n",
        "    # Save the results\n",
        "    experiment = {\n",
        "        'arquitecture': str(model),\n",
        "        'optimizer': optimizer,\n",
        "        'loss': str(loss_function),\n",
        "        'epochs': n_epochs,\n",
        "    }\n",
        "\n",
        "    # Print the statistics of the epoch\n",
        "    print(f'Completed training in {epoch} batch: ',\n",
        "          'Training Loss is: ' , epoch_training_loss,\n",
        "          '- Validation Loss is: ', epoch_validation_loss,\n",
        "          '- Accuracy is: ', (metrics[0]),\n",
        "          '- F1 is: ', (metrics[1])\n",
        "          )\n",
        "    return experiment, register_performance, best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data_loss_metrics(experiments_set, path):\n",
        "    df_base = pd.DataFrame()\n",
        "    for i in range(len(experiments_set)):\n",
        "        arquitecture = experiments_set[i][0]['arquitecture']\n",
        "        model_name = arquitecture.split('(')[0]\n",
        "        if len(arquitecture.split('activ1): ')) == 1:\n",
        "            activation_function_name = arquitecture.split('(1): ')[1].split('()\\n')[0].split('(negative_slope')[0]\n",
        "        else:\n",
        "            activation_function_name = arquitecture.split('activ1): ')[1].split('()\\n  (drop1)')[0].split('(negative_slope')[0]\n",
        "        optim = type(experiments_set[i][0]['optimizer']).__name__\n",
        "        lr = experiments_set[i][0]['optimizer'].param_groups[0]['lr']\n",
        "        weight_decay = experiments_set[i][0]['optimizer'].param_groups[0]['weight_decay']\n",
        "        df = pd.DataFrame(experiments_set[i][1])\n",
        "        df['model-activation-optimizer-lr-wd'] = f'{model_name}-{activation_function_name}-{optim}-{lr}-{weight_decay}'\n",
        "        df_base = pd.concat([df_base, df])\n",
        "\n",
        "    df_base.to_csv(path, index=False)\n",
        "\n",
        "    df_metrics = df_base.drop(columns=['epoch_training_loss', 'epoch_validation_loss'])\n",
        "    df_loss = df_base.drop(columns=['validation_accuracy', 'validation_f1']).melt(id_vars=['epoch', 'model-activation-optimizer-lr-wd'],\n",
        "                                                                                        value_vars=['epoch_training_loss', 'epoch_validation_loss'],\n",
        "                                                                                        var_name='task', value_name='loss')\n",
        "    return df_loss, df_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_results(n_epochs, path, exs_set=None, yLoss=[None, None], yMet=[None, None]):\n",
        "\n",
        "    L = [k*10+9 for k in range(int(n_epochs/10))]\n",
        "\n",
        "    if exs_set == None:\n",
        "        df_base = pd.read_csv(path)\n",
        "        df_metrics = df_base.drop(columns=['epoch_training_loss', 'epoch_validation_loss'])\n",
        "        df_loss = df_base.drop(columns=['validation_accuracy', 'validation_f1']).melt(id_vars=['epoch', 'model-activation-optimizer-lr-wd'],\n",
        "                                                                                            value_vars=['epoch_training_loss', 'epoch_validation_loss'],\n",
        "                                                                                            var_name='task', value_name='loss')\n",
        "    else:\n",
        "        df_loss, df_metrics = get_data_loss_metrics(exs_set, path)\n",
        "\n",
        "    print('Pérdidas:')\n",
        "    sns.catplot(data=df_loss, x='epoch', y='loss',  hue='task', col='model-activation-optimizer-lr-wd',\n",
        "                col_wrap=3, kind='point', height=4, aspect=1.5)\n",
        "    plt.xticks(L)\n",
        "    if yLoss[0] != None:\n",
        "        plt.ylim(yLoss[0], yLoss[1])\n",
        "    plt.show()\n",
        "\n",
        "    print('\\nMétricas:')\n",
        "    _, axs = plt.subplots(1, 2, figsize=(15, 4))\n",
        "    sns.pointplot(data=df_metrics, x='epoch', y='validation_accuracy', hue='model-activation-optimizer-lr-wd', ax=axs[0])\n",
        "    axs[0].set_xticks(L)\n",
        "    sns.pointplot(data=df_metrics, x='epoch', y='validation_f1', hue='model-activation-optimizer-lr-wd', ax=axs[1])\n",
        "    axs[1].set_xticks(L)\n",
        "    if yMet[0] != None:\n",
        "        axs[0].set_ylim(yMet[0], yMet[1])\n",
        "        axs[1].set_ylim(yMet[0], yMet[1])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_best(model, n_epochs, trainloader, valloader, testloader, loss_function, optimizer, device, use_tqdm=True):\n",
        "    '''\n",
        "    Ejecuta la corrida con los mejores hiperparámetros encontrados, entrenando, \n",
        "    validando y evaluando el modelo.\n",
        "\n",
        "    Args:\n",
        "        model: estructura de la red neuronal.\n",
        "        n_epochs: número de épocas a entrenar.\n",
        "        trainloader: cargador de datos de entrenamiento.\n",
        "        valloader: cargador de datos de validación.\n",
        "        testloader: cargador de datos de evaluación.\n",
        "        loss_function: función de costo a utilizar.\n",
        "        optimizer: tipo de descenso por gradiente a utilizar.\n",
        "        device: dónde se realiza el cálculo.\n",
        "        use_tqdm: muestra el progreso del entrenamiento.\n",
        "    '''\n",
        "\n",
        "    register_performance = {\n",
        "        'epoch': [],\n",
        "        'epoch_training_loss': [], \n",
        "        'epoch_validation_loss': [], \n",
        "        'epoch_testing_loss': [], \n",
        "        'training_accuracy': [], 'training_f1': [],\n",
        "        'validation_accuracy': [], 'validation_f1': [],\n",
        "        'testing_accuracy': [], 'testing_f1': []\n",
        "        }\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    print(\"Begin training...\")\n",
        "    start = time.time()\n",
        "    # Loop through the dataset multiple times\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        register_performance['epoch'].append(epoch)\n",
        "        # Train the model\n",
        "        epoch_training_loss = train(model, trainloader, loss_function, optimizer, epoch, device, use_tqdm)\n",
        "        register_performance['epoch_training_loss'].append(epoch_training_loss)\n",
        "        _, metrics_train = validation(model, trainloader, loss_function, device, use_tqdm)\n",
        "        register_performance['training_accuracy'].append(metrics_train[0])\n",
        "        register_performance['training_f1'].append(metrics_train[1])\n",
        "\n",
        "        # Validate the model\n",
        "        epoch_validation_loss, metrics_val = validation(model, valloader, loss_function, device, use_tqdm)\n",
        "        register_performance['epoch_validation_loss'].append(epoch_validation_loss)\n",
        "        register_performance['validation_accuracy'].append(metrics_val[0])\n",
        "        register_performance['validation_f1'].append(metrics_val[1])\n",
        "        \n",
        "        # Test the model\n",
        "        epoch_testing_loss, metrics_test = validation(model, testloader, loss_function, device, use_tqdm)\n",
        "        register_performance['epoch_testing_loss'].append(epoch_testing_loss)\n",
        "        register_performance['testing_accuracy'].append(metrics_test[0])\n",
        "        register_performance['testing_f1'].append(metrics_test[1])\n",
        "\n",
        "        # Save the model if the accuracy is the best\n",
        "        if metrics_val[0] > best_accuracy:\n",
        "            best_model = model\n",
        "            best_accuracy = metrics_val[0]\n",
        "\n",
        "        if (epoch % 10 == 0) and (epoch != n_epochs):\n",
        "            print(f'\\tVoy por la época {epoch}! :)')\n",
        "        elif epoch == n_epochs:\n",
        "            WallTime = time.time() - start\n",
        "            print(f'\\tTerminé! :D >>> WallTime = {WallTime/60:.2f} min')\n",
        "\n",
        "    # Save the results\n",
        "    experiment = {\n",
        "        'arquitecture': str(model),\n",
        "        'optimizer': optimizer,\n",
        "        'loss': str(loss_function),\n",
        "        'epochs': n_epochs,\n",
        "    }\n",
        "\n",
        "    # Print the statistics of the epoch\n",
        "    print(f'Completed training in {epoch} batch: ',\n",
        "          'Training Loss is: ' , epoch_training_loss,\n",
        "          'Training Accuracy is: ', (metrics_train[0]),\n",
        "          'Training F1 is: ', (metrics_train[1]),\n",
        "          'Validation Loss is: ', epoch_validation_loss,\n",
        "          'Validation Accuracy is: ', (metrics_val[0]),\n",
        "          'Validation F1 is: ', (metrics_val[1]),\n",
        "          'Testing Loss is: ', epoch_testing_loss,\n",
        "          'Testing Accuracy is: ', (metrics_test[0]),\n",
        "          'Testing F1 is: ', (metrics_test[1])\n",
        "          )\n",
        "    return experiment, register_performance, best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_best_loss_metrics(best_exp, path):\n",
        "    df_base = pd.DataFrame()\n",
        "    \n",
        "    arquitecture = best_exp[0]['arquitecture']\n",
        "    model_name = arquitecture.split('(')[0]\n",
        "    if len(arquitecture.split('activ1): ')) == 1:\n",
        "            activation_function_name = arquitecture.split('(1): ')[1].split('()\\n')[0].split('(negative_slope')[0]\n",
        "    else:\n",
        "        activation_function_name = arquitecture.split('activ1): ')[1].split('()\\n  (drop1)')[0].split('(negative_slope')[0]\n",
        "    optim = type(best_exp[0]['optimizer']).__name__\n",
        "    lr = best_exp[0]['optimizer'].param_groups[0]['lr']\n",
        "    weight_decay = best_exp[0]['optimizer'].param_groups[0]['weight_decay']\n",
        "    df = pd.DataFrame(best_exp[1])\n",
        "    df['model-activation-optimizer-lr-wd'] = f'{model_name}-{activation_function_name}-{optim}-{lr}-{weight_decay}'\n",
        "    df_base = pd.concat([df_base, df])\n",
        "\n",
        "    df_base.to_csv(path, index=False)\n",
        "\n",
        "    df_metrics = df_base.drop(columns=['epoch_training_loss', 'epoch_validation_loss', 'epoch_testing_loss'])\n",
        "    df_loss = df_base.drop(columns=['training_accuracy', 'training_f1', 'validation_accuracy', 'validation_f1', 'testing_accuracy', 'testing_f1']).melt(id_vars=['epoch', 'model-activation-optimizer-lr-wd'],\n",
        "                                                                                        value_vars=['epoch_training_loss', 'epoch_validation_loss', 'epoch_testing_loss'],\n",
        "                                                                                        var_name='task', value_name='loss')\n",
        "    return df_loss, df_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_best(n_epochs, path, experiments_set=None):\n",
        "\n",
        "    L = [k*10+9 for k in range(int(n_epochs/10))]\n",
        "\n",
        "    if experiments_set == None:\n",
        "        df_base = pd.read_csv(path)\n",
        "        df_metrics = df_base.drop(columns=['epoch_training_loss', 'epoch_validation_loss', 'epoch_testing_loss'])\n",
        "        df_loss = df_base.drop(columns=['training_accuracy', 'training_f1', 'validation_accuracy', 'validation_f1', 'testing_accuracy', 'testing_f1']).melt(id_vars=['epoch', 'model-activation-optimizer-lr-wd'],\n",
        "                                                                                            value_vars=['epoch_training_loss', 'epoch_validation_loss', 'epoch_testing_loss'],\n",
        "                                                                                            var_name='task', value_name='loss')\n",
        "    else:\n",
        "        df_loss, df_metrics = get_best_loss_metrics(experiments_set, path)\n",
        "\n",
        "    print('Pérdidas:')\n",
        "    sns.catplot(data=df_loss, x='epoch', y='loss',  hue='task', col='model-activation-optimizer-lr-wd',\n",
        "                col_wrap=1, kind='point', height=4, aspect=1.5)\n",
        "    plt.xticks(L)\n",
        "    plt.show()\n",
        "\n",
        "    print('\\nMétricas:')\n",
        "    sns.pointplot(data=df_metrics, x='epoch', y='training_accuracy', color='C0', label='Train')\n",
        "    sns.pointplot(data=df_metrics, x='epoch', y='validation_accuracy', color='C1', label='Val')\n",
        "    sns.pointplot(data=df_metrics, x='epoch', y='testing_accuracy', color='C2', label='Test')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(L)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b6ufcuoZ3E4"
      },
      "source": [
        "---\n",
        "# Descripción, carga y preprocesamiento del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El dataset a utilizar es el **[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)**, el cual es un conjunto estándar para hacer reconocimiento de imágenes. Consta de 60000 imágenes RGB de 32x32 divididas en 10 clases mutuamente excluyentes (avión, auto, pájaro, gato, ciervo, perro, rana, caballo, barco y camión), con 6000 imágenes por clase. De estas 60000 hay 50000 imágenes de entrenamiento y las otras 10000 son de evaluación.\n",
        "\n",
        "Nuestro objetivo es entrenar una CNN que clasifique los objetos de las imágenes dentro de alguna de las 10 categorías dadas.\n",
        "\n",
        "***Observación:*** este dataset ya está incoporado dentro de las librerías de pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tamaño de lote del baseline\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Convertimos en tensor y normalizamos, tomando el valor medio del rango \n",
        "# posible, i.e. [0; 1].\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Obtenemos el conjunto de entrenamiento y de evaluación, junto a sus loaders\n",
        "trainset, trainloader, testset, testloader = data_preparation(BATCH_SIZE, transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variables comunes a todas las corridas\n",
        "EPOCHS = 100\n",
        "\n",
        "n_inputs = np.prod(np.array(trainset[0][0].shape))\n",
        "\n",
        "CIFAR_CLASSES = trainset.classes\n",
        "n_outputs = len(CIFAR_CLASSES)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3uO5LOp3cI1"
      },
      "source": [
        "---\n",
        "# Modelo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqQ18h673cI1"
      },
      "source": [
        "## Definición del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmcwGhbv3cI2"
      },
      "outputs": [],
      "source": [
        "class SmallMLP(nn.Module):\n",
        "    '''\n",
        "    Modelo fully-connected con 5 capas ocultas, teniendo n_hidden neuronas en\n",
        "    cada una de ellas. Se incluye además la posibilidad de aplicar un mismo\n",
        "    dropout a todas las capas (tanto la de entrada como las ocultas).\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs, n_hidden, activation_function, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.drop0 = nn.Dropout(dropout)\n",
        "\n",
        "        self.hidden1 = nn.Linear(n_inputs, n_hidden)\n",
        "        self.activ1 = activation_function\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.hidden2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.activ2 = activation_function\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.hidden3 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.activ3 = activation_function\n",
        "        self.drop3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.hidden4 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.activ4 = activation_function\n",
        "        self.drop4 = nn.Dropout(dropout)\n",
        "\n",
        "        self.hidden5 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.activ5 = activation_function\n",
        "        self.drop5 = nn.Dropout(dropout)\n",
        "\n",
        "        self.output = nn.Linear(n_hidden, n_outputs)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.drop0(x)\n",
        "\n",
        "        x = self.activ1(self.hidden1(x))\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = self.activ2(self.hidden2(x))\n",
        "        x = self.drop2(x)\n",
        "\n",
        "        x = self.activ3(self.hidden3(x))\n",
        "        x = self.drop3(x)\n",
        "\n",
        "        x = self.activ4(self.hidden4(x))\n",
        "        x = self.drop4(x)\n",
        "\n",
        "        x = self.activ5(self.hidden5(x))\n",
        "        x = self.drop5(x)\n",
        "\n",
        "        x = self.output(x)  # Output Layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqaaTQ2R3cI3"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDeQqY9X_dra"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "baseline_param = {\n",
        "    'nH': 3,\n",
        "    'AF': nn.Sigmoid(),\n",
        "    'GD': optim.SGD,\n",
        "    'LR': 0.1,\n",
        "    'Mom': 0.9\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a9VW-Dv3cI4"
      },
      "outputs": [],
      "source": [
        "#baseline_exp = []\n",
        "# Instanciamos el modelo\n",
        "# model = SmallMLP(n_inputs, n_outputs, baseline_param['nH'], baseline_param['AF'])\n",
        "# Definimos el optimizador\n",
        "# optimizer = baseline_param['GD'](model.parameters(), lr=baseline_param['LR'], momentum=baseline_param['Mom'])\n",
        "# Corremos el baseline\n",
        "# experiment = run_experiment(model, EPOCHS, Load_train, Load_val, loss_function, optimizer, device, use_tqdm=False)\n",
        "# baseline_exp.append(experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "7yacizJ83cI5",
        "outputId": "97ad44c5-cf1b-4c83-a5a8-6b107737347e"
      },
      "outputs": [],
      "source": [
        "#plot_results(EPOCHS, f'SmallMLP/Baseline_Epocas-{EPOCHS}.csv', baseline_exp)\n",
        "SmallMLP_Baseline_Epocas_100 = \"https://raw.githubusercontent.com/Cheva94/Diplo_Opt/main/3_DL/Lab1/SmallMLP/Baseline_Epocas-100.csv\"\n",
        "plot_results(100, SmallMLP_Baseline_Epocas_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HTFl2ZI_drc"
      },
      "source": [
        "Nuestro baseline con esta red pequeña (5 capas ocultas con 3 neuronas por capa), es bastante pobre. Por un lado, el costo de entrenamiento es prácticamente constante y, además, el costo de validación es muy errático. Esto se ve reflejando en las métricas, ya que oscilan a lo largo de todas las épocas, sin alncazar alguna estabilización."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17fSO_O23cI5"
      },
      "source": [
        "## Estudio de la función de activación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK0eWDgO_drc"
      },
      "outputs": [],
      "source": [
        "activation_functions = {\n",
        "    'Sigmoid': nn.Sigmoid(),\n",
        "    'Tanh': nn.Tanh(),\n",
        "    'ReLU': nn.ReLU(),\n",
        "    'LeakyReLU': nn.LeakyReLU()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Mcn0BgA3cI6"
      },
      "outputs": [],
      "source": [
        "# ActFunc_Exps1 = []\n",
        "\n",
        "# for key in activation_functions.keys():\n",
        "#     print(f'\\n\\n Corriendo con {key}')\n",
        "#     model = SmallMLP(n_inputs, n_outputs, baseline_param['nH'], activation_functions[key])\n",
        "#     optimizer = baseline_param['GD'](model.parameters(), lr=baseline_param['LR'], momentum=baseline_param['Mom'])\n",
        "#     experiment = run_experiment(model, EPOCHS, Load_train, Load_val, loss_function, optimizer, device, use_tqdm=False)\n",
        "#     ActFunc_Exps1.append(experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "9SGlfm8l3cI6",
        "outputId": "f8f1a1c2-81bb-4a83-a9e3-b707aeb0d837"
      },
      "outputs": [],
      "source": [
        "# plot_results(EPOCHS, f'SmallMLP/ActFunc_Epocas-{EPOCHS}.csv', ActFunc_Exps1)\n",
        "SmallMLP_ActFunc_Epocas_100 = \"https://raw.githubusercontent.com/Cheva94/Diplo_Opt/main/3_DL/Lab1/SmallMLP/ActFunc_Epocas-100.csv\"\n",
        "plot_results(100, SmallMLP_ActFunc_Epocas_100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdspjkJJ_drd"
      },
      "source": [
        "Lo primero que analizamos es qué ocurre al variar la función de activación. La sigmoide es la función de activación utilizada en el baseline. Vemos que LeakyReLU tiene un comportamiento similar a ésta. Las otras dos mejoran bastante. Tenemos que la ReLU tiene bastantes saltos en el costo de validación, correspondiéndose con saltos en las métricas, mientras que la Tanh resulta ser la de mayor estabilidad, alcanzando dicha estabilidad en una menor cantidad de épocas (aprox. 30) respecto a ReLU (aprox. 70). También se observa una mejor correspondencia entre los costes de entrenamiento y validación para Tanh que para ReLU. Decidimos quedarnos con Tanh."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vYg4jx_iZ3Ey",
        "7b6ufcuoZ3E4",
        "O3uO5LOp3cI1",
        "17fSO_O23cI5",
        "py-hG15c3cI6",
        "gp66JoSR3cI7",
        "tWoJ8Jog3cI8",
        "amUB47Dd3cI_",
        "ML_YjvPr_drw",
        "XTAUvFPpNAsy",
        "VBnK5nqKO_15",
        "-1OvRv9Wn6pz",
        "ddpbSAdSDwau",
        "Un9Far-eJjWN",
        "Sk1lWQt-OB2B",
        "n-a1QJEzSBIS",
        "_9DWL-u_Xwfc"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
